{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QA_itt_prototype.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERU6X6WWohlz"
      },
      "source": [
        "import torch\n",
        "import torchtext.data as data\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import shutil\n",
        "import json\n",
        "import spacy\n",
        "import logging\n",
        "import numpy as np\n",
        "import torchtext\n",
        "import csv\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import socket\n",
        "import copy\n",
        "import datetime\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from typing import List, Tuple, Dict\n",
        "from torch.nn.modules.loss import _Loss, CrossEntropyLoss\n",
        "from torchtext.data import BucketIterator, Iterator, RawField, Example\n",
        "from urllib import request\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "from torch.optim import Adam\n",
        "# from torchviz import make_dot\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Remove cached data like this if needed\n",
        "# !rm -r *\n",
        "\n",
        "# Remove pre-cached sample data in colab's directory\n",
        "if os.path.isdir(\"sample_data\"):\n",
        "  shutil.rmtree(\"sample_data\")\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.datetime.now().strftime('%Y-%m-%d_%H:%M')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZkLtP6hmRV7"
      },
      "source": [
        "!wget http://www.stud.fit.vutbr.cz/~ifajcik/bissit19/evaluate_squad\n",
        "!mv evaluate_squad evaluate_squad.py\n",
        "from evaluate_squad import evaluate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY-VWc3tnp2r"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWWMtqQ-rf5z"
      },
      "source": [
        "print(\"Current working directory: \" + os.getcwd())\n",
        "print(f\"python version: {sys.version}\")\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchtext version: {torchtext.__version__}\")\n",
        "print(f\"spacy version: {spacy.__version__}\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulty_w2R039z"
      },
      "source": [
        "# This code snippet will download dataset for us\n",
        "\n",
        "TRAIN_V1_URL = 'https://github.com/rajpurkar/SQuAD-explorer/raw/master/dataset/train-v1.1.json  '\n",
        "DEV_V1_URL = 'https://github.com/rajpurkar/SQuAD-explorer/raw/master/dataset/dev-v1.1.json'\n",
        "TRAIN = \"train-v1.1.json\"\n",
        "VALIDATION = \"dev-v1.1.json\"\n",
        "\n",
        "def download_url(path, url):\n",
        "    sys.stderr.write(f'Downloading from {url} into {path}\\n')\n",
        "    sys.stderr.flush()\n",
        "    request.urlretrieve(url, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0to2fxn0wBBA"
      },
      "source": [
        "def find_sub_list(sl, l):\n",
        "    \"\"\"\n",
        "    Methods finds sublist in list and returns its indices.\n",
        "    The indices are inclusive\n",
        "    \n",
        "    Example:\n",
        "    print(find_sub_list([3,2,1],[4,3,2,1,0]))\n",
        "    \n",
        "    Returns:\n",
        "    [(1, 3)]\n",
        "    \n",
        "    \"\"\"\n",
        "    results = []\n",
        "    sll = len(sl)\n",
        "    for ind in (i for i, e in enumerate(l) if e == sl[0]):\n",
        "        if l[ind:ind + sll] == sl:\n",
        "            results.append((ind, ind + sll - 1))\n",
        "\n",
        "    return results\n",
        "\n",
        "print(find_sub_list([3,2,1],[4,3,2,1,0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHyNAJVn_EGA"
      },
      "source": [
        "def create_custom_tokenizer(nlp):\n",
        "    custom_prefixes = [r'[0-9]+', r'\\~', r'\\–', r'\\—', r'\\$']\n",
        "    custom_infixes = [r'[!&:,()]', r'\\.', r'\\-', r'\\–', r'\\—', r'\\$']\n",
        "    custom_suffixes = [r'\\.', r'\\–', r'\\—', r'\\$']\n",
        "    default_prefixes = list(nlp.Defaults.prefixes) + custom_prefixes\n",
        "    default_prefixes.remove(r'US\\$')\n",
        "    default_prefixes.remove(r'C\\$')\n",
        "    default_prefixes.remove(r'A\\$')\n",
        "    \n",
        "    all_prefixes_re = spacy.util.compile_prefix_regex(tuple(default_prefixes))\n",
        "    infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + custom_infixes))\n",
        "    suffix_re = spacy.util.compile_suffix_regex(tuple(list(nlp.Defaults.suffixes) + custom_suffixes))\n",
        "\n",
        "    rules = dict(nlp.Defaults.tokenizer_exceptions)\n",
        "    # remove \"a.\" to \"z.\" rules so \"a.\" gets tokenized as a|.\n",
        "    for c in range(ord(\"a\"), ord(\"z\") + 1):\n",
        "        if f\"{chr(c)}.\" in rules:\n",
        "            rules.pop(f\"{chr(c)}.\")\n",
        "\n",
        "    return Tokenizer(nlp.vocab, rules,\n",
        "                     prefix_search=all_prefixes_re.search,\n",
        "                     infix_finditer=infix_re.finditer, suffix_search=suffix_re.search,\n",
        "                     token_match=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLoQLW-OyiLL"
      },
      "source": [
        "# We will use this special token to join the pre-tokenized data\n",
        "JOIN_TOKEN = \"█\"\n",
        "\n",
        "_spacy_en = spacy.load('en')\n",
        "_spacy_en.tokenizer = create_custom_tokenizer(_spacy_en)\n",
        "\n",
        "def tokenize(text: string, tokenizer=_spacy_en):\n",
        "    tokens = [tok for tok in _spacy_en.tokenizer(text) if not tok.text.isspace()]\n",
        "    text_tokens = [tok.text for tok in tokens]\n",
        "    return tokens, text_tokens\n",
        "\n",
        "\n",
        "def tokenize_and_join(text: string, jointoken=JOIN_TOKEN):\n",
        "    return jointoken.join(tokenize(text)[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5erwEEwFpLaL"
      },
      "source": [
        "print(tokenize_and_join(\"Lazy fox doesn't like to travel too far in this heat...\"))\n",
        "print(tokenize_and_join(\"Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. \"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8bidRKHmTUj"
      },
      "source": [
        "class SquadDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, data, fields: List[Tuple[str, data.Field]], cachedir='./drive/MyDrive/data/squad', **kwargs):\n",
        "        # download dataset, if needed\n",
        "        self.check_for_download(cachedir)\n",
        "        \n",
        "        f = os.path.join(cachedir, data)\n",
        "        print(f)\n",
        "        \n",
        "        # The preprocessed file will be named like the original but with _preprocessed.json suffix\n",
        "        preprocessed_f = f + \"_preprocessed.json\"\n",
        "        if not os.path.exists(preprocessed_f):\n",
        "            s_time = time.time()\n",
        "            \n",
        "            # Process examples from file\n",
        "            raw_examples = SquadDataset.get_example_list(f)\n",
        "            # Save preprocessed examples, so they do not have to be processed again\n",
        "            self.save(preprocessed_f, raw_examples)\n",
        "            print(f\"Dataset {preprocessed_f} created in {time.time() - s_time}s\")\n",
        "\n",
        "        s_time = time.time()\n",
        "        \n",
        "        # Load preprocessed examples\n",
        "        examples = self.load(preprocessed_f, fields)\n",
        "        print(f\"Dataset {preprocessed_f} loaded in {time.time() - s_time:.2f} s\")\n",
        "\n",
        "        super(SquadDataset, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "    def save(self, preprocessed_f: string, raw_examples: List[Dict])-> None:\n",
        "        \"\"\"\n",
        "        Dump examples into json with name given in preprocessed_f variable.\n",
        "        \"\"\"\n",
        "        with open(preprocessed_f, \"w\") as f:\n",
        "            json.dump(raw_examples, f)\n",
        "\n",
        "    def load(self, preprocessed_f: string, fields: List[Tuple[str, RawField]]) -> List[Example]:\n",
        "        \"\"\"\n",
        "        Load preprocessed examples and construct torchtext examples from them\n",
        "        \"\"\"\n",
        "        with open(preprocessed_f, \"r\") as f:\n",
        "            raw_examples = json.load(f)\n",
        "            return [data.Example.fromlist([\n",
        "                e[\"id\"],\n",
        "                e[\"topic\"],\n",
        "                e[\"paragraph_token_positions\"],\n",
        "                e[\"raw_paragraph_context\"],\n",
        "                e[\"paragraph_context\"],\n",
        "                e[\"paragraph_context\"],\n",
        "                e[\"paragraph_context\"],\n",
        "                e[\"question\"],\n",
        "                e[\"question\"],\n",
        "                e[\"question\"],\n",
        "                e[\"a_start\"],\n",
        "                e[\"a_end\"],\n",
        "                e[\"a_extracted\"],\n",
        "                e[\"a_gt\"]\n",
        "            ], fields) for e in raw_examples]\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, fields, cachedir='./drive/MyDrive/data/squad'):\n",
        "        \"\"\"\n",
        "        Creates train/validation data split\n",
        "        \"\"\"\n",
        "        train_data = cls(TRAIN, fields, cachedir=cachedir)\n",
        "        val_data = cls(VALIDATION, fields, cachedir=cachedir)\n",
        "        return tuple(d for d in (train_data, val_data)\n",
        "                     if d is not None)\n",
        "\n",
        "    @staticmethod\n",
        "    def check_for_download(cachedir:string):\n",
        "        \"\"\"\n",
        "        Downloads data, if possible\n",
        "        \"\"\"\n",
        "        if not os.path.exists(cachedir):\n",
        "            os.makedirs(cachedir)\n",
        "            try:\n",
        "                download_url(os.path.join(cachedir, TRAIN), TRAIN_V1_URL)\n",
        "                download_url(os.path.join(cachedir, VALIDATION), DEV_V1_URL)\n",
        "            except BaseException as e:\n",
        "                sys.stderr.write(f'Download failed, removing directory {cachedir}\\n')\n",
        "                sys.stderr.flush()\n",
        "                shutil.rmtree(cachedir)\n",
        "                \n",
        "                raise e\n",
        "        \n",
        "                \n",
        "    @staticmethod\n",
        "    def prepare_fields():\n",
        "        \"\"\"\n",
        "        Prepare torchtext fields for individual aspects of batch\n",
        "        \"\"\"\n",
        "        # field, that will process sequential text, will use vocabulary, will tokenize the text by splitting\n",
        "        # it on JOIN_TOKEN token and will lowercase the text\n",
        "        \n",
        "        # IMPORTANT: as the use_vocab=True, for this field (implicitly), the contents of this field will be automatically numericalized\n",
        "        # numericalization - the process of replacing words with their integer representations e.g.:\n",
        "        # [i, love, NLP] can be numericalized as [47,21,743]\n",
        "        WORD_field = data.Field(batch_first=True, tokenize=lambda s: str.split(s, sep=JOIN_TOKEN), lower=True)\n",
        "        \n",
        "        # field, that will not contain sequences, does not need vocabulary and will represent dependent target variable\n",
        "        TARGET_field = data.Field(sequential=False, use_vocab=False, batch_first=True, is_target=True)\n",
        "        \n",
        "        # raw field means, the field will not be processed at all\n",
        "        RAW_field = data.RawField()\n",
        "        RAW_field.is_target=False\n",
        "        return [\n",
        "            ('id', RAW_field),\n",
        "            ('topic_title', RAW_field),\n",
        "            ('document_token_positions', RAW_field),\n",
        "            ('raw_document_context', RAW_field),\n",
        "            ('document', WORD_field), # documents are processed as described with the WORD_field \n",
        "            ('document_char', RAW_field),\n",
        "            ('raw_document', RAW_field),\n",
        "            ('question', WORD_field), # questions are processed as described with the WORD_field \n",
        "            ('question_char', RAW_field),\n",
        "            ('raw_question', RAW_field),\n",
        "            # token indices of answer start and answer end are processed as described with the TARGET_field \n",
        "            (\"a_start\", TARGET_field),\n",
        "            (\"a_end\", TARGET_field),\n",
        "            \n",
        "            ('ext_answer', RAW_field),\n",
        "            ('gt_answer', RAW_field)\n",
        "        ]\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_fields_char():\n",
        "        WORD_field = data.Field(batch_first=True, tokenize=lambda s: str.split(s, sep=JOIN_TOKEN), lower=True)\n",
        "        \n",
        "        # This is field is applied on each unit of CHAR_nested_field, here we pass list as tokenize argument to split \n",
        "        # tokenized string into characters\n",
        "        CHAR_field = data.Field(batch_first=True, tokenize=list, lower=True)\n",
        "        CHAR_nested_field = data.NestedField(CHAR_field, tokenize=lambda s: str.split(s, sep=JOIN_TOKEN))\n",
        "            \n",
        "        RAW_field = data.RawField()\n",
        "        RAW_field.is_target=False\n",
        "        return [\n",
        "            ('id', RAW_field),\n",
        "            ('topic_title', RAW_field),\n",
        "            ('document_token_positions', RAW_field),\n",
        "            ('raw_document_context', RAW_field),\n",
        "            ('document', WORD_field),\n",
        "            ('document_char', CHAR_nested_field),\n",
        "            ('raw_document', RAW_field),\n",
        "            ('question', WORD_field),\n",
        "            ('question_char', CHAR_nested_field),\n",
        "            ('raw_question', RAW_field),\n",
        "            (\"a_start\", data.Field(sequential=False, use_vocab=False, batch_first=True, is_target=True)),\n",
        "            (\"a_end\", data.Field(sequential=False, use_vocab=False, batch_first=True, is_target=True)),\n",
        "            ('ext_answer', RAW_field),\n",
        "            ('gt_answer', RAW_field)\n",
        "        ]\n",
        "      \n",
        "    @staticmethod    \n",
        "    def get_example_list(file:string):\n",
        "        \"\"\"\n",
        "        Extracts processed examples from original dataset\n",
        "        \"\"\"\n",
        "        examples = []\n",
        "        cnt = 0\n",
        "\n",
        "        ## Open file for error reporting\n",
        "        f = open(f\"./drive/MyDrive/data/squad/errors_{os.path.basename(file)}.csv\", \"a+\")\n",
        "        problems = 0\n",
        "\n",
        "        # Iterate over examples in dataset\n",
        "        with open(file) as fd:\n",
        "            data_json = json.load(fd)\n",
        "            for data_topic in data_json[\"data\"]:\n",
        "                topic_title = data_topic[\"title\"]\n",
        "                for paragraph in data_topic[\"paragraphs\"]:\n",
        "                    # Tokenize document paragraph\n",
        "                    paragraph_tokens, paragraph_context = tokenize(paragraph[\"context\"])\n",
        "                    # Keep positions of each token in document, we will need this later, when decoding model outputs\n",
        "                    paragraph_token_positions = [[token.idx, token.idx + len(token.text)] for token in paragraph_tokens]\n",
        "                    \n",
        "                    joined_paragraph_context = JOIN_TOKEN.join(paragraph_context)\n",
        "                    for question_and_answers in paragraph['qas']:\n",
        "                        example_id = question_and_answers[\"id\"]\n",
        "                        question = tokenize_and_join(question_and_answers['question'])\n",
        "                        answers = question_and_answers['answers']\n",
        "\n",
        "                        for possible_answer in answers:\n",
        "                            answer_start_ch = possible_answer[\"answer_start\"]\n",
        "                            answer_end = possible_answer[\"answer_start\"] + len(possible_answer[\"text\"])\n",
        "                            answer_tokens, answer = tokenize(possible_answer[\"text\"])\n",
        "                            \n",
        "                            \n",
        "                            # Try finding answer in the document\n",
        "                            answer_locations = find_sub_list(answer, paragraph_context)\n",
        "                            \n",
        "                            # If we found multiple answer locations, we select the one, which is closest to the annotation\n",
        "                            if len(answer_locations) > 1:\n",
        "                                # get start character offset of each span\n",
        "                                answer_ch_starts = [paragraph_tokens[token_span[0]].idx for token_span in\n",
        "                                                    answer_locations]\n",
        "                                distance_from_gt = np.abs((np.array(answer_ch_starts) - answer_start_ch))\n",
        "                                closest_match = distance_from_gt.argmin()\n",
        "\n",
        "                                answer_start, answer_end = answer_locations[closest_match]\n",
        "                                \n",
        "                            # If we have not found answer in document, call heuristic from AllenNLP\n",
        "                            elif not answer_locations:\n",
        "                                # Call heuristic from AllenNLP to help :(\n",
        "                                token_span = char_span_to_token_span(\n",
        "                                    [(t.idx, t.idx + len(t.text)) for t in paragraph_tokens],\n",
        "                                    (answer_start_ch, answer_end))\n",
        "                                answer_start, answer_end = token_span[0]\n",
        "                                \n",
        "                            # Otherwise, everything is OK\n",
        "                            else:\n",
        "                                answer_start, answer_end = answer_locations[0]\n",
        "                            cnt += 1\n",
        "\n",
        "                            ## Check if the token span is correct\n",
        "                            ## write correct cases into csv\n",
        "                            def is_correct():\n",
        "                                def remove_ws(s):\n",
        "                                    return \"\".join(s.split())\n",
        "\n",
        "                                csvf = csv.writer(f, delimiter=',')\n",
        "                                if remove_ws(possible_answer[\"text\"]) != remove_ws(\n",
        "                                        \"\".join(paragraph_context[answer_start:answer_end + 1])):\n",
        "                                    csvf.writerow({\"id\": example_id,\n",
        "                                                   \"topic\": topic_title,\n",
        "                                                   \"raw_paragraph_context\": paragraph[\"context\"],\n",
        "                                                   \"paragraph_context\": joined_paragraph_context,\n",
        "                                                   \"paragraph_token_positions\": paragraph_token_positions,\n",
        "                                                   \"question\": question,\n",
        "                                                   \"a_start\": answer_start,\n",
        "                                                   \"a_end\": answer_end,\n",
        "                                                   \"a_extracted\": JOIN_TOKEN.join(\n",
        "                                                       paragraph_context[answer_start:answer_end + 1]),\n",
        "                                                   \"a_gt\": possible_answer[\"text\"]}.values())\n",
        "                                    return False\n",
        "                                return True\n",
        "\n",
        "                            if not is_correct():\n",
        "                                problems += 1\n",
        "\n",
        "                            examples.append({\"id\": example_id,\n",
        "                                             \"topic\": topic_title,\n",
        "                                             \"raw_paragraph_context\": paragraph[\"context\"],\n",
        "                                             \"paragraph_context\": joined_paragraph_context,\n",
        "                                             \"paragraph_token_positions\": paragraph_token_positions,\n",
        "                                             \"question\": question,\n",
        "                                             \"a_start\": answer_start,\n",
        "                                             \"a_end\": answer_end,\n",
        "                                             \"a_extracted\": JOIN_TOKEN.join(\n",
        "                                                 paragraph_context[answer_start:answer_end + 1]),\n",
        "                                             \"a_gt\": possible_answer[\"text\"]})\n",
        "\n",
        "            # print how many problems token-span mapping problems have occured\n",
        "            print(f\"# problems: {problems}\")\n",
        "            print(f\"Problems affect {problems/len(examples)/100:.5f} % of dataset.\")\n",
        "            return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMevccsQyfk-"
      },
      "source": [
        "# Borrowed from AllenNLP\n",
        "# https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/util.py\n",
        "def char_span_to_token_span(token_offsets: List[Tuple[int, int]],\n",
        "                            character_span: Tuple[int, int]) -> Tuple[Tuple[int, int], bool]:\n",
        "    \"\"\"\n",
        "    Converts a character span from a passage into the corresponding token span in the tokenized\n",
        "    version of the passage.  If you pass in a character span that does not correspond to complete\n",
        "    tokens in the tokenized version, we'll do our best, but the behavior is officially undefined.\n",
        "    We return an error flag in this case, and have some debug logging so you can figure out the\n",
        "    cause of this issue (in SQuAD, these are mostly either tokenization problems or annotation\n",
        "    problems; there's a fair amount of both).\n",
        "    The basic outline of this method is to find the token span that has the same offsets as the\n",
        "    input character span.  If the tokenizer tokenized the passage correctly and has matching\n",
        "    offsets, this is easy.  We try to be a little smart about cases where they don't match exactly,\n",
        "    but mostly just find the closest thing we can.\n",
        "    The returned ``(begin, end)`` indices are `inclusive` for both ``begin`` and ``end``.\n",
        "    So, for example, ``(2, 2)`` is the one word span beginning at token index 2, ``(3, 4)`` is the\n",
        "    two-word span beginning at token index 3, and so on.\n",
        "    Returns\n",
        "    -------\n",
        "    token_span : ``Tuple[int, int]``\n",
        "        `Inclusive` span start and end token indices that match as closely as possible to the input\n",
        "        character spans.\n",
        "    error : ``bool``\n",
        "        Whether the token spans match the input character spans exactly.  If this is ``False``, it\n",
        "        means there was an error in either the tokenization or the annotated character span.\n",
        "    \"\"\"\n",
        "    # We have token offsets into the passage from the tokenizer; we _should_ be able to just find\n",
        "    # the tokens that have the same offsets as our span.\n",
        "    error = False\n",
        "    start_index = 0\n",
        "    while start_index < len(token_offsets) and token_offsets[start_index][0] < character_span[0]:\n",
        "        start_index += 1\n",
        "    # start_index should now be pointing at the span start index.\n",
        "    if token_offsets[start_index][0] > character_span[0]:\n",
        "        # In this case, a tokenization or labeling issue made us go too far - the character span\n",
        "        # we're looking for actually starts in the previous token.  We'll back up one.\n",
        "        start_index -= 1\n",
        "    if token_offsets[start_index][0] != character_span[0]:\n",
        "        error = True\n",
        "    end_index = start_index\n",
        "    while end_index < len(token_offsets) and token_offsets[end_index][1] < character_span[1]:\n",
        "        end_index += 1\n",
        "    if token_offsets[end_index][1] != character_span[1]:\n",
        "        error = True\n",
        "    return (start_index, end_index), error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL61dKy2PI1C"
      },
      "source": [
        "class Baseline(torch.nn.Module):\n",
        "    # We would like to define all the submodules of our model in initializer\n",
        "    def __init__(self, config, vocab):\n",
        "        super().__init__()\n",
        "        # Embedder - module that constructs token embeddings from token indices\n",
        "        self.embedder = Embedder(vocab, config)\n",
        "        # Encoder - which encodes our word representations\n",
        "        self.encoder = torch.nn.LSTM(\n",
        "            config[\"RNN_input_dim\"],\n",
        "            config[\"RNN_nhidden\"],\n",
        "            config[\"RNN_layers\"],\n",
        "            dropout=float(config['dropout_rate']),\n",
        "            batch_first=True,\n",
        "            bidirectional=True)\n",
        "        # linear projections, which project max-pooled question representation\n",
        "        # into answer_start/ answer_end representing space\n",
        "        self.lin_S = nn.Linear(config[\"RNN_nhidden\"] * 2, config[\"RNN_nhidden\"] * 2)\n",
        "        self.lin_E = nn.Linear(config[\"RNN_nhidden\"] * 2, config[\"RNN_nhidden\"] * 2)\n",
        "        \n",
        "        # dropout - regularization\n",
        "        self.dropout = nn.Dropout(p=config[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, batch,return_max=False):      \n",
        "        # abbreviations and symbols:\n",
        "        # batch_size - size of a mini-batch\n",
        "        # d - embedding dimension\n",
        "        # q_len - the length of the longest query in mini-batch\n",
        "        # d_len - the length of the longest document in mini-batch\n",
        "        # RNN_out - the output dimension of RNN\n",
        "      \n",
        "        # 1. Transform token indices to token embeddings\n",
        "        # dimensions of query token indices: batch_size x q_len\n",
        "        # dimensions of document token indices: batch_size x d_len\n",
        "        # dimensions of query/document token embeddings:\n",
        "        # batch_size x q_len x d / batch_size x d_len x d\n",
        "        # print(\"\")\n",
        "        # print(\"batch_size x q_len \", batch.question.size())\n",
        "        # print(\"batch_size x d_len \", batch.document.size())\n",
        "        # print(\"\")\n",
        "\n",
        "        q_emb = self.embedder(batch.question)\n",
        "        d_emb = self.embedder(batch.document)\n",
        "\n",
        "        # print(\"batch_size x q_len x d \", q_emb.size())\n",
        "        # print(\"batch_size x d_len x d \", d_emb.size())\n",
        "        # print(\"\")\n",
        "        # MIND that you can access documents/questions in batch via batch.document, batch.question\n",
        "        \n",
        "        # 2. Call the encoder, pass question / document representation to it\n",
        "        # batch_size x q_len x RNN_out / batch_size x d_len x RNN_out\n",
        "        # do not forget to apply dropout!\n",
        "        q_enc, _ = self.encoder(self.dropout(q_emb))\n",
        "        d_enc, _ = self.encoder(self.dropout(d_emb))\n",
        "        # print(\"batch_size x q_len x RNN_out \", q_enc.size())\n",
        "        # print(\"batch_size x d_len x RNN_out \", d_enc.size())\n",
        "        # print(\"\")\n",
        "        \n",
        "        # 3. Pick the maximum over time (dimension q_len) out of encoded question representations q_enc\n",
        "        # batch_size x RNN_out\n",
        "        # q = q_enc.max(dim=-2)[0]\n",
        "        # argmax_q = q_enc.argmax(dim=-2)[0]\n",
        "        q, argmax_q = torch.max(q_enc, dim=1)\n",
        "        # print(\"max-size - batch_size x RNN_out\", q.size())\n",
        "        # print(\"argmax-size - batch_size x RNN_out\", argmax_q.size())\n",
        "        # print(\"\")\n",
        "\n",
        "        # 4. Project the max-pooled representation in start/end seeking space\n",
        "        # do not forget to apply dropout!\n",
        "        # both of shape batch_size x RNN_out\n",
        "        q_s = self.dropout(self.lin_S(q))\n",
        "        q_e = self.dropout(self.lin_E(q))\n",
        "        # print(\"batch_size x RNN_out \", q_s.size())\n",
        "        # print(\"batch_size x RNN_out \", q_e.size())\n",
        "        # print(\"\")\n",
        "        \n",
        "        # 5. Change the shape of q_s, q_e to be:\n",
        "        # batch_size x RNN_out x 1\n",
        "\n",
        "        q_s.unsqueeze_(-1), q_e.unsqueeze_(-1)\n",
        "        # print(\"batch_size x RNN_out x 1 \", q_s.size())\n",
        "        # print(\"batch_size x RNN_out x 1 \", q_e.size())\n",
        "        \n",
        "        # 6. Attention \n",
        "        # do the batch-wise matrix multiplication between:\n",
        "        # - q_s and encoded document contents d_enc obtaining unnormalized log probabilities of \n",
        "        # answer start\n",
        "        # - q_e and encoded document contents d_enc obtaining unnormalized log probabilities of\n",
        "        # answer end\n",
        "        # both of shape batch_size x d_len x 1\n",
        "\n",
        "        # print(\"indamodel6\")\n",
        "        # print(d_enc.size())\n",
        "        # print(q_s.size())\n",
        "        s = torch.bmm(d_enc, q_s)\n",
        "        e = torch.bmm(d_enc, q_e)\n",
        "        \n",
        "        # 7. Reshape start / end representations to be batch_size x d_len\n",
        "        # and return the unnormalized log probabilities\n",
        "        # !Note that softmax is applied to these inside the CrossEntropyLoss error function.\n",
        "\n",
        "        s.squeeze_(-1), e.squeeze_(-1) \n",
        "        # print(\"outtadamodel\")\n",
        "        \n",
        "        \n",
        "        if return_max:\n",
        "          return s, e, argmax_q\n",
        "        return s, e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwQy375WPXV0"
      },
      "source": [
        "# Token indices to token embeddings\n",
        "# Next, we would like to implement submodule of our model, which returns embeddings of each input token. We can simply implement it like following:\n",
        "class Embedder(torch.nn.Module):\n",
        "    def __init__(self, vocab, config):\n",
        "        super().__init__()\n",
        "        # Whether to scale gradient for embeddings by its frequency\n",
        "        # (talk to me to explain this further)\n",
        "        self.scale_grad = config['scale_emb_grad_by_freq']\n",
        "        \n",
        "        self.init_vocab(vocab, config['optimize_embeddings'])\n",
        "        print(f\"Optimize embeddings = {config['optimize_embeddings']}\")\n",
        "        print(f\"Scale grad by freq: {self.scale_grad}\")\n",
        "        print(f\"Vocabulary size = {len(vocab.vectors)}\")\n",
        "\n",
        "    def init_vocab(self, vocab, optimize_embeddings=False, device=None):\n",
        "        self.embedding_dim = vocab.vectors.shape[1]\n",
        "        # Create an torch.nn.Embedding abstraction\n",
        "        self.embeddings = torch.nn.Embedding(len(vocab), self.embedding_dim, scale_grad_by_freq=self.scale_grad)\n",
        "        \n",
        "        # Copy over the pre-trained GloVe embeddings\n",
        "        self.embeddings.weight.data.copy_(vocab.vectors)\n",
        "        self.embeddings.weight.requires_grad = optimize_embeddings\n",
        "        \n",
        "        # Save also vocab, so we can access it later when loading the model if needed\n",
        "        self.vocab = vocab\n",
        "        \n",
        "        # map to gpu\n",
        "        if device is not None:\n",
        "            self.embeddings = self.embeddings.to(device)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.embeddings(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jNBmwNTRPO6"
      },
      "source": [
        "def decode(span_start_logits: torch.Tensor, span_end_logits: torch.Tensor) -> \\\n",
        "        Tuple[torch.Tensor,Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    This method has been borrowed from AllenNLP\n",
        "    :param span_start_logits: unnormalized start log probabilities\n",
        "    :param span_end_logits: unnormalized end log probabilities\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # We call the inputs \"logits\" - they could either be unnormalized logits or normalized log\n",
        "    # probabilities.  A log_softmax operation is a constant shifting of the entire logit\n",
        "    # vector, so taking an argmax over either one gives the same result.\n",
        "    if span_start_logits.dim() != 2 or span_end_logits.dim() != 2:\n",
        "        raise ValueError(\"Input shapes must be (batch_size, document_length)\")\n",
        "    batch_size, passage_length = span_start_logits.size()\n",
        "    device = span_start_logits.device\n",
        "   \n",
        "  \n",
        "    \n",
        "    # span_start_logits.unsqueeze(2) has shape:\n",
        "    # (batch_size, passage_length, 1)\n",
        "    \n",
        "    # span_end_logits.unsqueeze(1) has shape:\n",
        "    # (batch_size, 1, passage_length)\n",
        "    \n",
        "    # Addition in log-domain = multiplication in real domain\n",
        "    # This will create a matrix containing addition of each span_start_logit with span_end_logit\n",
        "    # (batch_size, passage_length, passage_length)\n",
        "    span_log_probs = span_start_logits.unsqueeze(2) + span_end_logits.unsqueeze(1)\n",
        "    \n",
        "    # Only the upper triangle of the span matrix is valid; the lower triangle has entries where\n",
        "    # the span ends before it starts. We will mask these values out\n",
        "    span_log_mask = torch.triu(torch.ones((passage_length, passage_length),\n",
        "                                          device=device)).log().unsqueeze(0)\n",
        "    # The mask will look like this\n",
        "    #0000000\n",
        "    #X000000\n",
        "    #XX00000\n",
        "    #XXX0000\n",
        "    #XXXX000\n",
        "    #XXXXX00\n",
        "    #XXXXXX0\n",
        "    # where X are -infinity\n",
        "    valid_span_log_probs = span_log_probs + span_log_mask # see image above, part 1.\n",
        "        \n",
        "    \n",
        "    # Here we take the span matrix and flatten it, then find the best span using argmax.  We\n",
        "    # can recover the start and end indices from this flattened list using simple modular\n",
        "    # arithmetic.\n",
        "    # (batch_size, passage_length * passage_length)\n",
        "    # valid_span_log_probs is a vector [s_00,s_01,...,s_0n,s10,s11,...,s1n, ... , sn0,sn1,..., snn] of span scores\n",
        "    # e.g. s_01 is a score of answer span from token 0 to token 1\n",
        "    valid_span_log_probs = valid_span_log_probs.view(batch_size, -1) # see image above, part 2.\n",
        "    \n",
        "    # Turn all the log-probabilities into probabilities\n",
        "    logprobs = valid_span_log_probs\n",
        "    valid_span_probs = F.softmax(valid_span_log_probs, dim=-1)\n",
        "\n",
        "    best_span_probs, best_spans = valid_span_probs.max(-1) # see image above, part 3.\n",
        "    logprobs, _ = logprobs.max(-1)\n",
        "    # best_span_probs of shape batch_size now contains all probabilities for each best span in the batch\n",
        "    # best_spans of shape batch_size now contains argmaxes of each answer from unrolled sequence valid_span_log_probs\n",
        "    \n",
        "    span_start_indices = best_spans // passage_length\n",
        "    span_end_indices = best_spans % passage_length\n",
        "\n",
        "    return best_span_probs, (span_start_indices, span_end_indices), logprobs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgbnvQEGXD7"
      },
      "source": [
        "def get_spans(batch, candidates):\n",
        "  r = []\n",
        "  for i in range(len(batch.raw_document_context)):\n",
        "      candidate_start = candidates[0][i]\n",
        "      candidates_end = candidates[1][i]\n",
        "      \n",
        "      # In initial state of learning, we can predict the start/end in the padding area\n",
        "      # since we do not do the masking\n",
        "      # We will fix that here.\n",
        "      if candidate_start > len(batch.document_token_positions[i]) - 1:\n",
        "          candidate_start = len(batch.document_token_positions[i]) - 1\n",
        "      if candidates_end > len(batch.document_token_positions[i]) - 1:\n",
        "          candidates_end = len(batch.document_token_positions[i]) - 1\n",
        "      \n",
        "      # If everything is OK, append (character_start,character_end) of answer span to r\n",
        "      r.append(batch.raw_document_context[i][batch.document_token_positions[i][candidate_start][0]:\n",
        "                                             batch.document_token_positions[i][candidates_end][-1]])\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD04wGkeP2dR"
      },
      "source": [
        "def train_epoch(model: torch.nn.Module, lossfunction: _Loss, optimizer: torch.optim.Optimizer,\n",
        "              train_iter: Iterator,gradient_clipping_norm = 5.) -> float:\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  # set gradients for all parameters to 0\n",
        "  optimizer.zero_grad()\n",
        "  for i, batch in enumerate(train_iter):\n",
        "      # get the unnormalized log probabilities\n",
        "      logprobs_S, logprobs_E = model(batch)\n",
        "      \n",
        "      # compute the (cross-entropy) loss for start and end separately\n",
        "      loss_s = lossfunction(logprobs_S, batch.a_start)\n",
        "      loss_e = lossfunction(logprobs_E, batch.a_end)\n",
        "      loss = loss_s + loss_e\n",
        "      \n",
        "      loss.backward() # compute gradients\n",
        "      torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), gradient_clipping_norm) # clip the gradients\n",
        "      optimizer.step() # add portion of negative gradients to model parameters\n",
        "      optimizer.zero_grad() # set gradients for all parameters to 0\n",
        "\n",
        "      train_loss += loss.item() # .item() returns integer value from 0-dimensional torch tensor (scalar).\n",
        "\n",
        "      if i % 300 == 0 and i > 0:\n",
        "          print(f\"Training loss: {train_loss / i + 1}\")\n",
        "\n",
        "  return train_loss / len(train_iter.data())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0k5D00oGaDR"
      },
      "source": [
        "@torch.no_grad() # do not create computational graph in this method, this saves memory\n",
        "def validate(model: torch.nn.Module, lossfunction: _Loss, iter: Iterator,log_results=False) -> \\\n",
        "      Tuple[float, float, float]:\n",
        "  # turn on evaluation mode (disable dropout etc.)\n",
        "  model.eval()\n",
        "\n",
        "  # initialize variables\n",
        "  results = dict()\n",
        "  ids = []\n",
        "  lossvalues = []\n",
        "  spans = []\n",
        "  gt_spans = []\n",
        "  span_probs = []\n",
        "  \n",
        "  # iterate over validation set\n",
        "  for i, batch in enumerate(iter):\n",
        "      ids += batch.id\n",
        "      \n",
        "      # get predictions\n",
        "      logprobs_S, logprobs_E = model(batch)\n",
        "      # compute loss\n",
        "      loss_s = lossfunction(logprobs_S, batch.a_start)\n",
        "      loss_e = lossfunction(logprobs_E, batch.a_end)\n",
        "      loss = loss_s + loss_e\n",
        "      \n",
        "      # save loss values into list, we compute loss for each answer position\n",
        "      # but later we will pick only the best prediction\n",
        "      lossvalues += loss.tolist()\n",
        "      \n",
        "      # decode from log probabilities to predictions\n",
        "      best_span_probs, candidates, _ = decode(logprobs_S, logprobs_E)\n",
        "      span_probs += best_span_probs.tolist()\n",
        "      spans += get_spans(batch, candidates)\n",
        "      gt_spans += batch.gt_answer\n",
        "\n",
        "  # compute the final loss and results\n",
        "  # we need to filter through multiple possible choices and pick the best one\n",
        "  lossdict = defaultdict(lambda: math.inf)\n",
        "  probs = defaultdict(lambda: 0)\n",
        "  for id, value, span, span_prob in zip(ids, lossvalues, spans, span_probs):\n",
        "      # record only lowest loss\n",
        "      if lossdict[id] > value:\n",
        "          lossdict[id] = value\n",
        "      # record predicted result\n",
        "      results[id] = span\n",
        "      # record probability of predicted result\n",
        "      probs[id] = span_prob\n",
        "  \n",
        "  # results logging \n",
        "  if log_results:\n",
        "      write_results(results, probs)\n",
        "  \n",
        "  # compute loss from best answer predictions\n",
        "  loss = sum(lossdict.values()) / len(lossdict)\n",
        "  \n",
        "  # write out predictions for evaluation script\n",
        "  prediction_file = f\"./drive/MyDrive/data/squad/dev_results_{socket.gethostname()}.json\"\n",
        "  with open(prediction_file, \"w\") as f:\n",
        "      json.dump(results, f)\n",
        "\n",
        "  # initialize arguments of evaluation script\n",
        "  dataset_file = \"./drive/MyDrive/data/squad/dev-v1.1.json\"\n",
        "  expected_version = '1.1'\n",
        "  with open(dataset_file) as dataset_file:\n",
        "      dataset_json = json.load(dataset_file)\n",
        "      if (dataset_json['version'] != expected_version):\n",
        "          print('Evaluation expects v-' + expected_version +\n",
        "                       ', but got dataset with v-' + dataset_json['version'],\n",
        "                       file=sys.stderr)\n",
        "      dataset = dataset_json['data']\n",
        "  with open(prediction_file) as prediction_file:\n",
        "      predictions = json.load(prediction_file)\n",
        "  # run the evaluation script\n",
        "  result = evaluate(dataset, predictions)\n",
        "  \n",
        "\n",
        "  return loss, result[\"exact_match\"], result[\"f1\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7_zWo0jGisA"
      },
      "source": [
        "def fit(config, device):\n",
        "  # print configuration\n",
        "  print(json.dumps(config, indent=4, sort_keys=True))\n",
        "\n",
        "  # prepare torchtext fields (different in case of character embeddings)\n",
        "  if config[\"char_embeddings\"]:\n",
        "      fields = SquadDataset.prepare_fields_char()\n",
        "  else:\n",
        "      fields = SquadDataset.prepare_fields()\n",
        "  \n",
        "  # create train/validation datasets\n",
        "  train, val = SquadDataset.splits(fields)\n",
        "  fields = dict(fields)\n",
        "  \n",
        "  # we use the same field for question and document\n",
        "  # we can build vocabulary of words it represents by calling build_vocab [this takes a while]\n",
        "  # for each used word, we can pick the glove embedding and create an embedding matrix with index to embedding mapping\n",
        "  fields[\"question\"].build_vocab(train, val, vectors=GloVe(name='6B', dim=config[\"embedding_size\"]))\n",
        "  \n",
        "  # similarly, we can build character vocabulary, if needed\n",
        "  if not type(fields[\"question_char\"]) == torchtext.data.field.RawField:\n",
        "      fields[\"question_char\"].build_vocab(train, val, max_size=config[\"char_maxsize_vocab\"])\n",
        "\n",
        "  # shuffle the examples to get the best distribution estimate\n",
        "  train_iter = BucketIterator(train, sort_key=lambda x: -(len(x.question) + len(x.document)),\n",
        "                              shuffle=True, sort=False, sort_within_batch=True,\n",
        "                              batch_size=config[\"train_batch_size\"], train=True,\n",
        "                              repeat=False,\n",
        "                              device=device)\n",
        "  \n",
        "  # sort validation examples for faster validation\n",
        "  val_iter = BucketIterator(val, sort_key=lambda x: -(len(x.question) + len(x.document)), sort=True,\n",
        "                            batch_size=config[\"validation_batch_size\"],\n",
        "                            repeat=False,\n",
        "                            device=device)\n",
        "  # create model\n",
        "  model = Baseline(config, fields[\"question\"].vocab).to(device)\n",
        "  \n",
        "  # create optimizer\n",
        "  optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                   lr=config[\"learning_rate\"])\n",
        "\n",
        "  start_time = time.time()\n",
        "  best_model = None\n",
        "  try:\n",
        "      best_val_loss = math.inf\n",
        "      best_val_f1 = 0\n",
        "      best_em = 0\n",
        "      no_improvement = 0\n",
        "      for it in range(config[\"max_iterations\"]):\n",
        "          print(f\"Iteration {it}\")\n",
        "          # run training epoch\n",
        "          train_epoch(model, CrossEntropyLoss(), optimizer, train_iter)\n",
        "          \n",
        "          # validate model\n",
        "          validation_loss, em, f1 = validate(model, CrossEntropyLoss(reduction='none'), val_iter)     \n",
        "          print(f\"Validation loss/F1/EM: {validation_loss:.2f}, {f1:.2f}, {em:.2f}\")\n",
        "          \n",
        "          # increment the patience counter\n",
        "          no_improvement+=1\n",
        "          \n",
        "          # Update the best statistics if needed\n",
        "          if validation_loss < best_val_loss: \n",
        "              best_val_loss = validation_loss\n",
        "              no_improvement = 0\n",
        "          if f1 > best_val_f1: \n",
        "            best_val_f1 = f1\n",
        "          if em > best_em: \n",
        "            best_em = em\n",
        "            model = model.to(torch.device(\"cpu\"))\n",
        "            best_model = copy.deepcopy(model)\n",
        "            model = model.to(torch.device(\"cuda\"))\n",
        "          print(f\"BEST L/F1/EM = {best_val_loss:.2f}/{best_val_f1:.2f}/{best_em:.2f}\")\n",
        "          \n",
        "          # Early stopping\n",
        "          # if the validation loss did not improved for several iterations\n",
        "          # the training is finished\n",
        "          if no_improvement>=config[\"patience\"]:\n",
        "            break\n",
        "       \n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "      print('-' * 120)\n",
        "      print('Exit from training early.')\n",
        "  finally:\n",
        "      print(f'Finished after {(time.time() - start_time) / 60} minutes.')\n",
        "      return best_model,best_val_loss, best_val_f1, best_em"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w8jiPOS4fuw"
      },
      "source": [
        "!ls -a\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgl9M3Zdt96"
      },
      "source": [
        "baseline_config = {\"modelname\": \"baseline\",\n",
        "                   \"train_batch_size\": 40,\n",
        "                   \"validation_batch_size\": 128,\n",
        "                   \"embedding_size\": 100,\n",
        "                   \"optimize_embeddings\": False,\n",
        "                   \"scale_emb_grad_by_freq\": False,\n",
        "                   \"RNN_input_dim\": 100,\n",
        "                   \"dropout_rate\": 0.2,\n",
        "                   \"RNN_nhidden\": 256,\n",
        "                   \"learning_rate\": 5e-3,\n",
        "                   \"RNN_layers\": 4,\n",
        "                   \"max_iterations\": 100,\n",
        "                   \"optimizer\": \"adam\",\n",
        "                   \"patience\":2,\n",
        "                   \"char_embeddings\": False}\n",
        "\n",
        "model, validation_loss, f1, em = fit(baseline_config, device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JaSxK0ycAme"
      },
      "source": [
        "baseline_config = {\"modelname\": \"baseline\",\r\n",
        "                   \"train_batch_size\": 40,\r\n",
        "                   \"validation_batch_size\": 128,\r\n",
        "                   \"embedding_size\": 100,\r\n",
        "                   \"optimize_embeddings\": False,\r\n",
        "                   \"scale_emb_grad_by_freq\": False,\r\n",
        "                   \"RNN_input_dim\": 100,\r\n",
        "                   \"dropout_rate\": 0.1,\r\n",
        "                   \"RNN_nhidden\": 200,\r\n",
        "                   \"learning_rate\": 5e-3,\r\n",
        "                   \"RNN_layers\": 4,\r\n",
        "                   \"max_iterations\": 100,\r\n",
        "                   \"optimizer\": \"adam\",\r\n",
        "                   \"patience\":2,\r\n",
        "                   \"char_embeddings\": False}\r\n",
        "                   \r\n",
        "def revalidate(config):\r\n",
        "  # validate model\r\n",
        "  # print configuration\r\n",
        "  print(json.dumps(config, indent=4, sort_keys=True))\r\n",
        "\r\n",
        "  # prepare torchtext fields (different in case of character embeddings)\r\n",
        "  if config[\"char_embeddings\"]:\r\n",
        "      fields = SquadDataset.prepare_fields_char()\r\n",
        "  else:\r\n",
        "      fields = SquadDataset.prepare_fields()\r\n",
        "  \r\n",
        "  # create train/validation datasets\r\n",
        "  train, val = SquadDataset.splits(fields)\r\n",
        "  fields = dict(fields)\r\n",
        "  \r\n",
        "  # we use the same field for question and document\r\n",
        "  # we can build vocabulary of words it represents by calling build_vocab [this takes a while]\r\n",
        "  # for each used word, we can pick the glove embedding and create an embedding matrix with index to embedding mapping\r\n",
        "  fields[\"question\"].build_vocab(train, val, vectors=GloVe(name='6B', dim=config[\"embedding_size\"]))\r\n",
        "  \r\n",
        "  # sort validation examples for faster validation\r\n",
        "  val_iter = BucketIterator(val, sort_key=lambda x: -(len(x.question) + len(x.document)), sort=True,\r\n",
        "                            batch_size=config[\"validation_batch_size\"],\r\n",
        "                            repeat=False,\r\n",
        "                            device=device)\r\n",
        "  validation_loss, em, f1 = validate(model, CrossEntropyLoss(reduction='none'), val_iter)     \r\n",
        "  print(f\"Validation loss/F1/EM: {validation_loss:.2f}, {f1:.2f}, {em:.2f}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3o4RnCe_5K"
      },
      "source": [
        "revalidate(baseline_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31IV1omUfXNP"
      },
      "source": [
        "# validate n stuff\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvRWyxkFdKPX"
      },
      "source": [
        "if not os.path.isdir(\"saved\"):\n",
        "  os.mkdir(\"saved\")\n",
        "torch.save(model, \"./drive/MyDrive/data/saved/model_saved_4layers_256hidden_40tB_100embb.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A1KQGXAkbdZ"
      },
      "source": [
        "# Uncomment to download and load pre-trained model\n",
        "#!wget -P \"saved/\" -nc \"www.stud.fit.vutbr.cz/~ifajcik/bissit19/checkpoint_Baseline_EM_28.33_F1_39.01_L_4.78.pt\"\n",
        "model = torch.load(\"./drive/MyDrive/data/saved/model_saved_4layers200hidden.pt\")\n",
        "model = model.to(torch.device(\"cuda\"))\n",
        "model = model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv7PYyFzhLWS"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  \n",
        "print(f\"Models has {count_parameters(model)} parameters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72C2xPKUMa06"
      },
      "source": [
        "def write_results(results, probs, val_file=\"./drive/MyDrive/data/squad/dev-v1.1.json\"):\n",
        "  if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "  f = open(f\"results/result_{get_timestamp()}_{socket.gethostname()}.csv\", mode=\"w\")\n",
        "  csvw = csv.writer(f, delimiter=',')\n",
        "  HEADER = [\"Correct\", \"Ground Truth(s)\", \"Prediction\", \"Confidence\", \"Question\", \"Context\", \"Topic\", \"ID\"]\n",
        "  csvw.writerow(HEADER)\n",
        "  with open(val_file) as fd:\n",
        "      data_json = json.load(fd)\n",
        "      for data_topic in data_json[\"data\"]:\n",
        "          for paragraph in data_topic[\"paragraphs\"]:\n",
        "              for question_and_answers in paragraph['qas']:\n",
        "                  prediction = results[question_and_answers[\"id\"]]\n",
        "                  confidence = str(f\"{probs[question_and_answers['id']]:.2f}\")\n",
        "                  answers = \"|\".join(map(lambda x: x['text'], question_and_answers['answers']))\n",
        "                  correct = int(results[question_and_answers[\"id\"]].lower() in map(lambda x: x['text'].lower(),\n",
        "                                                                                   question_and_answers[\n",
        "                                                                                       'answers']))\n",
        "                  ex = [correct,\n",
        "                        answers,\n",
        "                        prediction,\n",
        "                        confidence,\n",
        "                        question_and_answers['question'],\n",
        "                        paragraph[\"context\"],\n",
        "                        data_topic[\"title\"],\n",
        "                        question_and_answers[\"id\"]]\n",
        "                  csvw.writerow(ex)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whlGzus9Z4Tx"
      },
      "source": [
        "# GoogleTrans\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pziR4AjVGeVR"
      },
      "source": [
        "res = retrieve(\"Kdy se porařilo první prokazatelné dosažení severního pólu ?\")\r\n",
        "for result in res:\r\n",
        "  print(result)\r\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgwTXx4EsO6n"
      },
      "source": [
        "# get majka database\r\n",
        "!curl --remote-name-all https://nlp.fi.muni.cz/ma{/majka.w-lt}\r\n",
        "!mv majka.w-lt drive/MyDrive/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkccRML9JPBf"
      },
      "source": [
        "# download czech squad\r\n",
        "!curl --remote-name-all https://lindat.cz/repository/xmlui/bitstream/handle/11234/1-3069{/sqad_v3.tar.xz}\r\n",
        "!mv sqad_v3.tar.xz drive/MyDrive/data/\r\n",
        "!tar -xf drive/MyDrive/data/sqad_v3.tar.xz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEk8aPGcXPXw"
      },
      "source": [
        "!pip install googletrans==4.0.0-rc1\r\n",
        "!pip install wikipedia\r\n",
        "!pip install rank_bm25\r\n",
        "!pip install majka\r\n",
        "\r\n",
        "from googletrans import Translator\r\n",
        "translator = Translator()\r\n",
        "import wikipedia\r\n",
        "wikipedia.set_lang(\"cs\") \r\n",
        "from rank_bm25 import BM25Okapi\r\n",
        "import re\r\n",
        "import majka"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Ypv6zfMga-"
      },
      "source": [
        "wiki retriever\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrJsP4y_p7P-"
      },
      "source": [
        "def extract_que_ans(dirnum, filename):\r\n",
        "  # get question and answer from czech squad \r\n",
        "  f = open(f\"drive/MyDrive/data/cz_sqad/data/{dirnum}/{filename}\", \"r\")\r\n",
        "  q = f.read().split(\"\\n\")\r\n",
        "  question = \"\"\r\n",
        "\r\n",
        "  for line in q:\r\n",
        "    line = line.split(\"\\t\")[0]\r\n",
        "    if line in {\"<s>\", \"<g/>\", \"</s>\"}:\r\n",
        "      continue\r\n",
        "    question += line + \" \"\r\n",
        "\r\n",
        "  f.close()\r\n",
        "  return question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPfhmPY-xO85"
      },
      "source": [
        "# save the most common czech words\r\n",
        "common = \"být a se v na ten on že s z který mít do já o k i jeho ale svůj jako za moci rok pro tak po tento co když všechen už jak aby od nebo říci jeden jen můj jenž člověk ty stát u muset velký chtít také až než ještě při jít pak před dva však ani vědět nový hodně podle další celý jiný mezi dát tady den tam kde doba každý místo dobrý takový strana protože nic začít něco vidět říkat ne sám bez či dostat nějaký proto\"\r\n",
        "common = common.split()\r\n",
        "punctuation = \". , ? ! ... \\\" ( ) ; - /\"\r\n",
        "punctuation = punctuation.split()\r\n",
        "\r\n",
        "def iscommon(x):\r\n",
        "  if x in common or x in punctuation:\r\n",
        "    return True\r\n",
        "  else:\r\n",
        "    return False\r\n",
        "\r\n",
        "def delete_common(tokens):\r\n",
        "  tokens = [x for x in tokens if not iscommon(x)]\r\n",
        "      \r\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oNdBO3PCOjI"
      },
      "source": [
        "def search_again(tokens):\r\n",
        "\r\n",
        "  searched_term = (' ').join(tokens)\r\n",
        "  doc_list = wikipedia.search(searched_term)\r\n",
        "\r\n",
        "  if len(tokens) == 0:\r\n",
        "    return []\r\n",
        "\r\n",
        "  if len(doc_list) == 0:\r\n",
        "    del tokens[0]\r\n",
        "    return search_again(tokens)\r\n",
        "\r\n",
        "  return doc_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXpTErpEr1hN"
      },
      "source": [
        "morph = majka.Majka('drive/MyDrive/data/majka.w-lt')\r\n",
        "morph.flags |= majka.ADD_DIACRITICS  # find word forms with diacritics\r\n",
        "morph.flags |= majka.DISALLOW_LOWERCASE  # do not enable to find lowercase variants\r\n",
        "morph.flags |= majka.IGNORE_CASE  # ignore the word case whatsoever\r\n",
        "morph.flags = 0  # unset all flags\r\n",
        "\r\n",
        "morph.tags = False  # return just the lemma, do not process the tags\r\n",
        "morph.first_only = True  # return only the first entry\r\n",
        "morph.negative = \"ne\"\r\n",
        "\r\n",
        "# returns lemma of each token in a list of lemmatized tokens\r\n",
        "def lemmatize(text):\r\n",
        "\r\n",
        "  tok_text = text.lower()\r\n",
        "  tok_text = re.split(\"\\W\", text)\r\n",
        "\r\n",
        "  # lemmatize each token\r\n",
        "  lemmatized_tokens = []\r\n",
        "  for token in tok_text:\r\n",
        "    if token == '':\r\n",
        "      continue\r\n",
        "    lemma = morph.find(token)\r\n",
        "    if len(lemma) == 0:\r\n",
        "      lemmatized_tokens.append(token)\r\n",
        "    else:\r\n",
        "      lemmatized_tokens.append(lemma[0]['lemma'])\r\n",
        "\r\n",
        "  return lemmatized_tokens\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjYaStGxV7tw"
      },
      "source": [
        "def retrieve(question):  \r\n",
        "\r\n",
        "  #search for documents\r\n",
        "  doc_list = wikipedia.search(question)\r\n",
        "\r\n",
        "  # simplify the search if its too bad\r\n",
        "  if len(doc_list) == 0:\r\n",
        "    # extract important for wiki\r\n",
        "    tokens = lemmatize(question)\r\n",
        "    tokens = delete_common(tokens)\r\n",
        "    doc_list = search_again(tokens)\r\n",
        "    \r\n",
        "  if len(doc_list) == 0:\r\n",
        "      return \"\"\r\n",
        "\r\n",
        "  # split docs into paragraphs\r\n",
        "  pars = []\r\n",
        "  max_docs = 3\r\n",
        "  num_docs = 0\r\n",
        "\r\n",
        "  for doc in doc_list:\r\n",
        "    # get whole page content\r\n",
        "    try:\r\n",
        "      doc = wikipedia.page(doc)\r\n",
        "    except wikipedia.DisambiguationError as e:\r\n",
        "      s = e.options[0]\r\n",
        "      doc = wikipedia.page(s)\r\n",
        "    result = re.split('== .*. ==|\\n\\n', doc.content)\r\n",
        "\r\n",
        "    # save stripped paragraphs\r\n",
        "    for par in result:\r\n",
        "      par = par.strip()\r\n",
        "      par = par.strip('=')\r\n",
        "      par = par.strip('\\n')\r\n",
        "      par = par.strip('\\r\\n')\r\n",
        "\r\n",
        "      if par == '' or par == '\\n':\r\n",
        "        continue\r\n",
        "      pars.append(par)\r\n",
        "\r\n",
        "    num_docs += 1\r\n",
        "    if num_docs >= max_docs:\r\n",
        "      break\r\n",
        "\r\n",
        "  # tokenize for bm25\r\n",
        "  tok_text = []\r\n",
        "  for par in pars:\r\n",
        "    tok_par = par.lower()\r\n",
        "    tok_par = re.split(\"\\W\", tok_par)\r\n",
        "    for tok in tok_par:\r\n",
        "      if tok == \"\":\r\n",
        "        tok_par.remove(\"\")\r\n",
        "    tok_text.append(tok_par)\r\n",
        "\r\n",
        "  # build index\r\n",
        "  bm25 = BM25Okapi(tok_text)\r\n",
        "\r\n",
        "  # tokenize the query\r\n",
        "  tokenized_query = question.lower()\r\n",
        "  tokenized_query = re.split(\"\\W\", tokenized_query)\r\n",
        "\r\n",
        "  # get results\r\n",
        "  results = bm25.get_top_n(tokenized_query, pars, n=3)\r\n",
        "\r\n",
        "  return results\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4saxh9bfoqYe"
      },
      "source": [
        "# create very simple batch object\r\n",
        "class simple_batch():\r\n",
        "  def __init__(self,q,d,raw_d,d_pos):\r\n",
        "    map_to_gpu_tensor = lambda x: torch.Tensor(x).long().to(torch.device(\"cuda\")).unsqueeze(0)\r\n",
        "    self.question=map_to_gpu_tensor(q)\r\n",
        "    self.document=map_to_gpu_tensor(d)\r\n",
        "    self.raw_document_context = [raw_d]\r\n",
        "    self.document_token_positions = [d_pos]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIPCvVz8pq_d"
      },
      "source": [
        "final pipeline\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn5_5_YKeC0w"
      },
      "source": [
        "f = open(\"drive/MyDrive/data/saved_answers/test.txt\", \"w\")\n",
        "\n",
        "# write first question-answer pairs in sqad\n",
        "for i in range(1, 2):\n",
        "  name = \"\"\n",
        "  for _ in range(len(str(i)), 6):\n",
        "    name += \"0\"\n",
        "  name += str(i)\n",
        "\n",
        "  # extract from dataset\n",
        "  question = extract_que_ans(name, \"01question.vert\")\n",
        "  correct_answer = extract_que_ans(name, \"09answer_extraction.vert\")\n",
        "\n",
        "  # wiki search\n",
        "  documents = retrieve(question)\n",
        "  bestAnswers = []\n",
        "  bestProbs = []\n",
        "  bestDocs = []\n",
        "  bestLogProbs = []\n",
        "\n",
        "  question_cs = question # save czech question\n",
        "\n",
        "  # iterate over retrieved paragraphs\n",
        "  for document in documents:\n",
        "    document = document.strip()\n",
        "\n",
        "    if document == \"\":\n",
        "      f.write(\"question: \" + question + \"\\n\" +\n",
        "              \"answer: odpověď nenalezena\" + \"\\n\" + \n",
        "              \"correct answer: \" + correct_answer + \"\\n\\n\")\n",
        "      continue;\n",
        "    try:\n",
        "      document_cs = document\n",
        "      document = translator.translate(document, dest='en').text\n",
        "    except TypeError:\n",
        "      continue\n",
        "\n",
        "    # remove some trash\n",
        "    if (document_cs.strip().startswith(\"Obrázky, zvuky či videa k tématu\")):\n",
        "      continue\n",
        "\n",
        "    bestDocs.append(document_cs)\n",
        "\n",
        "    # translate\n",
        "    question = translator.translate(question, dest='en').text\n",
        "\n",
        "    # make sure the current vocab is model's vocab\n",
        "    vocab = model.embedder.vocab\n",
        "\n",
        "    # tokenization\n",
        "    document_tokens, tokenized_document_list = tokenize(document)\n",
        "    tokenized_question_list = tokenize(question)[1]\n",
        "\n",
        "    # keep positions of each token in document, we will need this later, when decoding model outputs\n",
        "    document_token_positions = [[token.idx, token.idx + len(token.text)] for token in document_tokens]\n",
        "\n",
        "    # lowercasing and numericalization\n",
        "    numericalized_document = [vocab.stoi[s.lower()] for s in tokenized_document_list]\n",
        "    numericalized_question = [vocab.stoi[s.lower()] for s in tokenized_question_list]\n",
        "\n",
        "    batch = simple_batch(numericalized_question,numericalized_document,document,document_token_positions)\n",
        "\n",
        "    # get predictions with arg_maxes\n",
        "    logprobs_S, logprobs_E, argmax_Q = model.forward(batch, return_max=True)\n",
        "\n",
        "    # decode from log probabilities to predictions\n",
        "    best_span_prob, candidate, logprobs = decode(logprobs_S, logprobs_E)\n",
        "    confidence =  best_span_prob.item()\n",
        "    log_conf = logprobs.item()\n",
        "\n",
        "    #get answer\n",
        "    answers = get_spans(batch, candidate)\n",
        "    answer = answers[0]\n",
        "\n",
        "    # save probs and answer\n",
        "    bestAnswers.append(answer)\n",
        "    bestProbs.append(confidence)\n",
        "    bestLogProbs.append(log_conf)\n",
        "\n",
        "  # check if any answer was found\n",
        "  if len(bestProbs) == 0:\n",
        "    f.write(\"question: \" + question_cs + \"\\n\" +\n",
        "              \"answer: odpověď nenalezena\" + \"\\n\" + \n",
        "              \"correct answer: \" + correct_answer + \"\\n\\n\")\n",
        "    continue\n",
        "\n",
        "  # get the best doc\n",
        "  # get best answer from retriever according to reader\n",
        "  document = bestDocs[np.argmax(bestLogProbs, axis=0)]\n",
        "  answer = bestAnswers[np.argmax(bestLogProbs, axis=0)]\n",
        "  confidence = bestProbs[np.argmax(bestLogProbs, axis=0)]\n",
        "\n",
        "  # translate the final answer\n",
        "  answer =  translator.translate(answer, dest='cs').text\n",
        "\n",
        "  f.write(\"otázka č.\" + name + \": \" + question_cs + \"\\n\" +\n",
        "          \"odpověď: \" + answer + \"\\n\" + \n",
        "          \"správná odpověď podle sqad : \" + correct_answer + \"\\n\" +\n",
        "          \"----------------------------------------------------------------\\n\"+\n",
        "          \"získaný dokument: \" + document + \n",
        "          \"\\n----------------------------------------------------------------\\n\"+\n",
        "          \"----------------------------------------------------------------\"+\n",
        "          \"\\n\\n\")\n",
        "  print(\"wrote: \" + name)\n",
        "\n",
        "  # print(f\"The answer is: \\\"{answer}\\\".\")\n",
        "  # print(f\"The model is confident with {confidence:.2f} probability.\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}