{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"albert_squad_fine-tuning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPPQU2azenCuHcwMKa+83st"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2JxI20j7KBbj"},"source":["# Requirments"]},{"cell_type":"code","metadata":{"id":"jVmBt1GkJscG"},"source":["!pip install sentencepiece\n","!pip install datasets transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bRQGzm8pJ4C6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616664864834,"user_tz":-60,"elapsed":900,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"c03be9b0-5840-456b-cc1f-4af2b9717064"},"source":["import torch\n","import string\n","import os\n","import sys\n","import time\n","import shutil\n","import json\n","import numpy as np\n","import collections\n","import datetime\n","from tqdm.auto import tqdm\n","import warnings\n","import json\n","import nltk.data\n","import nltk\n","\n","from datasets import load_dataset, load_metric\n","from typing import List, Tuple, Dict\n","from collections import defaultdict\n","from transformers import AlbertTokenizerFast, AlbertForQuestionAnswering, TrainingArguments, Trainer, default_data_collator\n","\n","from google.colab import drive\n","\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"3oFuZPSCKZiv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616075381448,"user_tz":-60,"elapsed":144374,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"230a816a-7976-4bed-dc75-8826d4a75aa0"},"source":["# Remove pre-cached sample data in colab's directory\n","if os.path.isdir(\"sample_data\"):\n","  shutil.rmtree(\"sample_data\")\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TshLGyuzEUpf"},"source":["# Preprocessing SQUAD"]},{"cell_type":"code","metadata":{"id":"6vES2bhXy4Z7"},"source":["# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n","# answers are allowed or not).\n","squad_v2 = False\n","if squad_v2:\n","  model_checkpoint = \"./drive/MyDrive/albert_models/albert_squad2_finetuned\"\n","else:\n","  model_checkpoint = \"./drive/MyDrive/albert_models/albert_finetuned\"\n","batch_size = 16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eT-5_Hse4MJA"},"source":["def prepare_train_features(examples):\n","    \"\"\"\n","    This method has been borrowed from huggingface notebook\n","    https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n","\n","    \"\"\"\n","\n","    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n","    # in one example possible giving several features when a context is long, each of those features having a\n","    # context that overlaps a bit the context of the previous feature.\n","    tokenized_examples = tokenizer(\n","        examples[\"question\"],\n","        examples[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","    # The offset mappings will give us a map from token to character position in the original context. This will\n","    # help us compute the start_positions and end_positions.\n","    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n","\n","    # Let's label those examples!\n","    tokenized_examples[\"start_positions\"] = []\n","    tokenized_examples[\"end_positions\"] = []\n","\n","    for i, offsets in enumerate(offset_mapping):\n","        # We will label impossible answers with the index of the CLS token.\n","        input_ids = tokenized_examples[\"input_ids\"][i]\n","        cls_index = input_ids.index(tokenizer.cls_token_id)\n","\n","        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        answers = examples[\"answers\"][sample_index]\n","        # If no answers are given, set the cls_index as answer.\n","        if len(answers[\"answer_start\"]) == 0:\n","            tokenized_examples[\"start_positions\"].append(cls_index)\n","            tokenized_examples[\"end_positions\"].append(cls_index)\n","        else:\n","            # Start/end character index of the answer in the text.\n","            start_char = answers[\"answer_start\"][0]\n","            end_char = start_char + len(answers[\"text\"][0])\n","\n","            # Start token index of the current span in the text.\n","            token_start_index = 0\n","            while sequence_ids[token_start_index] != 1:\n","                token_start_index += 1\n","\n","            # End token index of the current span in the text.\n","            token_end_index = len(input_ids) - 1\n","            while sequence_ids[token_end_index] != 1:\n","                token_end_index -= 1\n","\n","            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n","            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n","                tokenized_examples[\"start_positions\"].append(cls_index)\n","                tokenized_examples[\"end_positions\"].append(cls_index)\n","            else:\n","                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n","                # Note: we could go after the last offset if the answer is the last word (edge case).\n","                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n","                    token_start_index += 1\n","                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n","                while offsets[token_end_index][1] >= end_char:\n","                    token_end_index -= 1\n","                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n","\n","    return tokenized_examples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h_dPjTbRlsDL"},"source":["Create tokenizer, load squad dataset and prepare features for training"]},{"cell_type":"code","metadata":{"id":"pd4irRm65p06"},"source":["tokenizer = AlbertTokenizerFast.from_pretrained(model_checkpoint)\n","datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")\n","\n","max_length=384\n","doc_stride=128\n","tokenized_datasets = datasets.map(prepare_train_features,\n","                                  batched=True, \n","                                  remove_columns=datasets[\"train\"].column_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5N3VXCshEY3g"},"source":["# Albert model fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"oKowsABXmJTA"},"source":["Load model"]},{"cell_type":"code","metadata":{"id":"cxulpn9Q770q"},"source":["model = AlbertForQuestionAnswering.from_pretrained(model_checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzVL6NbxZREF"},"source":["Create trainer"]},{"cell_type":"code","metadata":{"id":"oZ03rHLM8MZ6"},"source":["# training arguments\n","args = TrainingArguments(\n","    f\"./drive/MyDrive/data/checkpoints\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n",")\n","data_collator = default_data_collator\n","\n","# creating trainer class\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=tokenized_datasets[\"train\"],\n","    eval_dataset=tokenized_datasets[\"validation\"],\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"esdYGARYZz73"},"source":["Train and save"]},{"cell_type":"code","metadata":{"id":"KBZMu-OB82D2"},"source":["trainer.train()\n","\n","# saved model to stated dict\n","trainer.save_model(\"./drive/MyDrive/data/albert_finetuned\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BYPwBfguEgAb"},"source":["# Model-reader evaluation"]},{"cell_type":"markdown","metadata":{"id":"Rec9XbzcxWBy"},"source":["This part has been borrowed from Huggingface notebook\n","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb"]},{"cell_type":"markdown","metadata":{"id":"2xdNTutOm1y8"},"source":["Prepare our validation dataset features"]},{"cell_type":"code","metadata":{"id":"biBzgvjUS54U"},"source":["def prepare_validation_features(examples):\n","    \"\"\"\n","    This method has been borrowed from huggingface notebook\n","    https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n","\n","    \"\"\"\n","    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n","    # in one example possible giving several features when a context is long, each of those features having a\n","    # context that overlaps a bit the context of the previous feature.\n","    tokenized_examples = tokenizer(\n","        examples[\"question\"],\n","        examples[\"context\"],\n","        truncation=\"only_second\",\n","        max_length=max_length,\n","        stride=doc_stride,\n","        return_overflowing_tokens=True,\n","        return_offsets_mapping=True,\n","        padding=\"max_length\",\n","    )\n","\n","    # Since one example might give us several features if it has a long context, we need a map from a feature to\n","    # its corresponding example. This key gives us just that.\n","    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n","\n","    # We keep the example_id that gave us this feature and we will store the offset mappings.\n","    tokenized_examples[\"example_id\"] = []\n","\n","    for i in range(len(tokenized_examples[\"input_ids\"])):\n","        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n","        sequence_ids = tokenized_examples.sequence_ids(i)\n","        context_index = 1\n","        # One example can give several spans, this is the index of the example containing this span of text.\n","        sample_index = sample_mapping[i]\n","        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n","\n","        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n","        # position is part of the context or not.\n","        tokenized_examples[\"offset_mapping\"][i] = [\n","            (o if sequence_ids[k] == context_index else None)\n","            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n","        ]\n","\n","    return tokenized_examples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1CiqBkeDmw44"},"source":["Get model predictions on validation dataset"]},{"cell_type":"code","metadata":{"id":"jmK8WQLEWyRN"},"source":["def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n","    \"\"\"\n","    This method has been borrowed from huggingface notebook\n","    https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n","\n","    \"\"\"\n","    all_start_logits, all_end_logits = raw_predictions\n","    # Build a map example to its corresponding features.\n","    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","    features_per_example = collections.defaultdict(list)\n","    for i, feature in enumerate(features):\n","        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","    # The dictionaries we have to fill.\n","    predictions = collections.OrderedDict()\n","\n","    # Logging.\n","    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n","\n","    # Let's loop over all the examples!\n","    for example_index, example in enumerate(tqdm(examples)):\n","        # Those are the indices of the features associated to the current example.\n","        feature_indices = features_per_example[example_index]\n","\n","        min_null_score = None # Only used if squad_v2 is True.\n","        valid_answers = []\n","        \n","        context = example[\"context\"]\n","        # Looping through all the features associated to the current example.\n","        for feature_index in feature_indices:\n","            # We grab the predictions of the model for this feature.\n","            start_logits = all_start_logits[feature_index]\n","            end_logits = all_end_logits[feature_index]\n","            # This is what will allow us to map some the positions in our logits to span of texts in the original\n","            # context.\n","            offset_mapping = features[feature_index][\"offset_mapping\"]\n","\n","            # Update minimum null prediction.\n","            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n","            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n","            if min_null_score is None or min_null_score < feature_null_score:\n","                min_null_score = feature_null_score\n","\n","            # Go through all possibilities for the `n_best_size` greater start and end logits.\n","            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n","                    # to part of the input_ids that are not in the context.\n","                    if (\n","                        start_index >= len(offset_mapping)\n","                        or end_index >= len(offset_mapping)\n","                        or offset_mapping[start_index] is None\n","                        or offset_mapping[end_index] is None\n","                    ):\n","                        continue\n","                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n","                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n","                        continue\n","\n","                    start_char = offset_mapping[start_index][0]\n","                    end_char = offset_mapping[end_index][1]\n","                    valid_answers.append(\n","                        {\n","                            \"score\": start_logits[start_index] + end_logits[end_index],\n","                            \"text\": context[start_char: end_char]\n","                        }\n","                    )\n","        \n","        if len(valid_answers) > 0:\n","            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n","        else:\n","            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n","            # failure.\n","            best_answer = {\"text\": \"\", \"score\": 0.0}\n","        \n","        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n","        if not squad_v2:\n","            predictions[example[\"id\"]] = best_answer[\"text\"]\n","        else:\n","            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n","            predictions[example[\"id\"]] = answer\n","\n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GCkrrwoTYWCN"},"source":["Load features and predictions for validation"]},{"cell_type":"code","metadata":{"id":"lse-A4J-U7-g"},"source":["# get ground truth features\n","validation_features = datasets[\"validation\"].map(prepare_validation_features,\n","                                                 batched=True,\n","                                                 remove_columns=datasets[\"validation\"].column_names)\n","\n","# get raw predictions\n","raw_predictions = trainer.predict(validation_features)\n","\n","validation_features.set_format(type=validation_features.format[\"type\"], \n","                               columns=list(validation_features.features.keys()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hUgPRi7NmiC6"},"source":["Evaluate"]},{"cell_type":"code","metadata":{"id":"hrYmOc4UYBcB"},"source":["# hyperparameters\n","max_answer_length = 30\n","n_best_size = 20\n","\n","# map examples to features\n","examples = datasets[\"validation\"]\n","features = validation_features\n","\n","example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n","features_per_example = collections.defaultdict(list)\n","for i, feature in enumerate(features):\n","    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n","\n","# get final predictions\n","final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)\n","# get metric used\n","metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n","\n","#evaluate\n","if squad_v2:\n","    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n","else:\n","    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n","references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n","metric.compute(predictions=formatted_predictions, references=references)"],"execution_count":null,"outputs":[]}]}