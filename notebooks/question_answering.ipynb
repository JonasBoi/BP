{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"question_answering.ipynb","provenance":[{"file_id":"1_lsnv1YRRPe5jUH0sw-9zYyc3e_V4KT9","timestamp":1603274924551},{"file_id":"1r8_oU3eCE-qMS0Yp33-BUk2LNR53Sst6","timestamp":1562254815408},{"file_id":"1O2DvLeTtJBF_5pKSXb6TTuY59IwDV1Wn","timestamp":1562252297292}],"collapsed_sections":["bPGazghDdOXn","whlGzus9Z4Tx","tn2ZrNBVGaHI","kd1sclnsRUsK"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"c0b370dbd82d421c8b31bf82d84434eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d15be6c72131485285b306dfb0712513","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d94065c879c34ee38c4f43deb5b9f21f","IPY_MODEL_db3c3f8e39ac4806b583fe8d166517a1"]}},"d15be6c72131485285b306dfb0712513":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d94065c879c34ee38c4f43deb5b9f21f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_597de0c0e69b4eeaba5c931a01b076e6","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":51,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":51,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1c2b8d65876c47859f8120c07068f841"}},"db3c3f8e39ac4806b583fe8d166517a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0f2b224c0fbd495d9bc82eab78973986","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 51/51 [00:44&lt;00:00,  1.15it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d0d525b46ecb41fd8213c2896a6026e8"}},"597de0c0e69b4eeaba5c931a01b076e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1c2b8d65876c47859f8120c07068f841":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f2b224c0fbd495d9bc82eab78973986":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d0d525b46ecb41fd8213c2896a6026e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aa67af5fa925419c920f874954d7b5da":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fb1456f4480c43b4919e11b4c4d287a9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6b8a7596fa00427e863d4fb6a444d3ed","IPY_MODEL_67896273d2924b32a0176cdaeb9bd733"]}},"fb1456f4480c43b4919e11b4c4d287a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b8a7596fa00427e863d4fb6a444d3ed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_73a9921a14854ad4a6b049b6375504c8","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":100,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":100,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d86ba74af4c42caab980fbc9b7e93c8"}},"67896273d2924b32a0176cdaeb9bd733":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8ad4b9876c584438b324916c731bbcc5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 100/100 [09:15&lt;00:00,  5.56s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_044360bd84774d19ad0bdf94ed5ba00d"}},"73a9921a14854ad4a6b049b6375504c8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1d86ba74af4c42caab980fbc9b7e93c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8ad4b9876c584438b324916c731bbcc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"044360bd84774d19ad0bdf94ed5ba00d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"ptvBUgFZkKmm"},"source":["# Requirments\n","\n"]},{"cell_type":"code","metadata":{"id":"KUgwJtaFLYTc"},"source":["\"\"\"\n","# get majka database\n","!curl --remote-name-all https://nlp.fi.muni.cz/ma{/majka.w-lt}\n","!mv majka.w-lt drive/MyDrive/data/\n","# download czech squad\n","!curl --remote-name-all https://lindat.cz/repository/xmlui/bitstream/handle/11234/1-3069{/sqad_v3.tar.xz}\n","!mv sqad_v3.tar.xz drive/MyDrive/data/\n","!tar -xf drive/MyDrive/data/sqad_v3.tar.xz\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RKgj8CeSViAh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617117803492,"user_tz":-120,"elapsed":27064,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"03ca7fa7-fce7-4b97-caf9-dfee61d585f6"},"source":["!pip install sentencepiece\n","!pip install datasets transformers\n","!pip install googletrans==4.0.0-rc1\n","!pip install wikipedia\n","!pip install rank_bm25\n","!pip install majka"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\r\u001b[K     |▎                               | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 31.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 21.7MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 20.1MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 21.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 16.5MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 16.5MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 15.8MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 16.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 16.5MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 16.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 16.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 16.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 16.5MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 16.5MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 16.5MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 16.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 16.5MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.95\n","Collecting datasets\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n","\u001b[K     |████████████████████████████████| 194kB 17.2MB/s \n","\u001b[?25hCollecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 33.5MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Collecting huggingface-hub<0.1.0\n","  Downloading https://files.pythonhosted.org/packages/af/07/bf95f398e6598202d878332280f36e589512174882536eb20d792532a57d/huggingface_hub-0.0.7-py3-none-any.whl\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n","Collecting fsspec\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n","\u001b[K     |████████████████████████████████| 112kB 56.2MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Collecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n","\u001b[K     |████████████████████████████████| 245kB 54.9MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 54.3MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 51.1MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=8794edba7e1a0283c19ed50a559532df47a8b88fed252b585cfa31c4d20d9348\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: huggingface-hub, fsspec, xxhash, datasets, sacremoses, tokenizers, transformers\n","Successfully installed datasets-1.5.0 fsspec-0.8.7 huggingface-hub-0.0.7 sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2 xxhash-2.0.0\n","Collecting googletrans==4.0.0-rc1\n","  Downloading https://files.pythonhosted.org/packages/fa/0d/a5fe8fb53dbf401f8a34cef9439c4c5b8f5037e20123b3e731397808d839/googletrans-4.0.0rc1.tar.gz\n","Collecting httpx==0.13.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.4MB/s \n","\u001b[?25hCollecting sniffio\n","  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n","Collecting rfc3986<2,>=1.3\n","  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n","Collecting httpcore==0.9.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n","\u001b[?25hCollecting hstspreload\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n","\u001b[K     |████████████████████████████████| 1.0MB 44.0MB/s \n","\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2020.12.5)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n","Collecting h2==3.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n","\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n","\u001b[?25hCollecting h11<0.10,>=0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n","\u001b[?25hCollecting hyperframe<6,>=5.2.0\n","  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n","Collecting hpack<4,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-4.0.0rc1-cp37-none-any.whl size=17417 sha256=dfc05e6f96b6cddb74abeed00a96160362ad6ff966d17459a72fb53ccfcdc2a5\n","  Stored in directory: /root/.cache/pip/wheels/09/32/56/fd8940f1b3c1d77c9f91b55597c52a4d4833b000a980bb0740\n","Successfully built googletrans\n","Installing collected packages: sniffio, rfc3986, hyperframe, hpack, h2, h11, httpcore, hstspreload, httpx, googletrans\n","Successfully installed googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.4.0 sniffio-1.2.0\n","Collecting wikipedia\n","  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n","Building wheels for collected packages: wikipedia\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp37-none-any.whl size=11686 sha256=43e94cf19aa08224ecf559196ec4ec3cd212b5c899a65619f5109cbca25fd20d\n","  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n","Successfully built wikipedia\n","Installing collected packages: wikipedia\n","Successfully installed wikipedia-1.4.0\n","Collecting rank_bm25\n","  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rank_bm25) (1.19.5)\n","Installing collected packages: rank-bm25\n","Successfully installed rank-bm25-0.2.1\n","Collecting majka\n","  Downloading https://files.pythonhosted.org/packages/6c/0c/92a788a342a880a676a9cf66b91ec6ec09fbabe5f87decc2cc7d1642b583/majka-0.8.tar.gz\n","Building wheels for collected packages: majka\n","  Building wheel for majka (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for majka: filename=majka-0.8-cp37-cp37m-linux_x86_64.whl size=71254 sha256=4bfe76adf3b135193848a637367e78baf8c6c3905da661c0404196d03314900c\n","  Stored in directory: /root/.cache/pip/wheels/e3/fa/b9/dccbac5d1bf537d79442725fe665ada8928b295fbe87b7d0c0\n","Successfully built majka\n","Installing collected packages: majka\n","Successfully installed majka-0.8\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HoTIyL_slAcA"},"source":["Importing important libraries"]},{"cell_type":"code","metadata":{"id":"bRQGzm8pJ4C6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617117809859,"user_tz":-120,"elapsed":29410,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"42705091-2d4e-49c9-86f0-20dedf5c5ecb"},"source":["import torch\n","import string\n","import os\n","import sys\n","import time\n","import shutil\n","import json\n","import numpy as np\n","import collections\n","import datetime\n","from tqdm.auto import tqdm\n","import warnings\n","import json\n","import nltk.data\n","import nltk\n","\n","from datasets import load_dataset, load_metric\n","from typing import List, Tuple, Dict\n","from collections import defaultdict\n","from transformers import AlbertTokenizerFast, AlbertForQuestionAnswering\n","from transformers import BertTokenizerFast, BertForQuestionAnswering\n","\n","from rank_bm25 import BM25Okapi, BM25Plus, BM25L\n","import re\n","import majka\n","import wikipedia\n","from googletrans import Translator\n","import requests\n","\n","from google.colab import drive\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","nltk.download('punkt')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"3oFuZPSCKZiv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617117840836,"user_tz":-120,"elapsed":535,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"d880079f-20e5-4bc0-8db1-39e6060a0750"},"source":["# Remove pre-cached sample data in colab's directory\n","if os.path.isdir(\"sample_data\"):\n","  shutil.rmtree(\"sample_data\")\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bPGazghDdOXn"},"source":["# Reader"]},{"cell_type":"code","metadata":{"id":"fjuzSaPEnesl"},"source":["class Reader():\n","\n","  def __init__(self, model_checkpoint, model_type=\"albert\", max_answer_length=10, n_best_size=10, max_length=384, stride=128, use_cpu=False):\n","    # load all parameters of the reader\n","    self.max_answer_length = max_answer_length  # max answer span length\n","    self.n_best_size = n_best_size  # \n","    self.max_length = max_length  # max count of tokens in one tokenized passage\n","    self.stride = stride  # the length of overlap between two mini-batches of tokenizer\n","\n","    # choose device; cuda if available\n","    self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and use_cpu == False) else \"cpu\")\n","\n","    if model_type == 'albert':\n","      # load tokenizer and model from pretrained checkpoint\n","      self.tokenizer = AlbertTokenizerFast.from_pretrained(model_checkpoint)\n","      # load model to device if possible\n","      self.model = AlbertForQuestionAnswering.from_pretrained(model_checkpoint).to(self.device)\n","    elif model_type == 'mbert':\n","      # load tokenizer and model from pretrained checkpoint\n","      self.tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n","      # load model to device if possible\n","      self.model = BertForQuestionAnswering.from_pretrained(model_checkpoint).to(self.device)\n","    else:\n","      print(\"Wrong model type parameter.\")\n","      return None\n","\n","    print(\"Model loaded from: \" + model_checkpoint)\n","    print(f\"Model has {self.count_parameters(self.model)} parameters\")\n","    print(\"Device selected:\")\n","    print(self.device)\n","\n","\n","  def decode(self, output, context, offset_mappings):\n","    \"\"\"\n","    get the text span from the unnormalized log probabilities\n","\n","    method has been partly borrowed from \n","    https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n","\n","    \"\"\"\n","    # enumerate over all outputs (max output size is 500 tokens in a log prob tensor)\n","    valid_answers = []\n","\n","    for i, _ in enumerate(output.start_logits):\n","\n","      start_logits = output.start_logits[i].cpu().detach().numpy()\n","      end_logits = output.end_logits[i].cpu().detach().numpy()\n","      offset_mapping = offset_mappings[i]\n","\n","      # Gather the indices the best start/end logits:\n","      start_indexes = np.argsort(start_logits)[-1 : - self.n_best_size - 1 : -1].tolist()\n","      end_indexes = np.argsort(end_logits)[-1 : - self.n_best_size - 1 : -1].tolist()\n","      for start_index in start_indexes:\n","          for end_index in end_indexes:\n","              # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n","              # to part of the input_ids that are not in the context.\n","              if (\n","                  start_index >= len(offset_mapping)\n","                  or end_index >= len(offset_mapping)\n","                  or offset_mapping[start_index] is None\n","                  or offset_mapping[end_index] is None\n","              ):\n","                  continue\n","              # Don't consider answers with a length that is either < 0 or > max_answer_length.\n","              if end_index < start_index or end_index - start_index + 1 > self.max_answer_length:\n","                  continue\n","              if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n","                  start_char = offset_mapping[start_index][0]\n","                  end_char = offset_mapping[end_index][1]\n","                  valid_answers.append(\n","                      {\n","                          \"score\": start_logits[start_index] + end_logits[end_index],\n","                          \"text\": context[start_char: end_char].strip()\n","                      }\n","                  )\n","    valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:self.n_best_size]\n","    return valid_answers\n","\n","\n","  def get_answers(self, question, context):\n","    \"\"\"\n","    get the best answers from the context to the question \n","\n","    \"\"\"\n","    inputs = self.tokenizer(question, context, \n","                      return_tensors='pt',\n","                      truncation=\"only_second\",\n","                      max_length=self.max_length, # to prevent cuda running out of memory\n","                      stride=self.stride,     # overlap within splitted long\n","                      return_offsets_mapping=True,\n","                      return_overflowing_tokens=True,\n","                      padding=\"max_length\")\n","    inputs.to(self.device)\n","\n","    outputs = self.model(inputs['input_ids'], \n","                    token_type_ids=inputs['token_type_ids'],\n","                    attention_mask=inputs['attention_mask'])\n","    \n","    valid_answers = self.decode(outputs, context, inputs['offset_mapping'])\n","    \n","    return valid_answers\n","\n","\n","  def count_parameters(self, model):\n","    \"\"\"\n","    Counts the parameters of the model\n","\n","    \"\"\"\n","\n","    return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"whlGzus9Z4Tx"},"source":["# Retriever\n"]},{"cell_type":"code","metadata":{"id":"iX5OVeUoufHc"},"source":["class Retriever():\n","\n","  def __init__(self):\n","    wikipedia.set_lang(\"cs\") \n","\n","    # save the most common czech words (stop words)\n","    common = \"kdy být a se v na ten on že s z který mít do já o k i jeho ale svůj jako za moci pro tak po tento co když všechen už jak aby od nebo říci jeden jen můj jenž ty stát u muset chtít také až než ještě při jít pak před však ani vědět hodně podle další celý jiný mezi dát tady tam kde každý takový protože nic něco ne sám bez či dostat nějaký proto\"\n","    self.common = common.split()\n","\n","    # save punctuation to be removed\n","    punctuation = \". , ? ! ... \\\" ( ) ; - /\"\n","    self.punctuation = punctuation.split()\n","\n","    # majka lemmatizer settings\n","    self.morph = majka.Majka('drive/MyDrive/data/majka.w-lt')\n","    self.morph.flags = 0  # unset all flags\n","    self.morph.tags = False  # return just the lemma, do not process the tags\n","    self.morph.first_only = True  # return only the first entry\n","    self.morph.negative = \"ne\"\n","\n","    # load wiki titles and build index for search\n","    self.bm25_articles_index, self.titles = self.get_title_search_index()\n","\n","    # load tokenizer to split text into sentences\n","    self.tokenizer = nltk.data.load('tokenizers/punkt/czech.pickle')\n","\n","\n","  def get_title_search_index(self):\n","    \"\"\"\n","    Build index for searching through relevant title names on czech wiki:\n","\n","    \"\"\"\n","\n","    f = open(\"drive/MyDrive/data/wiki/cswiki-latest-all-titles-in-ns0\", \"r\")\n","    titles = []\n","\n","    for line in f: \n","      title = ((\" \").join(line.split(\"_\"))).strip()\n","      title = title.strip('\\n')\n","      titles.append(title)\n","\n","    f.close()\n","\n","    # tokenize for bm25\n","    tok_titles = []\n","    for title in titles:\n","      tok_tit = re.split(\" \", title.lower())\n","      for tok in tok_tit:\n","        if tok == \"\":\n","          tok_tit.remove(\"\")\n","      tok_titles.append(tok_tit)\n","\n","    bm25 = BM25Okapi(tok_titles)\n","\n","    return bm25, titles\n","\n","\n","  def search_titles(self, question):\n","    \"\"\"\n","    Search with bm25 among the wiki titles\n","\n","    \"\"\"\n","    tokenized_query = self.delete_common(self.lemmatize(question.lower()))\n","    # print(tokenized_query)\n","    results = self.bm25_articles_index.get_top_n(tokenized_query, self.titles, n=5)\n","\n","    return results\n","\n","\n","  def get_named_entities(self, question):\n","    \"\"\"\n","    Extracts named entities from the question.\n","\n","    \"\"\"\n","\n","    URL = \"https://nlp.fi.muni.cz/projekty/ner/nerJSON.py\"\n","    text = question\n","    PARAMS = {'text': text}\n","    r = requests.get(url = URL, params = PARAMS)\n","    data = r.json() \n","\n","    lemmatas = []\n","\n","    if data != {}:\n","      for item in data:\n","        lemma = data[item]['lemma']\n","        lemmatas.append(lemma)\n","\n","    return lemmatas\n","\n","  \n","  def iscommon(self, x):\n","    \"\"\"\n","    decides if query token is common\n","\n","    \"\"\"\n","    if x in self.common or x in self.punctuation:\n","      return True\n","    else:\n","      return False\n","\n","\n","  def delete_common(self, tokens):\n","    \"\"\"\n","    Remove the most common czech words from the query tokens (low information value)\n","\n","    \"\"\"\n","    tokens = [x for x in tokens if not self.iscommon(x)]\n","        \n","    return tokens\n","\n","  \n","  def lemmatize(self, text):\n","    \"\"\"\n","    Returns lemma of each token in a list of lemmatized tokens\n","\n","    \"\"\"\n","\n","    tok_text = text.lower()\n","    tok_text = re.split(\"\\W\", text)\n","\n","    # lemmatize each token\n","    lemmatized_tokens = []\n","    for token in tok_text:\n","      if token == '':\n","        continue\n","      lemma = self.morph.find(token)\n","      if len(lemma) == 0:\n","        lemmatized_tokens.append(token)\n","      else:\n","        lemmatized_tokens.append(lemma[0]['lemma'])\n","\n","    return lemmatized_tokens\n","\n","\n","  def search_again(self, tokens):\n","    \"\"\"\n","    Performs repeated search in case wiki api didnt find any documents\n","\n","    \"\"\"\n","    searched_term = (' ').join(tokens)\n","    #print(searched_term)\n","    doc_list = wikipedia.search(searched_term, results=1)\n","\n","    if len(tokens) == 0:\n","      return []\n","\n","    if len(doc_list) == 0:\n","      del tokens[0]\n","      return self.search_again(tokens)\n","\n","    return doc_list\n","\n","\n","  def get_doc_list(self, question):\n","    \"\"\"\n","    Returns top 1-3 wiki arcitles that might answer the question topic\n","\n","    \"\"\"\n","\n","    # get names entities if present\n","    named_ERs = self.get_named_entities(question)\n","    # get relevant article title names\n","    relevant_titles = self.search_titles(question)\n","\n","    #search for documents\n","    max_docs = 1\n","    doc_list = []\n","\n","    # search based on recognised named entity\n","    if len(named_ERs) > 0:\n","      article = wikipedia.search(named_ERs[0], results=max_docs)\n","      if len(article) > 0:\n","        doc_list.append(article[0])\n","    # search based on best wiki title match\n","    if len(relevant_titles) > 0:\n","      article = wikipedia.search(relevant_titles[0], results=max_docs)\n","      if len(article) > 0:\n","        doc_list.append(article[0])\n","\n","    # basic search for the question\n","    article = wikipedia.search(question, results=max_docs)\n","    # simplify the search if its too bad\n","    if len(article) == 0:\n","      # extract important for wiki\n","      tokens = self.delete_common(self.lemmatize(question))\n","      article = self.search_again(tokens)\n","    doc_list.append(article[0])\n","\n","    return doc_list\n","\n","  \n","  def normalize_length(self, par):\n","    \"\"\"\n","    Splits too long paragraph into smaller ones\n","\n","    \"\"\"\n","\n","    #split long paragraph into sentences\n","    sentences = self.tokenizer.tokenize(par)\n","\n","    normalized_pars = []\n","    new_paragraph = \"\"\n","\n","    # iterate over sentences\n","    for idx, sentence in enumerate(sentences):\n","      \n","      if len(new_paragraph) + len(sentence) > 1500:\n","        normalized_pars.append(new_paragraph)\n","        new_paragraph = \"\"\n","\n","        # make some overlap\n","        for k, trailing in enumerate(sentences[idx-2:idx]):\n","          new_paragraph += trailing\n","\n","      else:\n","        new_paragraph += sentence\n","    \n","    return normalized_pars\n","\n","  \n","  def split_documents(self, doc_list):\n","    \"\"\"\n","    Splits each retrievede wiki article into paragraphs and normalizes its lengths\n","\n","    \"\"\"\n","\n","    pars = []\n","    lemm_pars = []\n","\n","    # iterate over articles and process each one\n","    for doc in doc_list:\n","      # get whole page content\n","      try:\n","        doc = wikipedia.page(doc)\n","      except wikipedia.DisambiguationError as e:\n","        s = e.options[0]\n","        try:\n","          doc = wikipedia.page(s)\n","        except wikipedia.DisambiguationError:\n","          continue\n","\n","      result = re.split('=== Reference ===|== Reference ==', doc.content)[0]\n","      # split article into paragraphs\n","      result = re.split('== .*. ==|\\\\n\\\\n', result)\n","\n","      # save stripped paragraphs\n","      for par in result:\n","        par = ((((par.strip()).strip('=')).strip('\\n')).strip('\\n\\n')).strip('\\r\\n')\n","\n","        # remove some trash\n","        if par == '' or par == '\\n' or par.strip().startswith(\"Obrázky, zvuky či videa k tématu\"):\n","          continue\n","\n","        # check max paragraph length\n","        if len(par) > 1500:\n","          # split into smaller paragraphs\n","          normalized_paragraphs = self.normalize_length(par)\n","          # append each smaller paragraph\n","          for norm_par in normalized_paragraphs:\n","            pars.append(norm_par)\n","            lemm_pars.append((' ').join(self.delete_common(self.lemmatize(norm_par.lower()))))\n","        else:\n","          # append paragraph\n","          pars.append(par)\n","\n","          # get lemmas and append\n","          lemm_pars.append((' ').join(self.delete_common(self.lemmatize(par.lower()))))\n","\n","    return pars, lemm_pars\n","\n","  def retrieve(self, question):  \n","    \"\"\"\n","    Returns the top 3 paragraphs for the given question\n","\n","    \"\"\"\n","    # max question length\n","    if len(question) > 250:\n","      return \"\"\n","    # strip questionmark\n","    question = question.strip('?')\n","\n","    # TODO SPEED THIS UP\n","    # get wiki documents\n","    doc_list = self.get_doc_list(question)\n","\n","    # convert to set to only work with unique article names\n","    doc_list = set(doc_list)\n","\n","    # TODO SPEED THIS UP\n","    # split docs into paragraphs\n","    pars, lemm_pars = self.split_documents(doc_list)\n","\n","    # tokenize for bm25\n","    tok_text = []\n","    for par in lemm_pars:\n","      tok_par = re.split(\"\\W\", par)\n","      for tok in tok_par:\n","        if tok == \"\":\n","          tok_par.remove(\"\")\n","      tok_text.append(tok_par)\n","\n","    # build index\n","    bm25 = BM25Plus(tok_text)\n","    # bm25 = BM25Okapi(tok_text)\n","\n","    # tokenize and lemmatize the query\n","    tokenized_query = (' ').join(self.delete_common(self.lemmatize(question.lower())))\n","    tokenized_query = re.split(\"\\W\", tokenized_query)\n","\n","    # get results\n","    results = bm25.get_top_n(tokenized_query, pars, n=3)\n","\n","    return results, doc_list\n","\n","\n","  @staticmethod\n","  def count_log_conf(best_answer, all_answers):\n","    \"\"\"\n","    Returns the sum of log probs \n","    \n","    \"\"\"\n","\n","    log_conf = 0\n","    for answer in all_answers:\n","      if (best_answer in answer['text']) or (answer['text'] in best_answer):\n","        log_conf += answer['score']\n","    \n","    return log_conf\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tn2ZrNBVGaHI"},"source":["# Preprocessing SQAD"]},{"cell_type":"code","metadata":{"id":"FFQTW92tCd5M"},"source":["class SqadDataset():\n","\n","  def __init__(self, sqad_dir, save_dir=\"./sqad_processed\", process_boolean=False):\n","    self.save_dir = save_dir\n","    self.sqad_dir = sqad_dir\n","    self.process_boolean = process_boolean\n","\n","  def extract_answer(self, dirnum):\n","    \"\"\"\n","    Parse the answer of current dataset record.\n","    Returns the parsed answer and its lemma.\n","\n","    \"\"\"\n","\n","    f = open(f\"{self.sqad_dir}/{dirnum}/09answer_extraction.vert\", \"r\")\n","\n","    q = f.read().split(\"\\n\")\n","    answer = \"\"\n","    answer_lemma = \"\"\n","\n","    # parse answer\n","    for line in q:\n","      # split into columns\n","      line = line.split(\"\\t\")\n","\n","      # end sign\n","      if line[0] == \"</s>\":\n","        break\n","      \n","      # get answer and its lemma\n","      line_a = line[0]\n","      if len(line) > 1:\n","        line_a_lemma = line[1]\n","        if line_a_lemma == \"[number]\":\n","          line_a_lemma = line[0]\n","\n","      # process special signs\n","      if line_a in {\"<s>\", \"<g/>\", \"</s>\"}:\n","        answer = answer[:-1]\n","        answer_lemma = answer_lemma[:-1]\n","        continue\n","\n","      # append answer\n","      answer += line_a + \" \"\n","      answer_lemma += line_a_lemma + \" \"\n","    \n","    f.close()\n","    return answer, answer_lemma\n","\n","\n","  def extract_question(self, dirnum):\n","    \"\"\"\n","    Parse the answer of current dataset record.\n","    Returns the parsed answer and its lemma.\n","    \n","    \"\"\"\n","\n","    f = open(f\"{self.sqad_dir}/{dirnum}/01question.vert\", \"r\")\n","\n","    q = f.read().split(\"\\n\")\n","    question = \"\"\n","\n","    for line in q:\n","      line = line.split(\"\\t\")\n","      # end sign\n","      if line[0] == \"</s>\" and question[-1] == \"?\":\n","        break\n","\n","      line = line[0]\n","      if line in {\"<s>\", \"<g/>\", \"</s>\"}:\n","        question = question[:-1]\n","        continue\n","      \n","      if line != \"?\":\n","        question += line + \" \"\n","      else:\n","        question += \"? \"\n","\n","    f.close()\n","    return question\n","\n","\n","  def process_dataset(self, from_q, to_q):\n","    \"\"\"\n","    Process the questions and answers from the sqad dataset and save it as a json file\n","\n","    Process the dataset from record from_q to record to_q\n","    \n","    \"\"\"\n","\n","    sqad_dataset = {}\n","    counter = from_q\n","\n","    for i in tqdm(range(from_q, to_q+1)):\n","\n","      # get question number\n","      q_number = \"\"\n","      for _ in range(len(str(i)), 6):\n","        q_number += \"0\"\n","      q_number += str(i)\n","\n","      # extract from dataset\n","      question = self.extract_question(q_number)\n","      correct_answer, lemmatized_answer = self.extract_answer(q_number)\n","\n","      # exclude yes/no questions\n","      if not self.process_boolean:\n","        if correct_answer.strip() == \"ano\" or correct_answer.strip() == \"ne\":\n","          continue\n","\n","      # save data\n","      data = {}\n","      data[\"question\"] = question\n","      data[\"answer\"] = correct_answer\n","      data[\"answer_lemma\"] = lemmatized_answer\n","\n","      sqad_dataset[counter] = data\n","      counter += 1\n","\n","    # save extracted data as json\n","    with open(self.save_dir, \"w\") as f:\n","      json.dump(sqad_dataset, f)\n","      print(\"Sqad dataset has been processed to: \" + self.save_dir)\n","\n","\n","  @staticmethod\n","  def load_sqad(saved_dataset_file):\n","    \"\"\"\n","    Loads the saved json daataset as a dictionary\n","    \n","    \"\"\"\n","\n","    # load preprocessed sqad dataset\n","    with open(saved_dataset_file) as f: \n","        data = json.load(f)\n","    print(\"Sqad dataset loaded from: \" + saved_dataset_file)\n","\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["c0b370dbd82d421c8b31bf82d84434eb","d15be6c72131485285b306dfb0712513","d94065c879c34ee38c4f43deb5b9f21f","db3c3f8e39ac4806b583fe8d166517a1","597de0c0e69b4eeaba5c931a01b076e6","1c2b8d65876c47859f8120c07068f841","0f2b224c0fbd495d9bc82eab78973986","d0d525b46ecb41fd8213c2896a6026e8"]},"id":"EHDsT3grGr-M","executionInfo":{"status":"ok","timestamp":1615842996828,"user_tz":-60,"elapsed":45007,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"18d856d2-5c54-4275-a4ba-9f10a1c736f5"},"source":["sqad = SqadDataset(sqad_dir=\"drive/MyDrive/data/cz_sqad/data/\", save_dir=\"drive/MyDrive/data/test.json\")\n","sqad.process_dataset(100, 150)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0b370dbd82d421c8b31bf82d84434eb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=51.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Sqad dataset has been processed to: drive/MyDrive/data/test.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"31IV1omUfXNP"},"source":["# Loading model\n"]},{"cell_type":"markdown","metadata":{"id":"6d3vTvLMn4ua"},"source":["Choose model checkpoint"]},{"cell_type":"code","metadata":{"id":"l9-UEUNjnGsm"},"source":["# either 'mbert' or 'albert'\n","model_type = 'mbert'\n","\n","if model_type == 'mbert':\n","  model_checkpoint = \"./drive/MyDrive/mbert_models/bert_finetuned_czech_squad\"\n","elif model_type == 'albert':\n","  model_checkpoint = \"./drive/MyDrive/albert_models/albert_finetuned\"\n","else:\n","  print(\"wrong model type name\")\n","\n","# albert xlarge pretrained on squad v2 for experiments\n","# model_checkpoint = \"ktrapeznikov/albert-xlarge-v2-squad-v2\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y8fIHfAgn7Oh"},"source":["Create reader\n","\n"]},{"cell_type":"code","metadata":{"id":"-A1KQGXAkbdZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617041154506,"user_tz":-120,"elapsed":16040,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"76641ccf-0f41-4a9f-d021-03b00ffa42ff"},"source":["reader = Reader(model_checkpoint, model_type=model_type)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model loaded from: ./drive/MyDrive/mbert_models/bert_finetuned_czech_squad\n","Model has 177264386 parameters\n","Device selected:\n","cuda:0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rmwj-8Q_PGJQ"},"source":["Create retriever"]},{"cell_type":"code","metadata":{"id":"GQcGScZhPC6m"},"source":["retriever = Retriever()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kd1sclnsRUsK"},"source":["# Final pipeline"]},{"cell_type":"code","metadata":{"id":"IgcqZ3uTIk7c"},"source":["def translate(question_cs, documents_cs, translator):\n","  \"\"\"\n","  Translates the czech question and documents and returns\n","  question and list of documents in english\n","\n","  \"\"\"\n","  # we concatenate the question with all the documents to be translated at once\n","  # so we minimize the number of requests for googletrans\n","  delimiter = \" _____ \"\n","  concatenated = question_cs\n","\n","  for doc in documents_cs:\n","    concatenated += delimiter\n","    concatenated += doc\n","\n","  # and translate as a whole\n","  concatenated = translator.translate(concatenated, src='cs', dest='en').text\n","  # and split again\n","  delimiter = \"_____\"\n","  concatenated = concatenated.split(delimiter)\n","  \n","  # get translated question and doc\n","  question = concatenated[0]\n","  documents = concatenated[1:]\n","\n","  return question, documents"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NlwJe2YLRWsu"},"source":["def find_answer(question, reader, retriever, translator, model_type):\n","  \"\"\"\n","  Finds the answer to the question\n","\n","  \"\"\"\n","\n","  question_cs = question # save czech question\n","\n","  # wiki search\n","  documents_cs, article_list = retriever.retrieve(question)\n","\n","  # for saving the best results\n","  bestAnswers = []\n","  bestDocs = []\n","  bestLogProbs = []\n","  bestSummedLogProbs = []\n","  \n","  # translate according to model type\n","  if model_type == 'mbert':\n","    question = question_cs\n","    documents = documents_cs\n","\n","  elif model_type == 'albert':\n","    # translate question and documents for reader\n","    question, documents = translate(question_cs, documents_cs, translator)\n","  # delete null strings\n","  documents = [x for x in documents if len(x.strip())]\n","\n","  # iterate over retrieved paragraphs\n","  for idx, document in enumerate(documents):\n","    # strip whitespaces\n","    document = document.strip()\n","\n","    # chceck if any document has been found for the question\n","    if document == \"\":\n","      continue;\n","\n","    #get answer -------------------------------------------\n","    answers = reader.get_answers(question, document)\n","    log_conf = 0\n","    log_conf_summed = 0\n","\n","    # choose valid answer\n","    answer = ''\n","    for answer in answers:\n","      if answer['text'] != '':\n","        log_conf = answer['score']\n","        answer = answer['text']\n","        log_conf_summed = Retriever.count_log_conf(answer, answers)\n","        break\n","    #######################################################\n","    if type(answer) is not str:\n","      continue\n","\n","    # save probs and answer\n","    bestAnswers.append(answer)\n","    bestLogProbs.append(log_conf)\n","    bestSummedLogProbs.append(log_conf_summed)\n","    # save retrieved doc\n","    bestDocs.append(documents_cs[idx])\n","\n","  ############################################################\n","  # check if any answer was found\n","  if len(bestLogProbs) == 0 or bestAnswers[np.argmax(bestLogProbs, axis=0)] == '':\n","    return \"\"\n","\n","  # get the best doc\n","  # get best answer from retriever according to reader\n","  document = bestDocs[np.argmax(bestLogProbs, axis=0)]\n","  answer = bestAnswers[np.argmax(bestLogProbs, axis=0)]\n","\n","  # translate the final answer\n","  answer_en = answer\n","  if model_type == 'albert':\n","    answer =  translator.translate(answer, src='en', dest='cs').text\n","\n","  return answer, answer_en, document, bestAnswers, bestLogProbs, bestSummedLogProbs, article_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vx4Wp01k0Gl"},"source":["# Evaluation on  sqad"]},{"cell_type":"code","metadata":{"id":"Tn5_5_YKeC0w"},"source":["def sqad_eval(from_q, to_q, data, save_results_to, model_type):\n","  \"\"\"\n","  Evaluates the model on sqad dataset\n","  \"\"\"\n","  # create translator\n","  translator = Translator()\n","\n","  # write results to\n","  f = open(save_results_to, \"w\")\n","\n","  # for counting correct answers\n","  score = 0\n","\n","  for i in tqdm(range(from_q, to_q+1)):\n","\n","    # get from dataset\n","    question = data[str(i)][\"question\"]\n","    correct_answer = data[str(i)][\"answer\"]\n","    lemmatized_answer = data[str(i)][\"answer_lemma\"]\n","\n","    # get answer and other info to the specific question\n","    answer, answer_en, document, bestAnswers, bestLogProbs, bestSummedLogProbs, article_list = find_answer(\n","        question, reader, retriever, translator, model_type)\n","\n","    # write the result to file\n","    f.write(\"----------------------------------------------------------------\\n\"+\n","            \"-----------------------| otázka č.\" + str(i) + \" |------------------------\\n\" +\n","            \"----------------------------------------------------------------\\n\"+\n","            \"::otázka : \" + question + \"\\n\" +\n","            \"::odpověď: \" + answer + \" / \" + answer_en + \"\\n\" + \n","            \"::správná odpověď podle sqad : \" + correct_answer + \"\\n\" +\n","            \"::lemma odpovědi podle sqad : \" + lemmatized_answer + \"\\n\\n\" +\n","            \"----------------------------------------------------------------\\n\"+\n","            \"získaný dokument: \" + document + \n","            \"\\n----------------------------------------------------------------\\n\")\n","    # what answers did we get over-all - debugging info\n","    for listitem in bestAnswers:\n","      f.write('%s ;; ' % listitem)\n","    f.write(\"\\n\")\n","    for listitem in bestLogProbs:\n","      f.write('%s ;; ' % listitem)\n","    f.write(\"\\n\")\n","    for listitem in bestSummedLogProbs:\n","      f.write('%s ;; ' % listitem)\n","    f.write(\"\\n\")\n","    for listitem in article_list:\n","      f.write('%s ;; ' % listitem)\n","    f.write(\"\\n\\n\\n\")\n","\n","    # convert to lowercase\n","    answer = answer.lower()\n","    answer_en = answer_en.lower()\n","    correct_answer = correct_answer.lower()\n","    lemmatized_answer = lemmatized_answer.lower()\n","    # increment score, if we got any match between the original and retrieved answer\n","    if (answer in correct_answer or correct_answer in answer or\n","        answer in lemmatized_answer or lemmatized_answer in answer or \n","        answer_en in lemmatized_answer or lemmatized_answer in answer_en):\n","      score += 1\n","\n","  # close file descriptor\n","  f.close()\n","\n","  # get the count of questions answered for score calculation\n","  answered_count = to_q - from_q + 1\n","\n","  print(\"done\")\n","  print(\"score: \" + str(score) + \"/\" + str(answered_count))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uf9PmQUh3bez","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617040522496,"user_tz":-120,"elapsed":954,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"f7a0732a-3054-4da7-d5b0-25ce437071be"},"source":["# load preprocessed sqad dataset\n","data = SqadDataset.load_sqad(\"drive/MyDrive/data/sqad_processed_without_yes_no.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sqad dataset loaded from: drive/MyDrive/data/sqad_processed_without_yes_no.json\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PgptuBcVtl8Z"},"source":["Run the evaluation and save the results to the chosen file"]},{"cell_type":"code","metadata":{"id":"ze44udf7oHHf","colab":{"base_uri":"https://localhost:8080/","height":100,"referenced_widgets":["aa67af5fa925419c920f874954d7b5da","fb1456f4480c43b4919e11b4c4d287a9","6b8a7596fa00427e863d4fb6a444d3ed","67896273d2924b32a0176cdaeb9bd733","73a9921a14854ad4a6b049b6375504c8","1d86ba74af4c42caab980fbc9b7e93c8","8ad4b9876c584438b324916c731bbcc5","044360bd84774d19ad0bdf94ed5ba00d"]},"executionInfo":{"status":"ok","timestamp":1617041731874,"user_tz":-120,"elapsed":556525,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"4cba39d5-0835-4db6-a23a-abea91ce8ac5"},"source":["save_to = \"drive/MyDrive/data/saved_answers/saved_answers_mbert/test5.txt\"\n","\n","sqad_eval(301, 400, data, save_to, model_type)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa67af5fa925419c920f874954d7b5da","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","done\n","score: 46/100\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SwPOzAjL8OQU"},"source":["# Testing NER"]},{"cell_type":"markdown","metadata":{"id":"Ap2XIvaHuSaW"},"source":["Download required libs"]},{"cell_type":"code","metadata":{"id":"Wk-YBt3W6Wbz"},"source":["!pip install tensorflow-gpu==1.15.2\n","!pip install deeppavlov\n","!pip install git+https://github.com/deepmipt/bert.git@feat/multi_gpu"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h2gcvsNmuI9i"},"source":["Move downloaded model from drive - if you dont have the ner models downloaded, skip to \"build model\""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NtZ5HtppwGG-","executionInfo":{"status":"ok","timestamp":1617118157747,"user_tz":-120,"elapsed":1041,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"ed792b45-5899-41a3-afb1-555c80a4c4c1"},"source":["!mkdir /root/.deeppavlov\n","!mkdir /root/.deeppavlov/models/\n","!mkdir /root/.deeppavlov/downloads/"],"execution_count":11,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘/root/.deeppavlov’: File exists\n","mkdir: cannot create directory ‘/root/.deeppavlov/models/’: File exists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"l1JPRiTQs740","executionInfo":{"status":"ok","timestamp":1617118289646,"user_tz":-120,"elapsed":84824,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}}},"source":["!cp -r ./drive/MyDrive/ner_models/bert_models /root/.deeppavlov/downloads/\n","!cp -r ./drive/MyDrive/ner_models/ner_ontonotes_bert_mult /root/.deeppavlov/models/"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2hR4KCpqOR6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617118289960,"user_tz":-120,"elapsed":17342,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"ad716622-0067-4aeb-ab02-b26005dafa9c"},"source":["!ls /root/.deeppavlov/downloads\n","!ls /root/.deeppavlov/models"],"execution_count":17,"outputs":[{"output_type":"stream","text":["bert_models\n","ner_ontonotes_bert_mult\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KkPQ1N6ZuUi1"},"source":["Build model - if you don't have the model stored, use 'download=True'"]},{"cell_type":"code","metadata":{"id":"ZYhDNVOL6iaJ"},"source":["from deeppavlov import build_model, configs\n","\n","# Download and load model (set download=False to skip download phase)\n","ner = build_model(configs.ner.ner_ontonotes_bert_mult, download=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tGzxwbmuuWH-"},"source":["Testing"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sd0E9xdApCk0","executionInfo":{"status":"ok","timestamp":1617118444036,"user_tz":-120,"elapsed":3154,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"92dd119e-051e-4a5c-fe2b-f1c9721c2f99"},"source":["ner([\"Kdo byl Hitler?\"])"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[['Kdo', 'byl', 'Hitler', '?']], [['O', 'O', 'B-PERSON', 'O']]]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"F8KuCxCtNFNo"},"source":["# Ask a question"]},{"cell_type":"code","metadata":{"id":"7KyVD2CDNhcy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617042784923,"user_tz":-120,"elapsed":7236,"user":{"displayName":"jonasssnn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg6orRp2KCIDAJKs7ZSDwjymUIXGRLfN3mGHWdFAg=s64","userId":"03917409440991879060"}},"outputId":"e3211795-c4fb-45d7-dbd3-a93caf949db6"},"source":["# create translator\n","translator = Translator()\n","\n","question = \"Kdy skončila druhá světová válka?\"\n","answer = find_answer(question, reader, retriever, translator, model_type)\n","\n","if len(answer) == 0:\n","  print(answer)\n","else:\n","  print(question)\n","  print()\n","  print(answer[0])\n","  if model_type == 'albert':\n","    print(answer[1])\n","  print()\n","  #print(answer[1])\n","  print(answer[2])\n","  print(answer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Kdy skončila druhá světová válka?\n","\n","8. května 1945\n","\n","Již počátkem května vypuklo v českých zemích květnové povstání českého lidu, které se z Přerova rozšířilo do dalších měst i na venkov. 5. května začalo pražské povstání, které tři dny poté skončilo podpisem příměří a ústupem Wehrmachtu. Následující den vstoupily do Prahy v rámci Pražské operace sovětské jednotky, jež západně od města sváděly s Němci boje až do 11. května. Posledním bojem československých jednotek byla bitva o Břest, která se uskutečnila 7. května. Válka v Evropě skončila 8. května 1945, kdy vstoupila v platnost bezpodmínečná kapitulace německých ozbrojených sil. 7. září 1945 se v Berlíně konala přehlídka všech armád, které se podílely na vítězném ukončení 2. světové války.\n","('8. května 1945', '8. května 1945', 'Již počátkem května vypuklo v českých zemích květnové povstání českého lidu, které se z Přerova rozšířilo do dalších měst i na venkov. 5. května začalo pražské povstání, které tři dny poté skončilo podpisem příměří a ústupem Wehrmachtu. Následující den vstoupily do Prahy v rámci Pražské operace sovětské jednotky, jež západně od města sváděly s Němci boje až do 11. května. Posledním bojem československých jednotek byla bitva o Břest, která se uskutečnila 7. května. Válka v Evropě skončila 8. května 1945, kdy vstoupila v platnost bezpodmínečná kapitulace německých ozbrojených sil. 7. září 1945 se v Berlíně konala přehlídka všech armád, které se podílely na vítězném ukončení 2. světové války.', ['8. května 1945', '1950', 'Ve strojírenství'], [18.008934, 1.20256, -4.934227], [120.6918773651123, 14.689391613006592, -23.311197757720947], {'Druhý Mních', 'Druhá světová válka'})\n"],"name":"stdout"}]}]}