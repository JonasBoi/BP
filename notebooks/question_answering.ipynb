{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "question_answering_odevzdani.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "bPGazghDdOXn",
        "whlGzus9Z4Tx",
        "tn2ZrNBVGaHI",
        "NmXFG3Ma_Vqf",
        "31IV1omUfXNP",
        "kd1sclnsRUsK",
        "5vx4Wp01k0Gl",
        "dvvDU07zhgaa",
        "F8KuCxCtNFNo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "38d90856b5e248418a10173fd0a8ded7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_400aa95216b84efa89dbe9bf8457d303",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0255dcd51374497d9565c0f9934502f3",
              "IPY_MODEL_b6922834134f4ee180e46d57f7202c11"
            ]
          }
        },
        "400aa95216b84efa89dbe9bf8457d303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0255dcd51374497d9565c0f9934502f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f23785ce104849b6b53f343d91a653db",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13476,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13476,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ab04549f3ad540a68e3bf5df76646386"
          }
        },
        "b6922834134f4ee180e46d57f7202c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_974a38eb0b2740259c17fbe79e26d2c3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13476/13476 [3:38:23&lt;00:00,  1.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_594b09fd1cd042dab7f8933cb23f9e70"
          }
        },
        "f23785ce104849b6b53f343d91a653db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ab04549f3ad540a68e3bf5df76646386": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "974a38eb0b2740259c17fbe79e26d2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "594b09fd1cd042dab7f8933cb23f9e70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptvBUgFZkKmm"
      },
      "source": [
        "# Requirments\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwzUnPho8nCo"
      },
      "source": [
        "For everything to work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKgj8CeSViAh"
      },
      "source": [
        "# !pip install torch # if you dont run in collab\n",
        "!pip install sentencepiece\n",
        "!pip install datasets transformers\n",
        "\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install wikipedia\n",
        "!pip install rank_bm25\n",
        "!pip install majka\n",
        "!pip install corpy\n",
        "\n",
        "!pip install tensorflow-gpu==1.15.2\n",
        "!pip install deeppavlov\n",
        "!pip install git+https://github.com/deepmipt/bert.git@feat/multi_gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoTIyL_slAcA"
      },
      "source": [
        "Importing important libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRQGzm8pJ4C6"
      },
      "source": [
        "import torch\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import shutil\n",
        "import json\n",
        "import numpy as np\n",
        "import collections\n",
        "import datetime\n",
        "import warnings\n",
        "import nltk.data\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "import pickle\n",
        "from xml.dom import minidom\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from transformers import AlbertTokenizerFast, AlbertForQuestionAnswering\n",
        "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
        "\n",
        "from deeppavlov import build_model, configs\n",
        "\n",
        "from rank_bm25 import BM25Okapi, BM25Plus, BM25L\n",
        "\n",
        "import majka\n",
        "from corpy.morphodita import Tagger\n",
        "\n",
        "import wikipedia\n",
        "from googletrans import Translator\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ECjdEt9riBQ"
      },
      "source": [
        "Download and extract morphodita tagger model for lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVPYl6CEiedR"
      },
      "source": [
        "!curl --remote-name-all https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-1836{/czech-morfflex-pdt-161115.zip}\n",
        "!unzip ./czech-morfflex-pdt-161115.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmtSOXrO8zxE"
      },
      "source": [
        "Download majka database for lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYxbqP968yDO"
      },
      "source": [
        "!curl --remote-name-all https://nlp.fi.muni.cz/ma{/majka.w-lt}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPGazghDdOXn"
      },
      "source": [
        "# Reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjuzSaPEnesl"
      },
      "source": [
        "class Reader():\n",
        "\n",
        "  def __init__(self, model_checkpoint, model_type=\"albert\", max_answer_length=10, n_best_size=10, max_length=384, stride=128, use_cpu=False):\n",
        "    # load all parameters of the reader\n",
        "    self.max_answer_length = max_answer_length  # max answer span length\n",
        "    self.n_best_size = n_best_size  # \n",
        "    self.max_length = max_length  # max count of tokens in one tokenized passage\n",
        "    self.stride = stride  # the length of overlap between two mini-batches of tokenizer\n",
        "\n",
        "    # choose device; cuda if available\n",
        "    self.device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and use_cpu == False) else \"cpu\")\n",
        "\n",
        "    if model_type == 'albert':\n",
        "      # load tokenizer and model from pretrained checkpoint\n",
        "      self.tokenizer = AlbertTokenizerFast.from_pretrained(model_checkpoint)\n",
        "      # load model to device if possible\n",
        "      self.model = AlbertForQuestionAnswering.from_pretrained(model_checkpoint).to(self.device)\n",
        "    elif model_type == 'mbert':\n",
        "      # load tokenizer and model from pretrained checkpoint\n",
        "      self.tokenizer = BertTokenizerFast.from_pretrained(model_checkpoint)\n",
        "      # load model to device if possible\n",
        "      self.model = BertForQuestionAnswering.from_pretrained(model_checkpoint).to(self.device)\n",
        "    else:\n",
        "      print(\"Wrong model type parameter.\")\n",
        "      return None\n",
        "\n",
        "    print(\"Model loaded from: \" + model_checkpoint)\n",
        "    print(f\"Model has {self.count_parameters(self.model)} parameters\")\n",
        "    print(\"Device selected:\")\n",
        "    print(self.device)\n",
        "\n",
        "\n",
        "  def decode(self, output, context, offset_mappings):\n",
        "    \"\"\"\n",
        "    get the text span from the unnormalized log probabilities\n",
        "\n",
        "    method has been partly borrowed from \n",
        "    https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb\n",
        "\n",
        "    its also thoroughly commented there\n",
        "    \"\"\"\n",
        "    # enumerate over all outputs (max output size is 500 tokens in a log prob tensor)\n",
        "    valid_answers = []\n",
        "\n",
        "    for i, _ in enumerate(output.start_logits):\n",
        "\n",
        "      start_logits = output.start_logits[i].cpu().detach().numpy()\n",
        "      end_logits = output.end_logits[i].cpu().detach().numpy()\n",
        "      offset_mapping = offset_mappings[i]\n",
        "\n",
        "      # Gather the indices the best start/end logits:\n",
        "      start_indexes = np.argsort(start_logits)[-1 : - self.n_best_size - 1 : -1].tolist()\n",
        "      end_indexes = np.argsort(end_logits)[-1 : - self.n_best_size - 1 : -1].tolist()\n",
        "      for start_index in start_indexes:\n",
        "          for end_index in end_indexes:\n",
        "              # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "              # to part of the input_ids that are not in the context.\n",
        "              if (\n",
        "                  start_index >= len(offset_mapping)\n",
        "                  or end_index >= len(offset_mapping)\n",
        "                  or offset_mapping[start_index] is None\n",
        "                  or offset_mapping[end_index] is None\n",
        "              ):\n",
        "                  continue\n",
        "              # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
        "              if end_index < start_index or end_index - start_index + 1 > self.max_answer_length:\n",
        "                  continue\n",
        "              if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
        "                  start_char = offset_mapping[start_index][0]\n",
        "                  end_char = offset_mapping[end_index][1]\n",
        "                  valid_answers.append(\n",
        "                      {\n",
        "                          \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                          \"text\": context[start_char: end_char].strip()\n",
        "                      }\n",
        "                  )\n",
        "    valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:self.n_best_size]\n",
        "    return valid_answers\n",
        "\n",
        "\n",
        "  def get_answers(self, question, context):\n",
        "    \"\"\"\n",
        "    get the best answers from the context to the question \n",
        "\n",
        "    \"\"\"\n",
        "    # tokenize the input for the model using special huggingface tokenizer\n",
        "    inputs = self.tokenizer(question, context, \n",
        "                      return_tensors='pt',\n",
        "                      truncation=\"only_second\",\n",
        "                      max_length=self.max_length, # to prevent cuda running out of memory\n",
        "                      stride=self.stride,     # overlap within splitted long\n",
        "                      return_offsets_mapping=True,\n",
        "                      return_overflowing_tokens=True,\n",
        "                      padding=\"max_length\")\n",
        "    inputs.to(self.device) # port inputs to gpu\n",
        "\n",
        "    # get the model predictions\n",
        "    outputs = self.model(inputs['input_ids'], \n",
        "                    token_type_ids=inputs['token_type_ids'],\n",
        "                    attention_mask=inputs['attention_mask'])\n",
        "    \n",
        "    # use the decode function to get the n_best_size best valid answers\n",
        "    valid_answers = self.decode(outputs, context, inputs['offset_mapping'])\n",
        "    \n",
        "    return valid_answers\n",
        "\n",
        "\n",
        "  def count_parameters(self, model):\n",
        "    \"\"\"\n",
        "    Counts the parameters of the model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    return sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whlGzus9Z4Tx"
      },
      "source": [
        "# Retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iX5OVeUoufHc"
      },
      "source": [
        "class Retriever():\n",
        "\n",
        "  def __init__(self, wiki_abstracts, wiki_titles, majka_file, dita_file, index_file, use_majka=False, download_ner_model=False, model_type='mbert'):\n",
        "    # set wiki api language to search the Czech Wikipedia\n",
        "    wikipedia.set_lang(\"cs\") \n",
        "\n",
        "    # save the model type for some small nuances in the retriever\n",
        "    self.model_type = model_type\n",
        "\n",
        "    # save the most common czech words (stop words)\n",
        "    common = \"kdy být a se v na ten on že s z který mít do já o k i jeho ale svůj jako za moci pro tak po tento co když všechen už jak aby od nebo říci jeden jen můj jenž ty stát u muset chtít také až než ještě při jít pak před však ani vědět hodně podle další celý jiný mezi dát tady tam kde každý takový protože nic něco ne sám bez či dostat nějaký proto\"\n",
        "    self.common = common.split()\n",
        "\n",
        "    # save punctuation to be removed\n",
        "    punctuation = \". , ? ! ... \\\" ( ) ; - /\"\n",
        "    self.punctuation = punctuation.split()\n",
        "\n",
        "    # majka lemmatizer settings\n",
        "    self.morph = majka.Majka(majka_file)\n",
        "    self.morph.flags = 0  # unset all flags\n",
        "    self.morph.tags = False  # return just the lemma, do not process the tags\n",
        "    self.morph.first_only = True  # return only the first entry\n",
        "    self.morph.negative = \"ne\"\n",
        "    \n",
        "    print(\"Loading lemmatizer\")\n",
        "    # morphodita lemmatizer\n",
        "    self.tagger = Tagger(dita_file)\n",
        "    print(\"Lemmatizer loaded\")\n",
        "\n",
        "    print(\"Building titles index\")\n",
        "    # load wiki titles and build index for search\n",
        "    self.bm25_articles_index, self.titles = self.get_title_search_index(wiki_titles)\n",
        "    print(\"Titles index done\")\n",
        "\n",
        "    print(\"Building articles index\")\n",
        "    # load wiki abstracts and build index for search\n",
        "    self.bm25_abstract_index, self.abstract_titles, self.abstracts = self.get_abstract_search_index(wiki_abstracts, index_file)\n",
        "    print(\"articles index done\")\n",
        "\n",
        "    print(\"Loading tokenizer\")\n",
        "    # load tokenizer to split text into sentences\n",
        "    self.tokenizer = nltk.data.load('tokenizers/punkt/czech.pickle')\n",
        "    print(\"Tokenizer loaded\")\n",
        "\n",
        "    print(\"Building ner model\")\n",
        "    # Download and load model (set download=False to skip download phase)\n",
        "    self.ner = build_model(configs.ner.ner_ontonotes_bert_mult, download=download_ner_model)\n",
        "    print(\"Ner model loaded\")\n",
        "\n",
        "    # choose the correct lemmatizer\n",
        "    if use_majka:\n",
        "      self.lemmatize = self.lemmatize_majka\n",
        "      print(\"using Majka\")\n",
        "    else:\n",
        "      self.lemmatize = self.lemmatize_morphodita\n",
        "      print(\"using MorphoDiTa\")\n",
        "\n",
        "    print(\"Retriever initialized\")\n",
        "\n",
        "\n",
        "  def get_title_search_index(self, wiki_titles):\n",
        "    \"\"\"\n",
        "    Build index for searching through relevant title names on czech wiki:\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # load all the titles from the file\n",
        "    f = open(wiki_titles, \"r\")\n",
        "    titles = []\n",
        "\n",
        "    for line in f: \n",
        "      title = ((\" \").join(line.split(\"_\"))).strip()\n",
        "      title = title.strip('\\n')\n",
        "      titles.append(title)\n",
        "    f.close()\n",
        "\n",
        "    # tokenize for bm25\n",
        "    tok_titles = []\n",
        "    for title in titles:\n",
        "\n",
        "      tok_tit = re.split(\" \", title.lower())\n",
        "      for tok in tok_tit:\n",
        "        if tok == \"\":\n",
        "          tok_tit.remove(\"\")\n",
        "\n",
        "      tok_titles.append(tok_tit)\n",
        "    \n",
        "    #build index\n",
        "    bm25 = BM25Okapi(tok_titles)\n",
        "\n",
        "    return bm25, titles\n",
        "\n",
        "\n",
        "  def get_abstract_search_index(self, saved_abstracts, index_file):\n",
        "    \"\"\"\n",
        "    Build index for searching through relevant abstracts on czech wiki:\n",
        "\n",
        "    \"\"\"\n",
        "    # load abstracts and titles from preprocessed JSON file\n",
        "    with open(saved_abstracts, \"r\") as f:\n",
        "      wiki_abstracts = json.load(f)\n",
        "\n",
        "    titles = []\n",
        "    for idx in wiki_abstracts:\n",
        "      titles.append(wiki_abstracts[idx]['title'])\n",
        "\n",
        "    abstracts = []\n",
        "    for idx in wiki_abstracts:\n",
        "      abstracts.append(wiki_abstracts[idx]['abstract'])\n",
        "    \n",
        "    # if index saved, load it with pickle and return\n",
        "    if os.path.isfile(index_file):\n",
        "      with open(index_file, \"rb\") as fd:\n",
        "        print(\"loading from pickle\")\n",
        "        bm25 = pickle.load(fd)\n",
        "        return bm25, titles, abstracts\n",
        "\n",
        "    # process for creating bm25 index\n",
        "    tok_abstracts = []\n",
        "    for abstract in abstracts:\n",
        "      tok_abstract = self.delete_common(self.lemmatize(abstract.lower()))\n",
        "      tok_abstracts.append(tok_abstract)\n",
        "\n",
        "    # build index\n",
        "    bm25 = BM25Okapi(tok_abstracts)\n",
        "\n",
        "    # save index with pickle\n",
        "    with open(index_file, \"wb\") as fd:\n",
        "      pickle.dump(bm25, fd, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return bm25, titles, abstracts\n",
        "\n",
        "\n",
        "  def search_titles(self, question):\n",
        "    \"\"\"\n",
        "    Search with bm25 among the wiki titles\n",
        "\n",
        "    \"\"\"\n",
        "    tokenized_query = self.delete_common(self.lemmatize(question.lower()))\n",
        "    results = self.bm25_articles_index.get_top_n(tokenized_query, self.titles, n=5)\n",
        "\n",
        "    return results\n",
        "  \n",
        "\n",
        "  def search_abstracts(self, question):\n",
        "    \"\"\"\n",
        "    Search with bm25 among the wiki abstracts\n",
        "\n",
        "    \"\"\"\n",
        "    tokenized_query = self.delete_common(self.lemmatize(question.lower()))\n",
        "    results = self.bm25_abstract_index.get_top_n(tokenized_query, self.abstract_titles, n=5)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "  def get_named_entities(self, question):\n",
        "    \"\"\"\n",
        "    Extracts named entities from the question.\n",
        "\n",
        "    \"\"\"\n",
        "    # tag the questions\n",
        "    ner_tags = self.ner([question])\n",
        "\n",
        "    # extracts the named entities\n",
        "    named_entities = \"\"\n",
        "    for idx, tag in enumerate(ner_tags[1][0]):\n",
        "      if tag != 'O':\n",
        "        named_entities += ner_tags[0][0][idx] + \" \"\n",
        "\n",
        "    # save NEs in a list\n",
        "    NEs = []\n",
        "    if len(named_entities.strip()) != 0:\n",
        "      NEs.append(named_entities.strip())\n",
        "\n",
        "    return NEs\n",
        "\n",
        "  \n",
        "  def iscommon(self, x):\n",
        "    \"\"\"\n",
        "    decides if query token is common\n",
        "\n",
        "    \"\"\"\n",
        "    if x in self.common or x in self.punctuation:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "\n",
        "  def delete_common(self, tokens):\n",
        "    \"\"\"\n",
        "    Remove the most common czech words from the query tokens (low information value)\n",
        "\n",
        "    \"\"\"\n",
        "    tokens = [x for x in tokens if not self.iscommon(x)]\n",
        "        \n",
        "    return tokens\n",
        "\n",
        "  \n",
        "  def lemmatize_majka(self, text):\n",
        "    \"\"\"\n",
        "    Returns lemma of each token in a list of lemmatized tokens\n",
        "\n",
        "    \"\"\"\n",
        "    # tokenize\n",
        "    tok_text = re.split(\"\\W\", text)\n",
        "\n",
        "    # lemmatize each token\n",
        "    lemmatized_tokens = []\n",
        "    for token in tok_text:\n",
        "      if token == '':\n",
        "        continue\n",
        "      lemma = self.morph.find(token)\n",
        "      if len(lemma) == 0:\n",
        "        lemmatized_tokens.append(token)\n",
        "      else:\n",
        "        lemmatized_tokens.append(lemma[0]['lemma'])\n",
        "\n",
        "    return lemmatized_tokens\n",
        "  \n",
        "\n",
        "  def lemmatize_morphodita(self, text):\n",
        "    \"\"\"\n",
        "    Returns lemma of each token in a list of lemmatized tokens\n",
        "\n",
        "    \"\"\"\n",
        "    # tokenize and join again\n",
        "    # (this works better with morphodita which sometimes fails to tokenize the \n",
        "    #  text correctly if it wasnt split before like this - it just works)\n",
        "    text = re.split(\"\\W\", text)\n",
        "    text = (\" \").join(text)\n",
        "\n",
        "    tokens = list(self.tagger.tag(text, convert='strip_lemma_id'))\n",
        "\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "      lemmas.append(token.lemma)\n",
        "\n",
        "    return lemmas\n",
        "\n",
        "\n",
        "  def search_again(self, tokens):\n",
        "    \"\"\"\n",
        "    Performs repeated search in case wiki api didnt find any documents\n",
        "\n",
        "    \"\"\"\n",
        "    # join the searched tokens and try to use wiki search\n",
        "    searched_term = (' ').join(tokens)\n",
        "    if searched_term.strip() != \"\":\n",
        "      doc_list = wikipedia.search(searched_term, results=1)\n",
        "\n",
        "    # if no tokens left, end\n",
        "    if len(tokens) == 0:\n",
        "      return []\n",
        "    # if nothing was found, strip the first searched token \n",
        "    # and perform the search recursivly like this\n",
        "    if len(doc_list) == 0:\n",
        "      del tokens[0]\n",
        "      return self.search_again(tokens)\n",
        "    # return the found article\n",
        "    return doc_list\n",
        "\n",
        "\n",
        "  def get_doc_list(self, question):\n",
        "    \"\"\"\n",
        "    Returns top 1-3 wiki arcitles that might answer the question topic\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # get names entities if present\n",
        "    named_ERs = self.get_named_entities(question)\n",
        "    # get relevant article title names\n",
        "    relevant_titles = self.search_titles(question)\n",
        "    # get article titles from relevant abstracts search\n",
        "    relevant_abstracts = self.search_abstracts(question)\n",
        "\n",
        "    #search for documents - 1 article for each search method\n",
        "    max_docs = 1\n",
        "    doc_list = []\n",
        "\n",
        "    # search based on recognised named entity\n",
        "    if len(named_ERs) > 0:\n",
        "      article = wikipedia.search(named_ERs[0], results=max_docs)\n",
        "      if len(article) > 0:\n",
        "        doc_list.append(article[0])\n",
        "    # search based on best wiki title match\n",
        "    if len(relevant_titles) > 0:\n",
        "      article = wikipedia.search(relevant_titles[0], results=max_docs)\n",
        "      if len(article) > 0:\n",
        "        doc_list.append(article[0])\n",
        "    # search based on best wiki abstract match\n",
        "    if len(relevant_abstracts) > 0:\n",
        "      article = wikipedia.search(relevant_abstracts[0], results=max_docs)\n",
        "      if len(article) > 0:\n",
        "        doc_list.append(article[0])\n",
        "        \n",
        "\n",
        "    # basic search for the non-processed question\n",
        "    article = wikipedia.search(question, results=max_docs)\n",
        "    # simplify the search if its too bad and search recursively\n",
        "    if len(article) == 0:\n",
        "      # extract important for wiki\n",
        "      tokens = self.delete_common(self.lemmatize(question.lower()))\n",
        "      article = self.search_again(tokens)\n",
        "    if len(article) > 0:\n",
        "      doc_list.append(article[0])\n",
        "\n",
        "    return doc_list\n",
        "\n",
        "  \n",
        "  def normalize_length(self, par):\n",
        "    \"\"\"\n",
        "    Splits too long paragraph into smaller ones\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    #split long paragraph into sentences\n",
        "    sentences = self.tokenizer.tokenize(par)\n",
        "\n",
        "    normalized_pars = []\n",
        "    new_paragraph = \"\"\n",
        "\n",
        "    # iterate over sentences\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "      # if max paragraph length was reached, save the created paragraph\n",
        "      # and start appending to a new one\n",
        "      if len(new_paragraph) + len(sentence) > 1300:\n",
        "        normalized_pars.append(new_paragraph)\n",
        "        new_paragraph = \"\"\n",
        "        # make some overlap by taking some sentences from the previous paragraph\n",
        "        for k, trailing in enumerate(sentences[idx-2:idx]):\n",
        "          new_paragraph += trailing\n",
        "      else:\n",
        "        new_paragraph += sentence\n",
        "    \n",
        "    return normalized_pars\n",
        "\n",
        "\n",
        "  def get_wiki_page(self, doc):\n",
        "    \"\"\"\n",
        "    Get the Wikipedia page content\n",
        "\n",
        "    \"\"\"\n",
        "    # note: wikipedia api throws some errors that are hard to deal with\n",
        "    # that is the reason for this unusual structure\n",
        "    try:\n",
        "      doc = wikipedia.page(doc, auto_suggest=False)\n",
        "    except wikipedia.DisambiguationError as e:\n",
        "      s = e.options[0]\n",
        "      try:\n",
        "        doc = wikipedia.page(s, auto_suggest=False)\n",
        "      except wikipedia.DisambiguationError or wikipedia.PageError:\n",
        "        return \"not_found\"\n",
        "\n",
        "    return doc\n",
        "\n",
        "\n",
        "  def split_documents(self, doc_list):\n",
        "    \"\"\"\n",
        "    Splits each retrieved wiki article into paragraphs and normalizes its lengths\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    pars = [] # the final paragraphs that will be ranked and retrieved\n",
        "    lemm_pars = [] # processed paragraphs for building the index\n",
        "\n",
        "    # iterate over articles and process each one\n",
        "    for doc in doc_list:\n",
        "      # get whole page content\n",
        "      try:\n",
        "        doc = self.get_wiki_page(doc)\n",
        "      except wikipedia.PageError:\n",
        "        continue\n",
        "      # check if actual page was found\n",
        "      # page is of Wikipedia instance\n",
        "      # if not found, string \"not found\" is returned\n",
        "      if isinstance(doc, str):\n",
        "        continue\n",
        "\n",
        "      # remove the references part of the page \n",
        "      result = re.split('=== Reference ===|== Reference ==', doc.content)[0]\n",
        "      # split article into paragraphs\n",
        "      # this regular expression catches the headings of paragraphs\n",
        "      result = re.split('== .*. ==|\\\\n\\\\n', result)\n",
        "\n",
        "      # save stripped paragraphs\n",
        "      for par in result:\n",
        "        par = ((((par.strip()).strip('=')).strip('\\n')).strip('\\n\\n')).strip('\\r\\n')\n",
        "\n",
        "        # remove some trash -- dont know, how to do this better\n",
        "        # its something like the references part\n",
        "        # usually contains a lot of searched terms but not the answer,\n",
        "        # so its removed to not end high in the bm25 ranking\n",
        "        if par == '' or par == '\\n' or par.strip().startswith(\"Obrázky, zvuky či videa k tématu\"):\n",
        "          continue\n",
        "\n",
        "        # for albert, the max paragraph length shall be shorter due to translation limits\n",
        "        if self.model_type == 'mbert':\n",
        "          max_len = 3000\n",
        "        else:\n",
        "          max_len = 1500\n",
        "\n",
        "        # check max paragraph length \n",
        "        if len(par) > max_len:\n",
        "          # split into smaller paragraphs - normalize lengths\n",
        "          normalized_paragraphs = self.normalize_length(par)\n",
        "          # append each smaller paragraph\n",
        "          for norm_par in normalized_paragraphs:\n",
        "            # append paragraph\n",
        "            pars.append(norm_par)\n",
        "            # get lemmas and append\n",
        "            lemm_pars.append((' ').join(self.delete_common(self.lemmatize(norm_par.lower()))))\n",
        "        else:\n",
        "          # append paragraph\n",
        "          pars.append(par)\n",
        "          # get lemmas and append\n",
        "          lemm_pars.append((' ').join(self.delete_common(self.lemmatize(par.lower()))))\n",
        "\n",
        "    return pars, lemm_pars\n",
        "\n",
        "\n",
        "  def retrieve(self, question, max_docs):  \n",
        "    \"\"\"\n",
        "    Returns the top 3 paragraphs for the given question\n",
        "\n",
        "    \"\"\"\n",
        "    # check max question length - just set something\n",
        "    if len(question) > 250:\n",
        "      return \"\"\n",
        "\n",
        "    # strip questionmark - its not necessary i guess\n",
        "    question = question.strip('?')\n",
        "\n",
        "    # get relevant wiki article names\n",
        "    doc_list = self.get_doc_list(question)\n",
        "\n",
        "    # convert from list to set -- only work with unique article names\n",
        "    doc_list = set(doc_list)\n",
        "\n",
        "    # split docs into paragraphs -- this is the slowest part of the process\n",
        "    # might need optimalization\n",
        "    pars, lemm_pars = self.split_documents(doc_list)\n",
        "\n",
        "    # if we didnt find anything using the wiki api -- we need to get atleast something\n",
        "    # so we just take the reelvant abstracts and hope the answer is there\n",
        "    if len(pars) == 0:\n",
        "      # tokenize the query and get the top 5 abstracts\n",
        "      tokenized_query = self.delete_common(self.lemmatize(question.lower()))\n",
        "      results = self.bm25_abstract_index.get_top_n(tokenized_query, self.abstracts, n=5)\n",
        "\n",
        "      # perform the paragraph normalization similar to the one in split_documents()\n",
        "      # get the paragraphs and their processed versions to build its search index\n",
        "      for par in results:\n",
        "        par = par.strip()\n",
        "        # check max paragraph length\n",
        "        # for albert, the max paragraph length shall be shorter due to translation limits\n",
        "        if self.model_type == 'mbert':\n",
        "          max_len = 3000\n",
        "        else:\n",
        "          max_len = 1500\n",
        "        if len(par) > max_len:\n",
        "          # split into smaller paragraphs\n",
        "          normalized_paragraphs = self.normalize_length(par)\n",
        "          # append each smaller paragraph\n",
        "          for norm_par in normalized_paragraphs:\n",
        "            pars.append(norm_par)\n",
        "            lemm_pars.append((' ').join(self.delete_common(self.lemmatize(norm_par.lower()))))\n",
        "        else:\n",
        "          # append paragraph\n",
        "          pars.append(par)\n",
        "          # get lemmas and append\n",
        "          lemm_pars.append((' ').join(self.delete_common(self.lemmatize(par.lower()))))\n",
        "    ##############################################################################################\n",
        "\n",
        "    # tokenize for bm25\n",
        "    tok_text = []\n",
        "    for par in lemm_pars:\n",
        "      tok_par = re.split(\"\\W\", par)\n",
        "      for tok in tok_par:\n",
        "        if tok == \"\":\n",
        "          tok_par.remove(\"\")\n",
        "      tok_text.append(tok_par)\n",
        "\n",
        "    # finally build the index from the processed paragraphs\n",
        "    bm25 = BM25Plus(tok_text)\n",
        "    # either BM25 function can be used - the results are similar\n",
        "    # bm25 = BM25Okapi(tok_text)\n",
        "\n",
        "    # tokenize and lemmatize the query\n",
        "    tokenized_query = (' ').join(self.delete_common(self.lemmatize(question.lower())))\n",
        "    tokenized_query = re.split(\"\\W\", tokenized_query)\n",
        "\n",
        "    # get top n results\n",
        "    results = bm25.get_top_n(tokenized_query, pars, n=max_docs)\n",
        "\n",
        "    return results, doc_list\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def count_log_conf(best_answer, all_answers):\n",
        "    \"\"\"\n",
        "    Returns the sum of log probs \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # this function tries to take into consideration not only top answer score\n",
        "    # but also if the model chose the answer multiple times\n",
        "    # if it did, the scores are summed\n",
        "    log_conf = 0\n",
        "    for answer in all_answers:\n",
        "      if (best_answer in answer['text']) or (answer['text'] in best_answer):\n",
        "        log_conf += answer['score']\n",
        "    \n",
        "    return log_conf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn2ZrNBVGaHI"
      },
      "source": [
        "# Preprocessing SQAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nwrDl1YxY_L"
      },
      "source": [
        "Download czech sqad dataset in case it needs to be processed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgwJtaFLYTc"
      },
      "source": [
        "# download czech sqad\n",
        "!curl --remote-name-all https://lindat.cz/repository/xmlui/bitstream/handle/11234/1-3069{/sqad_v3.tar.xz}\n",
        "!tar -xf sqad_v3.tar.xz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFQTW92tCd5M"
      },
      "source": [
        "class SqadDataset():\n",
        "\"\"\"\n",
        "Process sqad into json (only questions, answers and their lemmas)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "  def __init__(self, sqad_dir, save_dir=\"./sqad_processed\", process_boolean=False):\n",
        "    self.save_dir = save_dir # the file where the processed json should be stored\n",
        "    self.sqad_dir = sqad_dir # the directory containing the sqad dataset\n",
        "    self.process_boolean = process_boolean # if false, automatically skips the yes/no questions\n",
        "\n",
        "  def extract_answer(self, dirnum):\n",
        "    \"\"\"\n",
        "    Parse the answer of current dataset record.\n",
        "    Returns the parsed answer and its lemma.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # open the corresponding file\n",
        "    f = open(f\"{self.sqad_dir}/{dirnum}/09answer_extraction.vert\", \"r\")\n",
        "    # split into lines\n",
        "    q = f.read().split(\"\\n\")\n",
        "    answer = \"\"\n",
        "    answer_lemma = \"\"\n",
        "\n",
        "    # parse answer\n",
        "    for line in q:\n",
        "      # split into columns\n",
        "      line = line.split(\"\\t\")\n",
        "      # end sign\n",
        "      if line[0] == \"</s>\":\n",
        "        break\n",
        "      \n",
        "      # get answer and its lemma\n",
        "      line_a = line[0]\n",
        "      if len(line) > 1:\n",
        "        line_a_lemma = line[1]\n",
        "        if line_a_lemma == \"[number]\":\n",
        "          line_a_lemma = line[0]\n",
        "      # process special signs - kinda annoying to deal with this\n",
        "      if line_a in {\"<s>\", \"<g/>\", \"</s>\", \"<s desamb=\\\"1\\\">\"}:\n",
        "        answer = answer[:-1]\n",
        "        answer_lemma = answer_lemma[:-1]\n",
        "        continue\n",
        "\n",
        "      # append to answer string\n",
        "      answer += line_a + \" \"\n",
        "      answer_lemma += line_a_lemma + \" \"\n",
        "    \n",
        "    f.close()\n",
        "    return answer, answer_lemma\n",
        "\n",
        "\n",
        "  def extract_question(self, dirnum):\n",
        "    \"\"\"\n",
        "    Parse the answer of current dataset record.\n",
        "    Returns the parsed answer and its lemma.\n",
        "    \n",
        "    \"\"\"\n",
        "    # open the corresponding file\n",
        "    f = open(f\"{self.sqad_dir}/{dirnum}/01question.vert\", \"r\")\n",
        "    # split into lines\n",
        "    q = f.read().split(\"\\n\")\n",
        "    question = \"\"\n",
        "\n",
        "    # iterate over lines \n",
        "    for line in q:\n",
        "      # split into columns\n",
        "      line = line.split(\"\\t\")\n",
        "      # end sign\n",
        "      if line[0] == \"</s>\" and question[-1] == \"?\":\n",
        "        break\n",
        "      # the first item in each line is the q token we want\n",
        "      line = line[0]\n",
        "      # process some annoying special signs\n",
        "      if line in {\"<s>\", \"<g/>\", \"</s>\", \"<s desamb=\\\"1\\\">\"}:\n",
        "        if line != \"<s desamb=\\\"1\\\">\":\n",
        "          question = question[:-1]\n",
        "        else:\n",
        "          question += \" \"\n",
        "        continue\n",
        "      # get the questions mark\n",
        "      if line != \"?\":\n",
        "        question += line + \" \"\n",
        "      else:\n",
        "        question += \"? \"\n",
        "\n",
        "    f.close()\n",
        "    return question\n",
        "\n",
        "\n",
        "  def process_dataset(self, from_q, to_q):\n",
        "    \"\"\"\n",
        "    Process the questions and answers from the sqad dataset and save it as a json file\n",
        "\n",
        "    Process the dataset from record from_q to record to_q\n",
        "    \n",
        "    \"\"\"\n",
        "    # save processed as directory to be dumped as json\n",
        "    sqad_dataset = {}\n",
        "    counter = from_q\n",
        "\n",
        "    # iterate in the range of the whole dataset\n",
        "    for i in tqdm(range(from_q, to_q+1)):\n",
        "\n",
        "      # get the question number\n",
        "      q_number = \"\"\n",
        "      for _ in range(len(str(i)), 6):\n",
        "        q_number += \"0\"\n",
        "      q_number += str(i)\n",
        "\n",
        "      # extract from dataset\n",
        "      question = self.extract_question(q_number)\n",
        "      correct_answer, lemmatized_answer = self.extract_answer(q_number)\n",
        "\n",
        "      # exclude yes/no questions\n",
        "      if not self.process_boolean:\n",
        "        if correct_answer.lower().strip() == \"ano\" or correct_answer.lower().strip() == \"ne\":\n",
        "          continue\n",
        "\n",
        "      # save data\n",
        "      data = {}\n",
        "      data[\"question\"] = question\n",
        "      data[\"answer\"] = correct_answer\n",
        "      data[\"answer_lemma\"] = lemmatized_answer\n",
        "\n",
        "      # we want to preserve the original question number -- two counters\n",
        "      sqad_dataset[counter] = data\n",
        "      counter += 1\n",
        "\n",
        "    # dump extracted data as json\n",
        "    with open(self.save_dir, \"w\") as f:\n",
        "      json.dump(sqad_dataset, f)\n",
        "      print(\"Sqad dataset has been processed to: \" + self.save_dir)\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def load_sqad(saved_dataset_file):\n",
        "    \"\"\"\n",
        "    Loads the saved json dataset as a dictionary\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # load preprocessed sqad dataset\n",
        "    with open(saved_dataset_file) as f: \n",
        "        data = json.load(f)\n",
        "    print(\"Sqad dataset loaded from: \" + saved_dataset_file)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "38d90856b5e248418a10173fd0a8ded7",
            "400aa95216b84efa89dbe9bf8457d303",
            "0255dcd51374497d9565c0f9934502f3",
            "b6922834134f4ee180e46d57f7202c11",
            "f23785ce104849b6b53f343d91a653db",
            "ab04549f3ad540a68e3bf5df76646386",
            "974a38eb0b2740259c17fbe79e26d2c3",
            "594b09fd1cd042dab7f8933cb23f9e70"
          ]
        },
        "id": "EHDsT3grGr-M",
        "outputId": "e94e9b30-6c40-41a6-e653-8e1b0336957f"
      },
      "source": [
        "sqad = SqadDataset(sqad_dir=\"./data/\", save_dir=\"./sqad_processed.json\")\n",
        "sqad.process_dataset(1, 13476)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38d90856b5e248418a10173fd0a8ded7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=13476.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sqad dataset has been processed to: drive/MyDrive/data/sqad_processed.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmXFG3Ma_Vqf"
      },
      "source": [
        "# Wiki abstracts parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGcTg9zq9T9H"
      },
      "source": [
        "Download wiki abstracts dump "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_yUhY-w_R8T"
      },
      "source": [
        "!curl --remote-name-all https://dumps.wikimedia.org/cswiki/latest/{cswiki-latest-abstract.xml.gz}\n",
        "!gunzip ./cswiki-latest-abstract.xml.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMvMoXS_91wy"
      },
      "source": [
        "def parse_abstracts(save_file)\n",
        "  \"\"\"\n",
        "  this function is used to parse the xml input abstracts dump to a neat json file\n",
        "\n",
        "  \"\"\"\n",
        "  # load abstracts -- eats a lot of ram and save only titles and abstracts - no links\n",
        "  xmldoc = minidom.parse('cswiki-latest-abstract.xml')\n",
        "  abstracts = xmldoc.getElementsByTagName('abstract')\n",
        "  titles = xmldoc.getElementsByTagName('title')\n",
        "\n",
        "  # strip the 'Wikipedie: ' thing from each title\n",
        "  for title in titles:\n",
        "    title.firstChild.nodeValue = title.firstChild.nodeValue.lstrip('Wikipedie: ')\n",
        "\n",
        "  # save processed dump as dict\n",
        "  wiki_abstracts = {}\n",
        "  for idx, title in enumerate(titles):\n",
        "    data = {}\n",
        "    data['title'] = title.firstChild.nodeValue\n",
        "    # skip records without abstract\n",
        "    if abstracts[idx].firstChild != None:\n",
        "      data['abstract'] = abstracts[idx].firstChild.nodeValue\n",
        "      wiki_abstracts[idx] = data\n",
        "\n",
        "  # dump extracted data as json\n",
        "  with open(save_file, \"w\") as f:\n",
        "    json.dump(wiki_abstracts, f)\n",
        "    print(\"Sqad dataset has been processed to: \" + save_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK_JXkYe9x90"
      },
      "source": [
        "save_file = \"./wiki_abstracts_processed.json\"\n",
        "process_wiki_abstracts(save_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0UIdTLs6ZxQ"
      },
      "source": [
        "Download titles:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2GxK-wa6cFd"
      },
      "source": [
        "!curl --remote-name-all https://dumps.wikimedia.org/cswiki/latest/{cswiki-latest-all-titles-in-ns0.gz}\n",
        "!gunzip ./cswiki-latest-all-titles-in-ns0.gz\n",
        "!mv cswiki-latest-all-titles-in-ns0 wiki_titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31IV1omUfXNP"
      },
      "source": [
        "# Loading model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d3vTvLMn4ua"
      },
      "source": [
        "Choose model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9-UEUNjnGsm"
      },
      "source": [
        "# either 'mbert' or 'albert'\n",
        "model_type = 'mbert'\n",
        "squadv2 = True\n",
        "\n",
        "if squadv2:\n",
        "  if model_type == 'mbert':\n",
        "    model_checkpoint = \"../data/mbert_finetuned_czech_squad2\"\n",
        "  elif model_type == 'albert':\n",
        "    model_checkpoint = \"../data/albert_squad2_finetuned\"\n",
        "  else:\n",
        "    print(\"wrong model type name\")\n",
        "else:\n",
        "  if model_type == 'mbert':\n",
        "    model_checkpoint = \"../data/mbert_finetuned_czech_squad\"\n",
        "  elif model_type == 'albert':\n",
        "    model_checkpoint = \"../data/albert_squad_finetuned\"\n",
        "  else:\n",
        "    print(\"wrong model type name\")\n",
        "\n",
        "# files with additional saved data\n",
        "\"\"\"\n",
        "depends on where you run from, you can also download and process via another cells here\n",
        "or you can use the preprocessed/downloaded data from /data directory on the SD \n",
        "\n",
        "\"\"\"\n",
        "majka_file = './majka.w-lt'\n",
        "dita_file  = \"./czech-morfflex-pdt-161115/czech-morfflex-pdt-161115.tagger\"\n",
        "wiki_titles = \"../data/wiki_titles\" \n",
        "wiki_abstracts = \"../data/wiki_abstracts_processed.json\"\n",
        "index_file = \"../data/abstracts_index.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8fIHfAgn7Oh"
      },
      "source": [
        "Create reader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A1KQGXAkbdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e337d9bd-450c-49fe-8ba7-d5489b98d097"
      },
      "source": [
        "reader = Reader(model_checkpoint, model_type=model_type)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model loaded from: ./drive/MyDrive/mbert_models/bert_finetuned_czech_squad2\n",
            "Model has 177264386 parameters\n",
            "Device selected:\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmwj-8Q_PGJQ"
      },
      "source": [
        "Create retriever (Initialization of the retriever takes about 10 minutes because of building the index for wikipedia abstracts).\n",
        "If the saved index is supplied, its much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQcGScZhPC6m"
      },
      "source": [
        "retriever = Retriever(wiki_abstracts=wiki_abstracts, wiki_titles=wiki_titles, majka_file=majka_file, dita_file=dita_file, \n",
        "                      index_file=index_file, use_majka=False, download_ner_model=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd1sclnsRUsK"
      },
      "source": [
        "# Final pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgcqZ3uTIk7c"
      },
      "source": [
        "def translate(question_cs, documents_cs, translator):\n",
        "  \"\"\"\n",
        "  Translates the czech question and documents and returns\n",
        "  question and list of documents in english\n",
        "\n",
        "  \"\"\"\n",
        "  # we concatenate the question with all the documents to be translated at once\n",
        "  # so we minimize the number of requests for googletrans\n",
        "  delimiter = \" _____ \"\n",
        "  concatenated = question_cs\n",
        "\n",
        "  for doc in documents_cs:\n",
        "    concatenated += delimiter\n",
        "    concatenated += doc\n",
        "\n",
        "  # and translate as a whole\n",
        "  if len(concatenated) > 5000:\n",
        "    concatenated = concatenated[0:4999]\n",
        "  concatenated = translator.translate(concatenated, src='cs', dest='en').text\n",
        "  # and split again\n",
        "  delimiter = \"_____\"\n",
        "  concatenated = concatenated.split(delimiter)\n",
        "  \n",
        "  # get translated question and doc\n",
        "  question = concatenated[0]\n",
        "  documents = concatenated[1:]\n",
        "\n",
        "  return question, documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlwJe2YLRWsu"
      },
      "source": [
        "def find_answer(question, reader, retriever, translator, model_type):\n",
        "  \"\"\"\n",
        "  Finds the answer to the question - connects everything \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  question_cs = question # save czech question\n",
        "\n",
        "  # retrieve the relevant paragraphs of context\n",
        "  documents_cs, article_list = retriever.retrieve(question, max_docs=10)\n",
        "\n",
        "  # for saving the best results\n",
        "  bestAnswers = []\n",
        "  bestDocs = []\n",
        "  bestLogProbs = []\n",
        "  bestSummedLogProbs = []\n",
        "  \n",
        "  # translate according to model type - mbert doesnt need translation\n",
        "  if model_type == 'mbert':\n",
        "    question = question_cs\n",
        "    documents = documents_cs\n",
        "  elif model_type == 'albert':\n",
        "    # translate question and documents for reader\n",
        "    question, documents = translate(question_cs, documents_cs, translator)\n",
        "  # delete null strings\n",
        "  documents = [x for x in documents if len(x.strip())]\n",
        "\n",
        "  # iterate over retrieved paragraphs\n",
        "  for idx, document in enumerate(documents):\n",
        "    # strip whitespaces\n",
        "    document = document.strip()\n",
        "    # chceck if any document has been found for the question\n",
        "    if document == \"\":\n",
        "      continue\n",
        "\n",
        "    if len(question) > len(document):\n",
        "      question = question.split(\"____\")[0]\n",
        "    \n",
        "    #get answer -------------------------------------------\n",
        "    answers = reader.get_answers(question, document)\n",
        "    log_conf = 0\n",
        "    log_conf_summed = 0\n",
        "\n",
        "    # choose the first valid answer - which is not empty\n",
        "    answer = ''\n",
        "    for answer in answers:\n",
        "      if answer['text'] != '':\n",
        "        log_conf = answer['score']\n",
        "        answer = answer['text']\n",
        "        log_conf_summed = Retriever.count_log_conf(answer, answers)\n",
        "        break\n",
        "    #######################################################\n",
        "    if type(answer) is not str:\n",
        "      continue # this is just to make sure that the answer is really ok\n",
        "\n",
        "    # save probs and answer\n",
        "    bestAnswers.append(answer)\n",
        "    bestLogProbs.append(log_conf)\n",
        "    bestSummedLogProbs.append(log_conf_summed)\n",
        "    # save retrieved doc\n",
        "    bestDocs.append(documents_cs[idx])\n",
        "\n",
        "  ############################################################\n",
        "  # check if any answer was found\n",
        "  if len(bestLogProbs) == 0 or bestAnswers[np.argmax(bestLogProbs, axis=0)] == '':\n",
        "    return \"\"\n",
        "\n",
        "  # get the best doc\n",
        "  # get best answer from retriever according to reader\n",
        "  document = bestDocs[np.argmax(bestLogProbs, axis=0)]\n",
        "  answer = bestAnswers[np.argmax(bestLogProbs, axis=0)]\n",
        "\n",
        "  # translate the final answer - in case of using english albert\n",
        "  answer_en = answer\n",
        "  if model_type == 'albert':\n",
        "    delimiter = \" _____ \"\n",
        "    concatenated = answer\n",
        "    for ans in bestAnswers:\n",
        "      concatenated += delimiter\n",
        "      concatenated += ans\n",
        "    # translate concatenated\n",
        "    concatenated =  translator.translate(concatenated, src='en', dest='cs').text\n",
        "\n",
        "    # and split again\n",
        "    delimiter = \"_____\"\n",
        "    concatenated = concatenated.split(delimiter)\n",
        "    \n",
        "    # get translated question and doc\n",
        "    answer = concatenated[0]\n",
        "    bestAnswers = concatenated[1:]\n",
        "\n",
        "  return answer, answer_en, document, bestAnswers, bestLogProbs, bestSummedLogProbs, article_list, bestDocs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vx4Wp01k0Gl"
      },
      "source": [
        "# Get predictions on SQAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KEOMq9OCUKS"
      },
      "source": [
        "def get_timestamp():\n",
        "    # used for unique file names\n",
        "    return datetime.datetime.now().strftime('%d-%m-%Y_%H:%M')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn5_5_YKeC0w"
      },
      "source": [
        "def sqad_eval_predictions(from_q, to_q, data, save_dir, model_type):\n",
        "  \"\"\"\n",
        "  Gets answers for required range of sqad questions.\n",
        "  Saves results as txt and json for later evaluation\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # create translator\n",
        "  translator = Translator()\n",
        "\n",
        "  # for writing structured results to json\n",
        "  results = {}\n",
        "\n",
        "  #iterate over sqad questions in required range\n",
        "  for i in tqdm(range(from_q, to_q+1)):\n",
        "\n",
        "    # get data from processed dataset\n",
        "    question = data[str(i)][\"question\"]\n",
        "    correct_answer = data[str(i)][\"answer\"]\n",
        "    lemmatized_answer = data[str(i)][\"answer_lemma\"]\n",
        "\n",
        "    # get answer and other info for the specific question\n",
        "    try:\n",
        "      answer, answer_en, document, bestAnswers, bestLogProbs, bestSummedLogProbs, article_list, best_docs = find_answer(\n",
        "        question, reader, retriever, translator, model_type)\n",
        "    except ValueError:\n",
        "      continue\n",
        "    \n",
        "    # convert set to list\n",
        "    article_list = list(article_list)\n",
        "    # convert list of floats to strings\n",
        "    bestLogProbs_string = ['{:.2f}'.format(x) for x in bestLogProbs]\n",
        "\n",
        "    # write to dict for json dump for later evaluation\n",
        "    results[i] = {\n",
        "        \"question\" : question,\n",
        "        \"answer\" : answer,\n",
        "        \"answer_orig\" : answer_en,\n",
        "        \"answer_sqad\" : correct_answer,\n",
        "        \"answer_sqad_lemma\" : lemmatized_answer,\n",
        "        \"articles\" : article_list,\n",
        "        \"document\" : document,\n",
        "        \"all_answers\" : bestAnswers,\n",
        "        \"all_log_probs\" : bestLogProbs_string,\n",
        "        \"all_documents\" : best_docs\n",
        "    }\n",
        "\n",
        "    # interim save\n",
        "    if i % 100 == 0:\n",
        "      # save interim results\n",
        "      save_file = save_dir + f\"checkpoint_{from_q}-{i}__{get_timestamp()}.json\"\n",
        "      with open(save_file, \"w\") as j:\n",
        "        json.dump(results, j)\n",
        "\n",
        "  ##############################################################################\n",
        "\n",
        "  # save results as json for later evaluation\n",
        "  save_file = save_dir + f\"answers_{from_q}-{to_q}__time:{get_timestamp()}.json\"\n",
        "  with open(save_file, \"w\") as j:\n",
        "    json.dump(results, j)\n",
        "    print()\n",
        "    print(\"Results has been saved to \" + save_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgptuBcVtl8Z"
      },
      "source": [
        "Run the evaluation and save the results to the chosen file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze44udf7oHHf"
      },
      "source": [
        "# load preprocessed sqad dataset\n",
        "data = SqadDataset.load_sqad(\"../data/sqad_processed.json\")\n",
        "save_to = \"./\"\n",
        "\n",
        "# get the model predictions on the selected range of the dataset\n",
        "sqad_eval_predictions(1, 100, data, save_to, model_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvvDU07zhgaa"
      },
      "source": [
        "# Evaluate SQAD predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBVoQuUuYSvj"
      },
      "source": [
        "# morphodita lemmatizer\n",
        "tagger = Tagger(\"./czech-morfflex-pdt-161115/czech-morfflex-pdt-161115.tagger\")\n",
        "\n",
        "def lemmatize(text):\n",
        "    \"\"\"\n",
        "    Returns lemma of each token in a list of lemmatized tokens\n",
        "    Used to lemmatize during the evaluation\n",
        "\n",
        "    \"\"\"\n",
        "    # function is thoroughly described as the retriever method - see Retriever()\n",
        "    text = re.split(\"\\W\", text)\n",
        "    text = (\" \").join(text)\n",
        "\n",
        "    tokens = list(tagger.tag(text, convert='strip_lemma_id'))\n",
        "\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "      lemmas.append(token.lemma)\n",
        "\n",
        "    return lemmas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQHJL6TYEEvT"
      },
      "source": [
        "def sqad_eval_score(results_json):\n",
        "  \"\"\"\n",
        "  Evaluates results from file 'results_json' on standard metrics\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # load results\n",
        "  with open(results_json, \"r\") as f:\n",
        "      results = json.load(f)\n",
        "\n",
        "  # for counting correct answers\n",
        "  score = 0\n",
        "  n_answers = 0\n",
        "  EM = 0\n",
        "  EM_lemma = 0\n",
        "  f1 = 0\n",
        "  f3 = 0\n",
        "  mrr = 0\n",
        "  recall = 0\n",
        "  precision = 0\n",
        "  doc_score = 0\n",
        "  doc_mrr = 0\n",
        "\n",
        "  for i in results:\n",
        "\n",
        "    # convert to lowercase and strip\n",
        "    answer = results[i][\"answer\"].lower()\n",
        "    answer_en = results[i][\"answer_orig\"].lower()\n",
        "    correct_answer = results[i][\"answer_sqad\"].lower().strip()\n",
        "    lemmatized_answer = results[i][\"answer_sqad_lemma\"].lower().strip()\n",
        "    document = results[i][\"document\"]\n",
        "    all_docs = results[i][\"all_documents\"]\n",
        "\n",
        "    # lemmatized versions of answers\n",
        "    ans_lemma = (\" \").join(lemmatize(answer))\n",
        "    corr_ans_lemma = (\" \").join(lemmatize(correct_answer))\n",
        "    lemm_ans_lemma = (\" \").join(lemmatize(lemmatized_answer))\n",
        "\n",
        "    # doc score\n",
        "    lemm_docs = [(\" \").join(lemmatize(x)) for x in all_docs]\n",
        "    for idx, doc in enumerate(all_docs):\n",
        "      if (correct_answer in doc or corr_ans_lemma in doc or lemm_ans_lemma in doc or\n",
        "          correct_answer in lemm_docs[idx] or corr_ans_lemma in lemm_docs[idx] or lemm_ans_lemma in lemm_docs[idx]):\n",
        "        doc_score += 1\n",
        "        doc_mrr += 1/(idx+1)\n",
        "        is_there = True\n",
        "        break\n",
        "\n",
        "    # try to convert numbers to the same format\n",
        "    # e.g.: 15 350 == 15350 \n",
        "    # -> convert every number to the format without the spaces\n",
        "    is_int = True\n",
        "    try:\n",
        "      answer_int = (\"\").join(answer.split(\" \"))\n",
        "      answer_int = int(answer_int)\n",
        "    except ValueError:\n",
        "      is_int = False\n",
        "    if is_int:\n",
        "      try:\n",
        "        corr_answer_int = (\"\").join(correct_answer.split(\" \"))\n",
        "        corr_answer_int = int(corr_answer_int)\n",
        "      except ValueError:\n",
        "        is_int = False\n",
        "    if is_int:\n",
        "      answer = str(answer_int)\n",
        "      correct_answer = str(corr_answer_int)\n",
        "\n",
        "\n",
        "    # first three (F3) answers match\n",
        "    all_answers = results[i][\"all_answers\"]\n",
        "    for a in all_answers:\n",
        "      if a.lower().strip() == correct_answer:\n",
        "        f3 += 1\n",
        "        break\n",
        "    \n",
        "    # MRR\n",
        "    all_scores = results[i][\"all_log_probs\"]\n",
        "    answers_scores = []\n",
        "    # iterate, get tuples of answer-scores and sort in descending order via score\n",
        "    for idx, ans_score in enumerate(all_scores):\n",
        "      answers_scores.append((float(ans_score), all_answers[idx]))\n",
        "    answers_scores = sorted(answers_scores, key=lambda answer: answer[0], reverse=True)\n",
        "    # iterate and compare with the ground truth to compute the rank of the correct answer\n",
        "    for idx, ans in enumerate(answers_scores):\n",
        "      if ans[1].lower().strip() == correct_answer:\n",
        "        mrr += 1/(idx+1)\n",
        "        break\n",
        "    \n",
        "\n",
        "    # NAIVE SCORE - increment score, if we got any match between the original and retrieved answer\n",
        "    if (   answer in correct_answer or correct_answer in answer or\n",
        "        ans_lemma in corr_ans_lemma or corr_ans_lemma in ans_lemma or\n",
        "        ans_lemma in lemm_ans_lemma or lemm_ans_lemma in ans_lemma or \n",
        "        answer_en in lemm_ans_lemma or lemm_ans_lemma in answer_en):\n",
        "      # increment score\n",
        "      score += 1\n",
        "\n",
        "    # ELM\n",
        "    if ans_lemma == (\" \").join(lemmatize(correct_answer)) or ans_lemma == (\" \").join(lemmatize(lemmatized_answer)):\n",
        "      EM_lemma += 1\n",
        "\n",
        "    # exact match\n",
        "    if (answer == correct_answer or answer_en == correct_answer):\n",
        "      EM += 1\n",
        "\n",
        "    # F1 score\n",
        "    if correct_answer.find(answer) != -1 or answer.find(correct_answer) != -1:\n",
        "      if len(answer) > len(correct_answer):\n",
        "        tp = len(correct_answer)\n",
        "        fp = len(answer) - tp\n",
        "        fn = 0\n",
        "      elif len(answer) < len(correct_answer):\n",
        "        tp = len(answer)\n",
        "        fp = 0\n",
        "        fn = len(correct_answer) - tp\n",
        "      else:\n",
        "        tp = len(correct_answer)\n",
        "        fp = 0\n",
        "        fn = 0\n",
        "\n",
        "      f_precision = tp / (tp + fp)\n",
        "      f_recall = tp / (tp + fn)\n",
        "      f1 += 2 * (f_precision * f_recall) / (f_precision + f_recall)\n",
        "    else:\n",
        "      f_recall = 0\n",
        "      f_precision = 0\n",
        "      f1 += 0\n",
        "    # save precision and recall score as well\n",
        "    recall += f_recall\n",
        "    precision += f_precision\n",
        "\n",
        "    # increment total answer count\n",
        "    n_answers += 1\n",
        "\n",
        "\n",
        "  # count the procentual scores\n",
        "  percent = (score / n_answers) * 100\n",
        "  percentEM = (EM / n_answers) * 100\n",
        "  percentEML = (EM_lemma / n_answers) * 100\n",
        "  percentF3 = (f3 / n_answers) * 100\n",
        "  percentMRR = (mrr / n_answers) * 100\n",
        "  percentRecall = (recall / n_answers) * 100\n",
        "  percentPrecision = (precision / n_answers) * 100\n",
        "  percentF1 = (f1 / n_answers) * 100\n",
        "  percentDocScore = (doc_score / n_answers) * 100\n",
        "  percentDocMrr = (doc_mrr / n_answers) * 100\n",
        "  \n",
        "  print(\"results:\")\n",
        "  print()\n",
        "  print(f\"naive score: {score}/{n_answers} ... {percent:{5}.{4}} %\")\n",
        "  print(f\"exact match: {str(EM)}/{n_answers} ... {percentEM:{5}.{4}} %\")\n",
        "  print(f\"exact match - lemma: {EM_lemma}/{n_answers} ... {percentEML:{5}.{4}} %\")\n",
        "  print()\n",
        "  print(f\"EM3 - F3: {f3}/{n_answers} ... {percentF3:{5}.{4}} %\")\n",
        "  print(f\"MRR: {float(mrr):{8}.{6}}/{n_answers} ... {percentMRR:{5}.{4}} %\")\n",
        "  print()\n",
        "  print(f\"recall: {float(recall):{8}.{6}}/{n_answers} ... {percentRecall:{5}.{4}} %\")\n",
        "  print(f\"precision: {float(precision):{8}.{6}}/{n_answers} ... {percentPrecision:{5}.{4}} %\")\n",
        "  print(f\"F1: ... {float(percentF1):{5}.{4}} %\")\n",
        "  print()\n",
        "  print(f\"document score: {doc_score}/{n_answers} ... {percentDocScore:{5}.{4}} %\")\n",
        "  print(f\"document mrr score: {doc_mrr}/{n_answers} ... {percentDocMrr:{5}.{4}} %\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8GuQq-wLKbt"
      },
      "source": [
        "sqad_eval_score(\"../benchmarks/whole_sqad_eval/8answers_1-11273__20-04-2021_14_01.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8KuCxCtNFNo"
      },
      "source": [
        "# Ask a question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KyVD2CDNhcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50a025f1-46da-4f0c-abc7-76be3083ca0d"
      },
      "source": [
        "# create translator\n",
        "if model_type == 'albert':\n",
        "  translator = Translator()\n",
        "else:\n",
        "  translator = None\n",
        "\n",
        "# type the question, get the answer\n",
        "question = \"kdy začala první světová válka\"\n",
        "answer = find_answer(question, reader, retriever, translator, model_type)\n",
        "\n",
        "# getting the data we want (if len 0 -> no answer found)\n",
        "if len(answer) == 0:\n",
        "  print(answer)\n",
        "else:\n",
        "  print(question)\n",
        "  print()\n",
        "  print(answer[0])\n",
        "  if model_type == 'albert':\n",
        "    print(answer[1])\n",
        "  print()\n",
        "  print(answer[2])\n",
        "  print()\n",
        "  print(answer[3])\n",
        "  print(answer[4])\n",
        "  print(answer[6])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "kdy začala první světová válka\n",
            "\n",
            "28. července 1914\n",
            "\n",
            "První světová válka (před rokem 1939 známá jako Velká válka nebo světová válka) byl globální válečný konflikt probíhající od 28. července 1914 do 11. listopadu 1918.První světová válka zasáhla Evropu, Afriku a Asii a bojovalo se i na světových oceánech.Formální příčinou války byl úspěšný atentát na následníka rakousko-uherského trůnu arcivévodu Františka Ferdinanda d'Este v Sarajevu dne 28. června 1914.Měsíc poté, 28. července 1914, vyhlásilo Rakousko-Uhersko odvetou válku Srbsku.Na základě předchozích aliančních smluv následovala řetězová reakce ostatních států a během zhruba čtyř týdnů se ve válečném konfliktu ocitla většina Evropy.Ve válce se střetly dvě znepřátelené koalice: Trojdohoda (dále jen „Dohoda“) a Ústřední mocnosti.Dohodovými mocnostmi byly při vypuknutí války Francie, carské Rusko a Spojené království (resp. celé Britské impérium) stojící na straně napadeného Srbska a Belgie, jejíž neutralitu na počátku války nerespektovalo během útoku na západě Německo.K Dohodě se postupně připojovaly další státy: v roce 1914 Japonsko, v roce 1915 Itálie, v roce 1916 Rumunsko a Portugalsko a v roce 1917 Spojené státy americké a Řecko.K Ústředním mocnostem Německu a Rakousko-Uhersku se v roce 1914 přidala Osmanská říše a v roce 1915 také Bulharsko.\n",
            "\n",
            "['prvním roce války', '19. století', '1939', '28. července 1914', 'těsně před']\n",
            "[-5.5801287, 5.902072, 4.6685, 6.621578, 2.0266232]\n",
            "{'První světová válka', 'Začít spolu', '7,5cm polní kanón vzor 1911'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}