{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "albert_QA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd7366b4065945979da1f9328d01a22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_25fca21b3aec4749a18ed96be39a65e2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9e63944779614e6f80900069f2bdc351",
              "IPY_MODEL_e3fc19e9fdc24293bbbcd901ebfed477"
            ]
          }
        },
        "25fca21b3aec4749a18ed96be39a65e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e63944779614e6f80900069f2bdc351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6fc92b701f8b4be99685f1e65ac22590",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 684,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 684,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e281d0c7a18643548f152d0979f1d181"
          }
        },
        "e3fc19e9fdc24293bbbcd901ebfed477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c64ff2bda2ed46d8b3a081eaf1927cac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 684/684 [00:00&lt;00:00, 27.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a26b64e5057842538569fc5ff3d9de0a"
          }
        },
        "6fc92b701f8b4be99685f1e65ac22590": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e281d0c7a18643548f152d0979f1d181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c64ff2bda2ed46d8b3a081eaf1927cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a26b64e5057842538569fc5ff3d9de0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6837be7a2c7a4c97aed2a203574bc430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d0f5a3a976ae4aaf9a336dcdb0732c48",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2fa662ae2e9d40e38d08d8df2ed5d4d7",
              "IPY_MODEL_00c7939cc7f0461980c9dadfd9828814"
            ]
          }
        },
        "d0f5a3a976ae4aaf9a336dcdb0732c48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2fa662ae2e9d40e38d08d8df2ed5d4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90eef1896e6348629659a889c42b5587",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 47376696,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 47376696,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f405c7728e1d42be9c6dabeb03173087"
          }
        },
        "00c7939cc7f0461980c9dadfd9828814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ddf11f676f5747a9843798cf12a27ef1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 47.4M/47.4M [00:00&lt;00:00, 51.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_31e3aae6c184467a92863abd125b074b"
          }
        },
        "90eef1896e6348629659a889c42b5587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f405c7728e1d42be9c6dabeb03173087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ddf11f676f5747a9843798cf12a27ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "31e3aae6c184467a92863abd125b074b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2ef56e4727444ebdad97273f80cf23cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1f0631f60f3d4af7b6e99a0e5da5efeb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_596dd95fbc534361a2bd095c6a60f2ec",
              "IPY_MODEL_1cbb29ceeb75429eb0f90a347010a692"
            ]
          }
        },
        "1f0631f60f3d4af7b6e99a0e5da5efeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "596dd95fbc534361a2bd095c6a60f2ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4502f2148e79403ab21e2f387561bca4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760289,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760289,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49b3203de6a8464583c2078cd1c59bec"
          }
        },
        "1cbb29ceeb75429eb0f90a347010a692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4acd3f86d5c7492c9707e39b96c20e44",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760k/760k [00:10&lt;00:00, 70.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_45edf71f092544dcbfb9f486175a52bd"
          }
        },
        "4502f2148e79403ab21e2f387561bca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49b3203de6a8464583c2078cd1c59bec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4acd3f86d5c7492c9707e39b96c20e44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "45edf71f092544dcbfb9f486175a52bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptvBUgFZkKmm"
      },
      "source": [
        "# Requirments\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUgwJtaFLYTc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7d0565d3-7a2a-4def-8e0b-c18f6c8cbc8a"
      },
      "source": [
        "\"\"\"\r\n",
        "# get majka database\r\n",
        "!curl --remote-name-all https://nlp.fi.muni.cz/ma{/majka.w-lt}\r\n",
        "!mv majka.w-lt drive/MyDrive/data/\r\n",
        "# download czech squad\r\n",
        "!curl --remote-name-all https://lindat.cz/repository/xmlui/bitstream/handle/11234/1-3069{/sqad_v3.tar.xz}\r\n",
        "!mv sqad_v3.tar.xz drive/MyDrive/data/\r\n",
        "!tar -xf drive/MyDrive/data/sqad_v3.tar.xz\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# get majka database\\n!curl --remote-name-all https://nlp.fi.muni.cz/ma{/majka.w-lt}\\n!mv majka.w-lt drive/MyDrive/data/\\n# download czech squad\\n!curl --remote-name-all https://lindat.cz/repository/xmlui/bitstream/handle/11234/1-3069{/sqad_v3.tar.xz}\\n!mv sqad_v3.tar.xz drive/MyDrive/data/\\n!tar -xf drive/MyDrive/data/sqad_v3.tar.xz\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKgj8CeSViAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73cf61b7-eeb6-46c5-d057-ae90b4f5c402"
      },
      "source": [
        "!pip install sentencepiece\r\n",
        "!pip install transformers\r\n",
        "!pip install googletrans==4.0.0-rc1\r\n",
        "!pip install wikipedia\r\n",
        "!pip install rank_bm25\r\n",
        "!pip install majka"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\r\u001b[K     |▎                               | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 20.5MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 15.3MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 14.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51kB 10.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61kB 12.2MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 11.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81kB 12.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 92kB 11.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102kB 10.0MB/s eta 0:00:01\r\u001b[K     |███                             | 112kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133kB 10.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143kB 10.0MB/s eta 0:00:01\r\u001b[K     |████                            | 153kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 163kB 10.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 184kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 204kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 225kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 235kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 256kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 266kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 276kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 296kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 307kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 327kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 348kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 368kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 389kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 399kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 409kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 419kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 440kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 450kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 460kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 471kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 481kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 501kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 512kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 522kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 532kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 542kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 552kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 563kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 573kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 583kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 593kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 614kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 624kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 634kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 645kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 655kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 665kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 675kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 686kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 696kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 706kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 727kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 737kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 747kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 757kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 768kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 778kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 788kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 798kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 808kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 819kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 829kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 839kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 849kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 860kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 870kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 880kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 890kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 901kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 911kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 921kB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 931kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 942kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 952kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 962kB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 972kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 983kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 993kB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.0MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1MB 10.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2MB 10.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2MB 10.0MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/87/ef312eef26f5cecd8b17ae9654cdd8d1fae1eb6dbd87257d6d73c128a4d0/transformers-4.3.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 8.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/5b/44baae602e0a30bcc53fbdbc60bd940c15e143d252d658dfdefce736ece5/tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=1dd57ca7025b9d26f539c3ec5f8f2560c58199dfa54dd71e72d7114ae46f9cf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.2\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/0d/a5fe8fb53dbf401f8a34cef9439c4c5b8f5037e20123b3e731397808d839/googletrans-4.0.0rc1.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2020.12.5)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 10.0MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hCollecting contextvars>=2.1; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/52/e64a14a99c509cbdfe0405e9f076aef0331cb9548a3efa1d5bacd524978a/immutables-0.15-cp36-cp36m-manylinux1_x86_64.whl (100kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: googletrans, contextvars\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-cp36-none-any.whl size=17417 sha256=fbcdebf97a1206876a3045fe29afa11cf6e9516bf938d67500988630213172d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/32/56/fd8940f1b3c1d77c9f91b55597c52a4d4833b000a980bb0740\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7667 sha256=e1de1a344f125e016e8831c8b8e9a9c633db9a2266b14457904a2bba95403202\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built googletrans contextvars\n",
            "Installing collected packages: rfc3986, h11, hyperframe, hpack, h2, immutables, contextvars, sniffio, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed contextvars-2.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.15 rfc3986-1.4.0 sniffio-1.2.0\n",
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=a4c9ed389cb2f25dd37cbc3b1278aa6639dee11b4deedb78fcebeb4da5f032e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n",
            "Collecting rank_bm25\n",
            "  Downloading https://files.pythonhosted.org/packages/16/5a/23ed3132063a0684ea66fb410260c71c4ffda3b99f8f1c021d1e245401b5/rank_bm25-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from rank_bm25) (1.19.5)\n",
            "Installing collected packages: rank-bm25\n",
            "Successfully installed rank-bm25-0.2.1\n",
            "Collecting majka\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/0c/92a788a342a880a676a9cf66b91ec6ec09fbabe5f87decc2cc7d1642b583/majka-0.8.tar.gz\n",
            "Building wheels for collected packages: majka\n",
            "  Building wheel for majka (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for majka: filename=majka-0.8-cp36-cp36m-linux_x86_64.whl size=70407 sha256=8146b66841d55dab9875c104154362354bc93518916bb7caa97b99311be6cfc6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/fa/b9/dccbac5d1bf537d79442725fe665ada8928b295fbe87b7d0c0\n",
            "Successfully built majka\n",
            "Installing collected packages: majka\n",
            "Successfully installed majka-0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j14NduZeKrGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3abcdf-5e64-4344-b2dc-2e585a3470d7"
      },
      "source": [
        "!wget http://www.stud.fit.vutbr.cz/~ifajcik/bissit19/evaluate_squad\r\n",
        "!mv evaluate_squad evaluate_squad.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-19 14:14:51--  http://www.stud.fit.vutbr.cz/~ifajcik/bissit19/evaluate_squad\n",
            "Resolving www.stud.fit.vutbr.cz (www.stud.fit.vutbr.cz)... 147.229.176.14, 2001:67c:1220:8b0::93e5:b00e\n",
            "Connecting to www.stud.fit.vutbr.cz (www.stud.fit.vutbr.cz)|147.229.176.14|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3440 (3.4K) [text/plain]\n",
            "Saving to: ‘evaluate_squad’\n",
            "\n",
            "evaluate_squad      100%[===================>]   3.36K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-02-19 14:14:51 (495 MB/s) - ‘evaluate_squad’ saved [3440/3440]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRQGzm8pJ4C6"
      },
      "source": [
        "import torch\r\n",
        "import torchtext.data as data\r\n",
        "import string\r\n",
        "import os\r\n",
        "import sys\r\n",
        "import time\r\n",
        "import shutil\r\n",
        "import json\r\n",
        "import spacy\r\n",
        "import logging\r\n",
        "import numpy as np\r\n",
        "import torchtext\r\n",
        "import csv\r\n",
        "import pandas as pd\r\n",
        "import torch.nn as nn\r\n",
        "import math\r\n",
        "import torch.nn.functional as F\r\n",
        "import socket\r\n",
        "import copy\r\n",
        "import datetime\r\n",
        "import warnings\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from typing import List, Tuple, Dict\r\n",
        "from torch.nn.modules.loss import _Loss, CrossEntropyLoss\r\n",
        "from torchtext.data import BucketIterator, Iterator, RawField, Example\r\n",
        "from urllib import request\r\n",
        "from spacy.tokenizer import Tokenizer\r\n",
        "from torchtext.vocab import GloVe\r\n",
        "from torch.optim import Adam\r\n",
        "\r\n",
        "from collections import defaultdict\r\n",
        "from transformers import AlbertTokenizer, AlbertForQuestionAnswering\r\n",
        "from evaluate_squad import evaluate\r\n",
        "\r\n",
        "from rank_bm25 import BM25Okapi\r\n",
        "import re\r\n",
        "import majka\r\n",
        "import wikipedia\r\n",
        "from googletrans import Translator\r\n",
        "\r\n",
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oFuZPSCKZiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b87ab8-f91e-4232-87da-7e6cfb118335"
      },
      "source": [
        "translator = Translator()\r\n",
        "wikipedia.set_lang(\"cs\") \r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH3q3NhVkQ5p"
      },
      "source": [
        "# PROTOTYP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERU6X6WWohlz"
      },
      "source": [
        "# Remove pre-cached sample data in colab's directory\n",
        "if os.path.isdir(\"sample_data\"):\n",
        "  shutil.rmtree(\"sample_data\")\n",
        "\n",
        "def get_timestamp():\n",
        "    return datetime.datetime.now().strftime('%Y-%m-%d_%H:%M')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWWMtqQ-rf5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf066593-70ac-4050-be66-93c1990f2d7a"
      },
      "source": [
        "print(\"Current working directory: \" + os.getcwd())\n",
        "print(f\"python version: {sys.version}\")\n",
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchtext version: {torchtext.__version__}\")\n",
        "print(f\"spacy version: {spacy.__version__}\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current working directory: /content\n",
            "python version: 3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n",
            "torch version: 1.7.0+cu101\n",
            "torchtext version: 0.3.1\n",
            "spacy version: 2.2.4\n",
            "Tesla T4\n",
            "Fri Feb 19 14:16:16 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8    13W /  70W |      3MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulty_w2R039z"
      },
      "source": [
        "# This code snippet will download dataset for us\n",
        "\n",
        "TRAIN_V1_URL = 'https://github.com/rajpurkar/SQuAD-explorer/raw/master/dataset/train-v1.1.json  '\n",
        "DEV_V1_URL = 'https://github.com/rajpurkar/SQuAD-explorer/raw/master/dataset/dev-v1.1.json'\n",
        "TRAIN = \"train-v1.1.json\"\n",
        "VALIDATION = \"dev-v1.1.json\"\n",
        "\n",
        "def download_url(path, url):\n",
        "    sys.stderr.write(f'Downloading from {url} into {path}\\n')\n",
        "    sys.stderr.flush()\n",
        "    request.urlretrieve(url, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0to2fxn0wBBA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0515d563-4112-44c1-d482-054ccad84913"
      },
      "source": [
        "def find_sub_list(sl, l):\n",
        "    \"\"\"\n",
        "    Methods finds sublist in list and returns its indices.\n",
        "    The indices are inclusive\n",
        "    \n",
        "    Example:\n",
        "    print(find_sub_list([3,2,1],[4,3,2,1,0]))\n",
        "    \n",
        "    Returns:\n",
        "    [(1, 3)]\n",
        "    \n",
        "    \"\"\"\n",
        "    results = []\n",
        "    sll = len(sl)\n",
        "    for ind in (i for i, e in enumerate(l) if e == sl[0]):\n",
        "        if l[ind:ind + sll] == sl:\n",
        "            results.append((ind, ind + sll - 1))\n",
        "\n",
        "    return results\n",
        "\n",
        "print(find_sub_list([3,2,1],[4,3,2,1,0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 3)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHyNAJVn_EGA"
      },
      "source": [
        "def create_custom_tokenizer(nlp):\n",
        "    custom_prefixes = [r'[0-9]+', r'\\~', r'\\–', r'\\—', r'\\$']\n",
        "    custom_infixes = [r'[!&:,()]', r'\\.', r'\\-', r'\\–', r'\\—', r'\\$']\n",
        "    custom_suffixes = [r'\\.', r'\\–', r'\\—', r'\\$']\n",
        "    default_prefixes = list(nlp.Defaults.prefixes) + custom_prefixes\n",
        "    default_prefixes.remove(r'US\\$')\n",
        "    default_prefixes.remove(r'C\\$')\n",
        "    default_prefixes.remove(r'A\\$')\n",
        "    \n",
        "    all_prefixes_re = spacy.util.compile_prefix_regex(tuple(default_prefixes))\n",
        "    infix_re = spacy.util.compile_infix_regex(tuple(list(nlp.Defaults.infixes) + custom_infixes))\n",
        "    suffix_re = spacy.util.compile_suffix_regex(tuple(list(nlp.Defaults.suffixes) + custom_suffixes))\n",
        "\n",
        "    rules = dict(nlp.Defaults.tokenizer_exceptions)\n",
        "    # remove \"a.\" to \"z.\" rules so \"a.\" gets tokenized as a|.\n",
        "    for c in range(ord(\"a\"), ord(\"z\") + 1):\n",
        "        if f\"{chr(c)}.\" in rules:\n",
        "            rules.pop(f\"{chr(c)}.\")\n",
        "\n",
        "    return Tokenizer(nlp.vocab, rules,\n",
        "                     prefix_search=all_prefixes_re.search,\n",
        "                     infix_finditer=infix_re.finditer, suffix_search=suffix_re.search,\n",
        "                     token_match=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLoQLW-OyiLL"
      },
      "source": [
        "# We will use this special token to join the pre-tokenized data\n",
        "JOIN_TOKEN = \"█\"\n",
        "\n",
        "_spacy_en = spacy.load('en')\n",
        "_spacy_en.tokenizer = create_custom_tokenizer(_spacy_en)\n",
        "\n",
        "def tokenize(text: string, tokenizer=_spacy_en):\n",
        "    tokens = [tok for tok in _spacy_en.tokenizer(text) if not tok.text.isspace()]\n",
        "    text_tokens = [tok.text for tok in tokens]\n",
        "    return tokens, text_tokens\n",
        "\n",
        "\n",
        "def tokenize_and_join(text: string, jointoken=JOIN_TOKEN):\n",
        "    return jointoken.join(tokenize(text)[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8bidRKHmTUj"
      },
      "source": [
        "class SquadDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, data, fields: List[Tuple[str, data.Field]], cachedir='./drive/MyDrive/data/squad', **kwargs):\n",
        "        # download dataset, if needed\n",
        "        self.check_for_download(cachedir)\n",
        "        \n",
        "        f = os.path.join(cachedir, data)\n",
        "        print(f)\n",
        "        \n",
        "        # The preprocessed file will be named like the original but with _preprocessed.json suffix\n",
        "        preprocessed_f = f + \"_preprocessed.json\"\n",
        "        if not os.path.exists(preprocessed_f):\n",
        "            s_time = time.time()\n",
        "            \n",
        "            # Process examples from file\n",
        "            raw_examples = SquadDataset.get_example_list(f)\n",
        "            # Save preprocessed examples, so they do not have to be processed again\n",
        "            self.save(preprocessed_f, raw_examples)\n",
        "            print(f\"Dataset {preprocessed_f} created in {time.time() - s_time}s\")\n",
        "\n",
        "        s_time = time.time()\n",
        "        \n",
        "        # Load preprocessed examples\n",
        "        examples = self.load(preprocessed_f, fields)\n",
        "        print(f\"Dataset {preprocessed_f} loaded in {time.time() - s_time:.2f} s\")\n",
        "\n",
        "        super(SquadDataset, self).__init__(examples, fields, **kwargs)\n",
        "\n",
        "    def save(self, preprocessed_f: string, raw_examples: List[Dict])-> None:\n",
        "        \"\"\"\n",
        "        Dump examples into json with name given in preprocessed_f variable.\n",
        "        \"\"\"\n",
        "        with open(preprocessed_f, \"w\") as f:\n",
        "            json.dump(raw_examples, f)\n",
        "\n",
        "    def load(self, preprocessed_f: string, fields: List[Tuple[str, RawField]]) -> List[Example]:\n",
        "        \"\"\"\n",
        "        Load preprocessed examples and construct torchtext examples from them\n",
        "        \"\"\"\n",
        "        with open(preprocessed_f, \"r\") as f:\n",
        "            raw_examples = json.load(f)\n",
        "            return [data.Example.fromlist([\n",
        "                e[\"id\"],\n",
        "                e[\"topic\"],\n",
        "                e[\"paragraph_token_positions\"],\n",
        "                e[\"raw_paragraph_context\"],\n",
        "                e[\"paragraph_context\"],\n",
        "                e[\"paragraph_context\"],\n",
        "                e[\"paragraph_context\"],\n",
        "                e[\"question\"],\n",
        "                e[\"question\"],\n",
        "                e[\"question\"],\n",
        "                e[\"a_start\"],\n",
        "                e[\"a_end\"],\n",
        "                e[\"a_extracted\"],\n",
        "                e[\"a_gt\"]\n",
        "            ], fields) for e in raw_examples]\n",
        "\n",
        "    @classmethod\n",
        "    def splits(cls, fields, cachedir='./drive/MyDrive/data/squad'):\n",
        "        \"\"\"\n",
        "        Creates train/validation data split\n",
        "        \"\"\"\n",
        "        train_data = cls(TRAIN, fields, cachedir=cachedir)\n",
        "        val_data = cls(VALIDATION, fields, cachedir=cachedir)\n",
        "        return tuple(d for d in (train_data, val_data)\n",
        "                     if d is not None)\n",
        "\n",
        "    @staticmethod\n",
        "    def check_for_download(cachedir:string):\n",
        "        \"\"\"\n",
        "        Downloads data, if possible\n",
        "        \"\"\"\n",
        "        if not os.path.exists(cachedir):\n",
        "            os.makedirs(cachedir)\n",
        "            try:\n",
        "                download_url(os.path.join(cachedir, TRAIN), TRAIN_V1_URL)\n",
        "                download_url(os.path.join(cachedir, VALIDATION), DEV_V1_URL)\n",
        "            except BaseException as e:\n",
        "                sys.stderr.write(f'Download failed, removing directory {cachedir}\\n')\n",
        "                sys.stderr.flush()\n",
        "                shutil.rmtree(cachedir)\n",
        "                \n",
        "                raise e\n",
        "        \n",
        "                \n",
        "    @staticmethod\n",
        "    def prepare_fields():\n",
        "        \"\"\"\n",
        "        Prepare torchtext fields for individual aspects of batch\n",
        "        \"\"\"\n",
        "        # field, that will process sequential text, will use vocabulary, will tokenize the text by splitting\n",
        "        # it on JOIN_TOKEN token and will lowercase the text\n",
        "        \n",
        "        # IMPORTANT: as the use_vocab=True, for this field (implicitly), the contents of this field will be automatically numericalized\n",
        "        # numericalization - the process of replacing words with their integer representations e.g.:\n",
        "        # [i, love, NLP] can be numericalized as [47,21,743]\n",
        "        WORD_field = data.Field(batch_first=True, tokenize=lambda s: str.split(s, sep=JOIN_TOKEN), lower=True)\n",
        "        \n",
        "        # field, that will not contain sequences, does not need vocabulary and will represent dependent target variable\n",
        "        TARGET_field = data.Field(sequential=False, use_vocab=False, batch_first=True, is_target=True)\n",
        "        \n",
        "        # raw field means, the field will not be processed at all\n",
        "        RAW_field = data.RawField()\n",
        "        RAW_field.is_target=False\n",
        "        return [\n",
        "            ('id', RAW_field),\n",
        "            ('topic_title', RAW_field),\n",
        "            ('document_token_positions', RAW_field),\n",
        "            ('raw_document_context', RAW_field),\n",
        "            ('document', WORD_field), # documents are processed as described with the WORD_field \n",
        "            ('document_char', RAW_field),\n",
        "            ('raw_document', RAW_field),\n",
        "            ('question', WORD_field), # questions are processed as described with the WORD_field \n",
        "            ('question_char', RAW_field),\n",
        "            ('raw_question', RAW_field),\n",
        "            # token indices of answer start and answer end are processed as described with the TARGET_field \n",
        "            (\"a_start\", TARGET_field),\n",
        "            (\"a_end\", TARGET_field),\n",
        "            \n",
        "            ('ext_answer', RAW_field),\n",
        "            ('gt_answer', RAW_field)\n",
        "        ]\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_fields_char():\n",
        "        WORD_field = data.Field(batch_first=True, tokenize=lambda s: str.split(s, sep=JOIN_TOKEN), lower=True)\n",
        "        \n",
        "        # This is field is applied on each unit of CHAR_nested_field, here we pass list as tokenize argument to split \n",
        "        # tokenized string into characters\n",
        "        CHAR_field = data.Field(batch_first=True, tokenize=list, lower=True)\n",
        "        CHAR_nested_field = data.NestedField(CHAR_field, tokenize=lambda s: str.split(s, sep=JOIN_TOKEN))\n",
        "            \n",
        "        RAW_field = data.RawField()\n",
        "        RAW_field.is_target=False\n",
        "        return [\n",
        "            ('id', RAW_field),\n",
        "            ('topic_title', RAW_field),\n",
        "            ('document_token_positions', RAW_field),\n",
        "            ('raw_document_context', RAW_field),\n",
        "            ('document', WORD_field),\n",
        "            ('document_char', CHAR_nested_field),\n",
        "            ('raw_document', RAW_field),\n",
        "            ('question', WORD_field),\n",
        "            ('question_char', CHAR_nested_field),\n",
        "            ('raw_question', RAW_field),\n",
        "            (\"a_start\", data.Field(sequential=False, use_vocab=False, batch_first=True, is_target=True)),\n",
        "            (\"a_end\", data.Field(sequential=False, use_vocab=False, batch_first=True, is_target=True)),\n",
        "            ('ext_answer', RAW_field),\n",
        "            ('gt_answer', RAW_field)\n",
        "        ]\n",
        "      \n",
        "    @staticmethod    \n",
        "    def get_example_list(file:string):\n",
        "        \"\"\"\n",
        "        Extracts processed examples from original dataset\n",
        "        \"\"\"\n",
        "        examples = []\n",
        "        cnt = 0\n",
        "\n",
        "        ## Open file for error reporting\n",
        "        f = open(f\"./drive/MyDrive/data/squad/errors_{os.path.basename(file)}.csv\", \"a+\")\n",
        "        problems = 0\n",
        "\n",
        "        # Iterate over examples in dataset\n",
        "        with open(file) as fd:\n",
        "            data_json = json.load(fd)\n",
        "            for data_topic in data_json[\"data\"]:\n",
        "                topic_title = data_topic[\"title\"]\n",
        "                for paragraph in data_topic[\"paragraphs\"]:\n",
        "                    # Tokenize document paragraph\n",
        "                    paragraph_tokens, _ = tokenize(paragraph[\"context\"])\n",
        "                    paragraph_context = paragraph[\"context\"]\n",
        "                    # Keep positions of each token in document, we will need this later, when decoding model outputs\n",
        "                    paragraph_token_positions = [[token.idx, token.idx + len(token.text)] for token in paragraph_tokens]\n",
        "                    \n",
        "                    joined_paragraph_context = paragraph_context\n",
        "                    for question_and_answers in paragraph['qas']:\n",
        "                        example_id = question_and_answers[\"id\"]\n",
        "                        question = question_and_answers['question']\n",
        "                        answers = question_and_answers['answers']\n",
        "\n",
        "                        for possible_answer in answers:\n",
        "                            answer_start_ch = possible_answer[\"answer_start\"]\n",
        "                            answer_end = possible_answer[\"answer_start\"] + len(possible_answer[\"text\"])\n",
        "                            answer_tokens, answer = tokenize(possible_answer[\"text\"])\n",
        "                            \n",
        "                            # Try finding answer in the document\n",
        "                            answer_locations = find_sub_list(answer, paragraph_context)\n",
        "                            \n",
        "                            # If we found multiple answer locations, we select the one, which is closest to the annotation\n",
        "                            if len(answer_locations) > 1:\n",
        "                                # get start character offset of each span\n",
        "                                answer_ch_starts = [paragraph_tokens[token_span[0]].idx for token_span in\n",
        "                                                    answer_locations]\n",
        "                                distance_from_gt = np.abs((np.array(answer_ch_starts) - answer_start_ch))\n",
        "                                closest_match = distance_from_gt.argmin()\n",
        "\n",
        "                                answer_start, answer_end = answer_locations[closest_match]\n",
        "                                \n",
        "                            # If we have not found answer in document, call heuristic from AllenNLP\n",
        "                            elif not answer_locations:\n",
        "                                # Call heuristic from AllenNLP to help :(\n",
        "                                token_span = char_span_to_token_span(\n",
        "                                    [(t.idx, t.idx + len(t.text)) for t in paragraph_tokens],\n",
        "                                    (answer_start_ch, answer_end))\n",
        "                                answer_start, answer_end = token_span[0]\n",
        "                                \n",
        "                            # Otherwise, everything is OK\n",
        "                            else:\n",
        "                                answer_start, answer_end = answer_locations[0]\n",
        "                            cnt += 1\n",
        "\n",
        "                            answer = possible_answer[\"text\"]\n",
        "\n",
        "                            ## Check if the token span is correct\n",
        "                            ## write correct cases into csv\n",
        "                            def is_correct():\n",
        "                                def remove_ws(s):\n",
        "                                    return \"\".join(s.split())\n",
        "\n",
        "                                csvf = csv.writer(f, delimiter=',')\n",
        "                                if remove_ws(possible_answer[\"text\"]) != remove_ws(\n",
        "                                        \"\".join(paragraph_context[answer_start:answer_end + 1])):\n",
        "                                    csvf.writerow({\"id\": example_id,\n",
        "                                                   \"topic\": topic_title,\n",
        "                                                   \"raw_paragraph_context\": paragraph[\"context\"],\n",
        "                                                   \"paragraph_context\": joined_paragraph_context,\n",
        "                                                   \"paragraph_token_positions\": paragraph_token_positions,\n",
        "                                                   \"question\": question,\n",
        "                                                   \"a_start\": answer_start,\n",
        "                                                   \"a_end\": answer_end,\n",
        "                                                   \"a_extracted\": JOIN_TOKEN.join(\n",
        "                                                       paragraph_context[answer_start:answer_end + 1]),\n",
        "                                                   \"a_gt\": possible_answer[\"text\"]}.values())\n",
        "                                    return False\n",
        "                                return True\n",
        "\n",
        "                            if not is_correct():\n",
        "                                problems += 1\n",
        "\n",
        "                            examples.append({\"id\": example_id,\n",
        "                                             \"topic\": topic_title,\n",
        "                                             \"raw_paragraph_context\": paragraph[\"context\"],\n",
        "                                             \"paragraph_context\": joined_paragraph_context,\n",
        "                                             \"paragraph_token_positions\": paragraph_token_positions,\n",
        "                                             \"question\": question,\n",
        "                                             \"a_start\": answer_start,\n",
        "                                             \"a_end\": answer_end,\n",
        "                                             \"a_extracted\": JOIN_TOKEN.join(\n",
        "                                                 paragraph_context[answer_start:answer_end + 1]),\n",
        "                                             \"a_gt\": possible_answer[\"text\"]})\n",
        "\n",
        "            # print how many problems token-span mapping problems have occured\n",
        "            print(f\"# problems: {problems}\")\n",
        "            print(f\"Problems affect {problems/len(examples)/100:.5f} % of dataset.\")\n",
        "            return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMevccsQyfk-"
      },
      "source": [
        "# Borrowed from AllenNLP\n",
        "# https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/reading_comprehension/util.py\n",
        "def char_span_to_token_span(token_offsets: List[Tuple[int, int]],\n",
        "                            character_span: Tuple[int, int]) -> Tuple[Tuple[int, int], bool]:\n",
        "    \"\"\"\n",
        "    Converts a character span from a passage into the corresponding token span in the tokenized\n",
        "    version of the passage.  If you pass in a character span that does not correspond to complete\n",
        "    tokens in the tokenized version, we'll do our best, but the behavior is officially undefined.\n",
        "    We return an error flag in this case, and have some debug logging so you can figure out the\n",
        "    cause of this issue (in SQuAD, these are mostly either tokenization problems or annotation\n",
        "    problems; there's a fair amount of both).\n",
        "    The basic outline of this method is to find the token span that has the same offsets as the\n",
        "    input character span.  If the tokenizer tokenized the passage correctly and has matching\n",
        "    offsets, this is easy.  We try to be a little smart about cases where they don't match exactly,\n",
        "    but mostly just find the closest thing we can.\n",
        "    The returned ``(begin, end)`` indices are `inclusive` for both ``begin`` and ``end``.\n",
        "    So, for example, ``(2, 2)`` is the one word span beginning at token index 2, ``(3, 4)`` is the\n",
        "    two-word span beginning at token index 3, and so on.\n",
        "    Returns\n",
        "    -------\n",
        "    token_span : ``Tuple[int, int]``\n",
        "        `Inclusive` span start and end token indices that match as closely as possible to the input\n",
        "        character spans.\n",
        "    error : ``bool``\n",
        "        Whether the token spans match the input character spans exactly.  If this is ``False``, it\n",
        "        means there was an error in either the tokenization or the annotated character span.\n",
        "    \"\"\"\n",
        "    # We have token offsets into the passage from the tokenizer; we _should_ be able to just find\n",
        "    # the tokens that have the same offsets as our span.\n",
        "    error = False\n",
        "    start_index = 0\n",
        "    while start_index < len(token_offsets) and token_offsets[start_index][0] < character_span[0]:\n",
        "        start_index += 1\n",
        "    # start_index should now be pointing at the span start index.\n",
        "    if token_offsets[start_index][0] > character_span[0]:\n",
        "        # In this case, a tokenization or labeling issue made us go too far - the character span\n",
        "        # we're looking for actually starts in the previous token.  We'll back up one.\n",
        "        start_index -= 1\n",
        "    if token_offsets[start_index][0] != character_span[0]:\n",
        "        error = True\n",
        "    end_index = start_index\n",
        "    while end_index < len(token_offsets) and token_offsets[end_index][1] < character_span[1]:\n",
        "        end_index += 1\n",
        "    if token_offsets[end_index][1] != character_span[1]:\n",
        "        error = True\n",
        "    return (start_index, end_index), error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL61dKy2PI1C"
      },
      "source": [
        "class Baseline(torch.nn.Module):\n",
        "    # We would like to define all the submodules of our model in initializer\n",
        "    def __init__(self, config, vocab):\n",
        "        super().__init__()\n",
        "        # Embedder - module that constructs token embeddings from token indices\n",
        "        self.tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "        # Encoder - which encodes our word representations\n",
        "        self.albert = AlbertForQuestionAnswering.from_pretrained('albert-base-v2')\n",
        "\n",
        "    def forward(self, batch,return_max=False):      \n",
        "\n",
        "        print(batch.raw_question)\n",
        "        print(batch.raw_document)\n",
        "        s = []\n",
        "        e = []\n",
        "        argmax_q = []\n",
        "\n",
        "        for i in range(0, len(batch.raw_question)):\n",
        "          print(\"1\")\n",
        "          inputs = self.tokenizer(batch.raw_question[i], batch.raw_document[i], return_tensors='pt')\n",
        "          print(\"2\")\n",
        "\n",
        "          start_positions = torch.tensor([1])\n",
        "          end_positions = torch.tensor([3])\n",
        "          print(\"3\")\n",
        "          print(inputs.keys())\n",
        "\n",
        "          outputs = self.albert(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
        "          print(\"4\")\n",
        "          s.append(outputs.start_logits)\n",
        "          e.append(outputs.end_logits)\n",
        "          argmax_q.append(i)\n",
        "\n",
        "          print(\"hi\")\n",
        "\n",
        "        \n",
        "        print(\"7\")\n",
        "        print(s.size())\n",
        "        print(e.size())\n",
        "        \n",
        "        if return_max:\n",
        "          return s, e, argmax_q\n",
        "        return s, e"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jNBmwNTRPO6"
      },
      "source": [
        "def decode(span_start_logits: torch.Tensor, span_end_logits: torch.Tensor) -> \\\n",
        "        Tuple[torch.Tensor,Tuple[torch.Tensor, torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    This method has been borrowed from AllenNLP\n",
        "    :param span_start_logits: unnormalized start log probabilities\n",
        "    :param span_end_logits: unnormalized end log probabilities\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # We call the inputs \"logits\" - they could either be unnormalized logits or normalized log\n",
        "    # probabilities.  A log_softmax operation is a constant shifting of the entire logit\n",
        "    # vector, so taking an argmax over either one gives the same result.\n",
        "    if span_start_logits.dim() != 2 or span_end_logits.dim() != 2:\n",
        "        raise ValueError(\"Input shapes must be (batch_size, document_length)\")\n",
        "    batch_size, passage_length = span_start_logits.size()\n",
        "    device = span_start_logits.device\n",
        "   \n",
        "  \n",
        "    \n",
        "    # span_start_logits.unsqueeze(2) has shape:\n",
        "    # (batch_size, passage_length, 1)\n",
        "    \n",
        "    # span_end_logits.unsqueeze(1) has shape:\n",
        "    # (batch_size, 1, passage_length)\n",
        "    \n",
        "    # Addition in log-domain = multiplication in real domain\n",
        "    # This will create a matrix containing addition of each span_start_logit with span_end_logit\n",
        "    # (batch_size, passage_length, passage_length)\n",
        "    span_log_probs = span_start_logits.unsqueeze(2) + span_end_logits.unsqueeze(1)\n",
        "    \n",
        "    # Only the upper triangle of the span matrix is valid; the lower triangle has entries where\n",
        "    # the span ends before it starts. We will mask these values out\n",
        "    span_log_mask = torch.triu(torch.ones((passage_length, passage_length),\n",
        "                                          device=device)).log().unsqueeze(0)\n",
        "    # The mask will look like this\n",
        "    #0000000\n",
        "    #X000000\n",
        "    #XX00000\n",
        "    #XXX0000\n",
        "    #XXXX000\n",
        "    #XXXXX00\n",
        "    #XXXXXX0\n",
        "    # where X are -infinity\n",
        "    valid_span_log_probs = span_log_probs + span_log_mask # see image above, part 1.\n",
        "        \n",
        "    \n",
        "    # Here we take the span matrix and flatten it, then find the best span using argmax.  We\n",
        "    # can recover the start and end indices from this flattened list using simple modular\n",
        "    # arithmetic.\n",
        "    # (batch_size, passage_length * passage_length)\n",
        "    # valid_span_log_probs is a vector [s_00,s_01,...,s_0n,s10,s11,...,s1n, ... , sn0,sn1,..., snn] of span scores\n",
        "    # e.g. s_01 is a score of answer span from token 0 to token 1\n",
        "    valid_span_log_probs = valid_span_log_probs.view(batch_size, -1) # see image above, part 2.\n",
        "    \n",
        "    # Turn all the log-probabilities into probabilities\n",
        "    logprobs = valid_span_log_probs\n",
        "    valid_span_probs = F.softmax(valid_span_log_probs, dim=-1)\n",
        "\n",
        "    best_span_probs, best_spans = valid_span_probs.max(-1) # see image above, part 3.\n",
        "    logprobs, _ = logprobs.max(-1)\n",
        "    # best_span_probs of shape batch_size now contains all probabilities for each best span in the batch\n",
        "    # best_spans of shape batch_size now contains argmaxes of each answer from unrolled sequence valid_span_log_probs\n",
        "    \n",
        "    span_start_indices = best_spans // passage_length\n",
        "    span_end_indices = best_spans % passage_length\n",
        "\n",
        "    return best_span_probs, (span_start_indices, span_end_indices), logprobs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crgbnvQEGXD7"
      },
      "source": [
        "def get_spans(batch, candidates):\n",
        "  r = []\n",
        "  for i in range(len(batch.raw_document_context)):\n",
        "      candidate_start = candidates[0][i]\n",
        "      candidates_end = candidates[1][i]\n",
        "      \n",
        "      # In initial state of learning, we can predict the start/end in the padding area\n",
        "      # since we do not do the masking\n",
        "      # We will fix that here.\n",
        "      if candidate_start > len(batch.document_token_positions[i]) - 1:\n",
        "          candidate_start = len(batch.document_token_positions[i]) - 1\n",
        "      if candidates_end > len(batch.document_token_positions[i]) - 1:\n",
        "          candidates_end = len(batch.document_token_positions[i]) - 1\n",
        "      \n",
        "      # If everything is OK, append (character_start,character_end) of answer span to r\n",
        "      r.append(batch.raw_document_context[i][batch.document_token_positions[i][candidate_start][0]:\n",
        "                                             batch.document_token_positions[i][candidates_end][-1]])\n",
        "  return r"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD04wGkeP2dR"
      },
      "source": [
        "def train_epoch(device, tokenizer, model: torch.nn.Module, lossfunction: _Loss, optimizer: torch.optim.Optimizer,\n",
        "              train_iter: Iterator,gradient_clipping_norm = 5.) -> float:\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  # set gradients for all parameters to 0\n",
        "  optimizer.zero_grad()\n",
        "  for i, batch in enumerate(train_iter):\n",
        "      # get the unnormalized log probabilities\n",
        "      # logprobs_S, logprobs_E = model(batch)\n",
        "\n",
        "      logprobs_S = []\n",
        "      logprobs_E = []\n",
        "      for i in range(0, len(batch.raw_question)):\n",
        "        inputs = tokenizer(batch.raw_question[i], batch.raw_document[i], return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "        start_positions = torch.tensor([1])\n",
        "        end_positions = torch.tensor([3])\n",
        "        #inputs.to(device)\n",
        "        #start_positions.to(device)\n",
        "        #end_positions.to(device)\n",
        "        outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
        "\n",
        "        logprobs_S.append(outputs.start_logits)\n",
        "        logprobs_E.append(outputs.end_logits)\n",
        "\n",
        "        print(i)\n",
        "\n",
        "      print(\"hello\")\n",
        "      print(logprobs_S)\n",
        "      print(batch.a_start)\n",
        "\n",
        "      # compute the (cross-entropy) loss for start and end separately\n",
        "      loss_s = lossfunction(logprobs_S, batch.a_start)\n",
        "      loss_e = lossfunction(logprobs_E, batch.a_end)\n",
        "      loss = loss_s + loss_e\n",
        "      \n",
        "      loss.backward() # compute gradients\n",
        "      torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), gradient_clipping_norm) # clip the gradients\n",
        "      optimizer.step() # add portion of negative gradients to model parameters\n",
        "      optimizer.zero_grad() # set gradients for all parameters to 0\n",
        "\n",
        "      train_loss += loss.item() # .item() returns integer value from 0-dimensional torch tensor (scalar).\n",
        "\n",
        "      print(f\"Training loss: {train_loss / i + 1}\")\n",
        "\n",
        "      if i % 300 == 0 and i > 0:\n",
        "          print(f\"Training loss: {train_loss / i + 1}\")\n",
        "\n",
        "  return train_loss / len(train_iter.data())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0k5D00oGaDR"
      },
      "source": [
        "@torch.no_grad() # do not create computational graph in this method, this saves memory\n",
        "def validate(model: torch.nn.Module, lossfunction: _Loss, iter: Iterator,log_results=False) -> \\\n",
        "      Tuple[float, float, float]:\n",
        "  # turn on evaluation mode (disable dropout etc.)\n",
        "  model.eval()\n",
        "\n",
        "  # initialize variables\n",
        "  results = dict()\n",
        "  ids = []\n",
        "  lossvalues = []\n",
        "  spans = []\n",
        "  gt_spans = []\n",
        "  span_probs = []\n",
        "  \n",
        "  # iterate over validation set\n",
        "  for i, batch in enumerate(iter):\n",
        "      ids += batch.id\n",
        "      \n",
        "      # get predictions\n",
        "      # logprobs_S, logprobs_E = model(batch)\n",
        "      logprobs_S = []\n",
        "      logprobs_E = []\n",
        "      for i in range(0, len(batch.raw_question)):\n",
        "        inputs = tokenizer(batch.raw_question[i], batch.raw_document[i], return_tensors='pt')\n",
        "\n",
        "        start_positions = torch.tensor([1])\n",
        "        end_positions = torch.tensor([3])\n",
        "\n",
        "        outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)\n",
        "        logprobs_S.append(outputs.start_logits)\n",
        "        logprobs_E.append(outputs.end_logits)\n",
        "        \n",
        "      # compute loss\n",
        "      loss_s = lossfunction(logprobs_S, batch.a_start)\n",
        "      loss_e = lossfunction(logprobs_E, batch.a_end)\n",
        "      loss = loss_s + loss_e\n",
        "      \n",
        "      # save loss values into list, we compute loss for each answer position\n",
        "      # but later we will pick only the best prediction\n",
        "      lossvalues += loss.tolist()\n",
        "      \n",
        "      # decode from log probabilities to predictions\n",
        "      best_span_probs, candidates, _ = decode(logprobs_S, logprobs_E)\n",
        "      span_probs += best_span_probs.tolist()\n",
        "      spans += get_spans(batch, candidates)\n",
        "      gt_spans += batch.gt_answer\n",
        "\n",
        "  # compute the final loss and results\n",
        "  # we need to filter through multiple possible choices and pick the best one\n",
        "  lossdict = defaultdict(lambda: math.inf)\n",
        "  probs = defaultdict(lambda: 0)\n",
        "  for id, value, span, span_prob in zip(ids, lossvalues, spans, span_probs):\n",
        "      # record only lowest loss\n",
        "      if lossdict[id] > value:\n",
        "          lossdict[id] = value\n",
        "      # record predicted result\n",
        "      results[id] = span\n",
        "      # record probability of predicted result\n",
        "      probs[id] = span_prob\n",
        "  \n",
        "  # results logging \n",
        "  if log_results:\n",
        "      write_results(results, probs)\n",
        "  \n",
        "  # compute loss from best answer predictions\n",
        "  loss = sum(lossdict.values()) / len(lossdict)\n",
        "  \n",
        "  # write out predictions for evaluation script\n",
        "  prediction_file = f\"./drive/MyDrive/data/squad/dev_results_{socket.gethostname()}.json\"\n",
        "  with open(prediction_file, \"w\") as f:\n",
        "      json.dump(results, f)\n",
        "\n",
        "  # initialize arguments of evaluation script\n",
        "  dataset_file = \"./drive/MyDrive/data/squad/dev-v1.1.json\"\n",
        "  expected_version = '1.1'\n",
        "  with open(dataset_file) as dataset_file:\n",
        "      dataset_json = json.load(dataset_file)\n",
        "      if (dataset_json['version'] != expected_version):\n",
        "          print('Evaluation expects v-' + expected_version +\n",
        "                       ', but got dataset with v-' + dataset_json['version'],\n",
        "                       file=sys.stderr)\n",
        "      dataset = dataset_json['data']\n",
        "  with open(prediction_file) as prediction_file:\n",
        "      predictions = json.load(prediction_file)\n",
        "  # run the evaluation script\n",
        "  result = evaluate(dataset, predictions)\n",
        "  \n",
        "\n",
        "  return loss, result[\"exact_match\"], result[\"f1\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7_zWo0jGisA"
      },
      "source": [
        "def fit(config, device):\n",
        "  # print configuration\n",
        "  # print(json.dumps(config, indent=4, sort_keys=True))\n",
        "\n",
        "  # prepare torchtext fields (different in case of character embeddings)\n",
        "  if config[\"char_embeddings\"]:\n",
        "      fields = SquadDataset.prepare_fields_char()\n",
        "  else:\n",
        "      fields = SquadDataset.prepare_fields()\n",
        "  \n",
        "  # create train/validation datasets\n",
        "  train, val = SquadDataset.splits(fields)\n",
        "  fields = dict(fields)\n",
        "  \n",
        "  # we use the same field for question and document\n",
        "  # we can build vocabulary of words it represents by calling build_vocab [this takes a while]\n",
        "  # for each used word, we can pick the glove embedding and create an embedding matrix with index to embedding mapping\n",
        "  fields[\"question\"].build_vocab(train, val, vectors=GloVe(name='6B', dim=config[\"embedding_size\"]))\n",
        "  \n",
        "  # similarly, we can build character vocabulary, if needed\n",
        "  if not type(fields[\"question_char\"]) == torchtext.data.field.RawField:\n",
        "      fields[\"question_char\"].build_vocab(train, val, max_size=config[\"char_maxsize_vocab\"])\n",
        "\n",
        "  # shuffle the examples to get the best distribution estimate\n",
        "  train_iter = BucketIterator(train, sort_key=lambda x: -(len(x.question) + len(x.document)),\n",
        "                              shuffle=True, sort=False, sort_within_batch=True,\n",
        "                              batch_size=config[\"train_batch_size\"], train=True,\n",
        "                              repeat=False,\n",
        "                              device=device)\n",
        "  \n",
        "  # sort validation examples for faster validation\n",
        "  val_iter = BucketIterator(val, sort_key=lambda x: -(len(x.question) + len(x.document)), sort=True,\n",
        "                            batch_size=config[\"validation_batch_size\"],\n",
        "                            repeat=False,\n",
        "                            device=device)\n",
        "  # create model\n",
        "  model = AlbertForQuestionAnswering.from_pretrained('albert-base-v2') #.to(device)\n",
        "\n",
        "  # create tokenizer\n",
        "  tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "  \n",
        "  # create optimizer\n",
        "  optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                   lr=config[\"learning_rate\"])\n",
        "\n",
        "  start_time = time.time()\n",
        "  best_model = None\n",
        "  try:\n",
        "      best_val_loss = math.inf\n",
        "      best_val_f1 = 0\n",
        "      best_em = 0\n",
        "      no_improvement = 0\n",
        "      for it in range(config[\"max_iterations\"]):\n",
        "          print(f\"Iteration {it}\")\n",
        "          # run training epoch\n",
        "          train_epoch(device, tokenizer, model, CrossEntropyLoss(), optimizer, train_iter)\n",
        "          \n",
        "          # validate model\n",
        "          validation_loss, em, f1 = validate(model, CrossEntropyLoss(reduction='none'), val_iter)     \n",
        "          print(f\"Validation loss/F1/EM: {validation_loss:.2f}, {f1:.2f}, {em:.2f}\")\n",
        "          \n",
        "          # increment the patience counter\n",
        "          no_improvement+=1\n",
        "          \n",
        "          # Update the best statistics if needed\n",
        "          if validation_loss < best_val_loss: \n",
        "              best_val_loss = validation_loss\n",
        "              no_improvement = 0\n",
        "          if f1 > best_val_f1: \n",
        "            best_val_f1 = f1\n",
        "          if em > best_em: \n",
        "            best_em = em\n",
        "            model = model.to(torch.device(\"cpu\"))\n",
        "            best_model = copy.deepcopy(model)\n",
        "            model = model.to(torch.device(\"cuda\"))\n",
        "          print(f\"BEST L/F1/EM = {best_val_loss:.2f}/{best_val_f1:.2f}/{best_em:.2f}\")\n",
        "          \n",
        "          # Early stopping\n",
        "          # if the validation loss did not improved for several iterations\n",
        "          # the training is finished\n",
        "          if no_improvement>=config[\"patience\"]:\n",
        "            break\n",
        "       \n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "      print('-' * 120)\n",
        "      print('Exit from training early.')\n",
        "  finally:\n",
        "      print(f'Finished after {(time.time() - start_time) / 60} minutes.')\n",
        "      return best_model,best_val_loss, best_val_f1, best_em"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5JwC_8OkgtE"
      },
      "source": [
        "train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEgl9M3Zdt96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cd7366b4065945979da1f9328d01a22a",
            "25fca21b3aec4749a18ed96be39a65e2",
            "9e63944779614e6f80900069f2bdc351",
            "e3fc19e9fdc24293bbbcd901ebfed477",
            "6fc92b701f8b4be99685f1e65ac22590",
            "e281d0c7a18643548f152d0979f1d181",
            "c64ff2bda2ed46d8b3a081eaf1927cac",
            "a26b64e5057842538569fc5ff3d9de0a",
            "6837be7a2c7a4c97aed2a203574bc430",
            "d0f5a3a976ae4aaf9a336dcdb0732c48",
            "2fa662ae2e9d40e38d08d8df2ed5d4d7",
            "00c7939cc7f0461980c9dadfd9828814",
            "90eef1896e6348629659a889c42b5587",
            "f405c7728e1d42be9c6dabeb03173087",
            "ddf11f676f5747a9843798cf12a27ef1",
            "31e3aae6c184467a92863abd125b074b",
            "2ef56e4727444ebdad97273f80cf23cd",
            "1f0631f60f3d4af7b6e99a0e5da5efeb",
            "596dd95fbc534361a2bd095c6a60f2ec",
            "1cbb29ceeb75429eb0f90a347010a692",
            "4502f2148e79403ab21e2f387561bca4",
            "49b3203de6a8464583c2078cd1c59bec",
            "4acd3f86d5c7492c9707e39b96c20e44",
            "45edf71f092544dcbfb9f486175a52bd"
          ]
        },
        "outputId": "e35ff41b-4ad9-4f14-8395-42fc3c531cf8"
      },
      "source": [
        "baseline_config = {\"modelname\": \"baseline\",\n",
        "                   \"train_batch_size\": 10,\n",
        "                   \"validation_batch_size\": 128,\n",
        "                   \"embedding_size\": 100,\n",
        "                   \"optimize_embeddings\": False,\n",
        "                   \"scale_emb_grad_by_freq\": False,\n",
        "                   \"RNN_input_dim\": 100,\n",
        "                   \"dropout_rate\": 0.2,\n",
        "                   \"RNN_nhidden\": 256,\n",
        "                   \"learning_rate\": 5e-3,\n",
        "                   \"RNN_layers\": 4,\n",
        "                   \"max_iterations\": 100,\n",
        "                   \"optimizer\": \"adam\",\n",
        "                   \"patience\":2,\n",
        "                   \"char_embeddings\": False}\n",
        "\n",
        "model, validation_loss, f1, em = fit(baseline_config, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./drive/MyDrive/data/squad/train-v1.1.json\n",
            "Dataset ./drive/MyDrive/data/squad/train-v1.1.json_preprocessed.json loaded in 14.97 s\n",
            "./drive/MyDrive/data/squad/dev-v1.1.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r.vector_cache/glove.6B.zip: 0.00B [00:00, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataset ./drive/MyDrive/data/squad/dev-v1.1.json_preprocessed.json loaded in 7.76 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:51, 2.09MB/s]                           \n",
            "100%|█████████▉| 399638/400000 [00:16<00:00, 24132.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd7366b4065945979da1f9328d01a22a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=684.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6837be7a2c7a4c97aed2a203574bc430",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=47376696.0, style=ProgressStyle(descrip…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n",
            "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ef56e4727444ebdad97273f80cf23cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 399638/400000 [00:29<00:00, 24132.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "hello\n",
            "[tensor([[ 1.7750, -0.8592,  0.4131, -0.5775,  0.5654, -0.2569, -1.3863, -0.1632,\n",
            "         -0.0298,  0.3264, -1.5523, -0.2977, -1.3396, -0.6042, -0.0657, -0.2041,\n",
            "          0.1654, -0.5210, -0.1493, -0.4607, -0.8745, -0.5276,  0.0106,  0.2108,\n",
            "         -0.1205,  0.0182,  0.6751,  0.0761,  0.1326, -0.6672, -0.4545,  0.0429,\n",
            "         -0.3767,  0.5702,  0.6710, -0.0603,  0.2435, -1.4026, -0.4942, -0.3057,\n",
            "          0.0595, -0.9795, -1.2700,  0.0358,  0.1889, -0.5704, -0.7602, -0.6766,\n",
            "         -0.5364, -0.5016, -0.5288, -0.6157,  1.3282, -1.3155, -0.2534, -0.1276,\n",
            "         -0.1010, -0.7975, -0.9446,  0.6309,  0.1327,  0.4155,  0.1285,  0.0909,\n",
            "         -0.1014,  0.3447,  0.4615, -0.1814, -0.4864, -0.2541,  0.1361, -0.1293,\n",
            "          0.0099, -0.0537, -0.3368, -0.1874,  0.4906,  0.0883, -0.5706,  0.4199,\n",
            "          0.0893,  0.5263,  0.0977,  0.1923,  0.3098, -0.0912,  0.4533, -0.1127,\n",
            "          0.8486,  0.7268,  1.1347, -0.3238, -0.4008, -1.4154, -0.5534, -0.7368,\n",
            "         -1.2301, -0.4203, -0.2598, -0.2281,  0.2028,  0.0797, -0.9252, -0.1778,\n",
            "         -0.1940,  0.2096, -0.0676,  0.7213, -0.3443,  0.7939, -0.6122,  0.4985,\n",
            "         -0.1776, -0.3373,  1.1503, -1.3143, -1.4080, -0.5823, -0.2876, -1.0406,\n",
            "          0.7358, -0.5483,  0.2903,  0.1911,  0.3779,  0.2795,  0.5799,  0.9468,\n",
            "          0.6295,  0.2509,  1.1960,  0.4103,  0.0029,  0.3300, -0.0080,  0.6910,\n",
            "          0.3249, -0.0474,  0.4333,  0.1501, -0.0721,  0.6567,  0.3056,  0.5633,\n",
            "          0.9584, -0.0025, -0.1495, -0.4830, -0.8589, -0.9776, -0.8198, -1.5134,\n",
            "          0.5915, -0.0845, -0.8033, -0.9335, -1.0566,  0.3958, -0.1351, -0.1339,\n",
            "         -0.2131, -0.3832,  0.1947,  0.2143,  0.3513, -0.2184, -0.4760,  0.3569,\n",
            "         -0.0854, -0.5903, -1.3421, -1.0415, -0.4927, -0.8839, -0.5145, -0.0763,\n",
            "         -0.2571,  0.0860,  0.9370, -1.0165,  0.8850,  0.5167, -0.3351,  0.5011,\n",
            "          0.3847, -0.4292, -0.0179, -0.2289,  0.1312, -1.2703, -1.0399,  0.8777,\n",
            "         -0.0329, -0.9327, -0.5589, -0.2923, -1.2550, -1.0834,  0.1489, -1.4788,\n",
            "         -0.8651, -0.5466, -0.9208,  0.5117,  0.2753, -1.0981,  0.4879, -0.4010,\n",
            "         -0.2960,  0.0385, -0.1385, -0.2765, -1.2021, -0.6687, -0.6229, -0.0183,\n",
            "          0.3343,  0.3184,  0.0163, -0.2969, -0.7432, -0.7417, -0.1781,  0.4698,\n",
            "         -0.2254, -0.0083, -0.3796, -0.8223,  0.5581, -0.2552, -0.7842, -1.1297,\n",
            "         -0.4678, -0.6919, -1.2457, -0.7353, -1.6284,  0.4347, -0.0778, -0.1782,\n",
            "         -0.9341, -0.3147, -0.5604, -0.2462, -0.1852,  0.4303,  0.5371, -0.0685,\n",
            "          0.0585,  0.3336, -0.0541, -0.7761, -0.0875, -0.2172,  0.0702, -0.2799,\n",
            "          0.3517,  0.5099,  0.0762, -0.0730, -0.0627,  0.4932, -0.0619, -0.6194,\n",
            "         -0.2923,  0.3083,  0.3271,  1.1997,  0.3138,  0.4644,  0.1619,  0.7341,\n",
            "         -0.0749,  0.1047,  0.2320,  0.4493,  0.4076, -0.1402, -0.0630, -0.2881,\n",
            "         -0.2724,  0.6441,  0.2518,  0.3935,  0.0661,  0.5122,  0.8538, -0.3887,\n",
            "          0.1468, -0.6162, -0.2565,  0.6612, -0.0992,  0.1497,  0.7947,  1.0740,\n",
            "          0.6781, -0.4860,  0.3264]], grad_fn=<SqueezeBackward1>), tensor([[ 1.2174,  0.8631,  0.3931,  0.3955,  0.3078,  0.0351,  0.0288,  0.0697,\n",
            "          0.1003, -0.0656,  0.2797, -0.1525,  0.2855,  0.3365,  0.1266,  0.0355,\n",
            "         -0.2854, -0.1872,  0.1617, -0.9841,  0.4314, -0.2746, -0.0518, -0.2213,\n",
            "          0.2253,  0.4788,  0.2167,  0.6637, -0.3970,  0.2543, -0.2291, -0.3314,\n",
            "         -0.1835,  0.4695, -1.7164,  0.1872,  0.2987,  0.3539,  0.3129,  0.2492,\n",
            "         -0.5188, -0.2344, -0.2701, -1.4318,  0.5535, -0.1009,  0.7498, -0.7407,\n",
            "          0.1786,  1.0022,  0.0166, -0.8777,  0.0577, -1.1600,  0.0366,  0.6873,\n",
            "          0.1002,  0.6821,  0.5629, -0.3092, -0.5750, -0.4241, -0.7677,  0.1951,\n",
            "         -0.8448, -1.6311,  0.2697, -0.3299, -1.4077, -0.1037,  0.1017,  0.0271,\n",
            "         -0.2040, -0.7634, -0.4991, -1.6610,  0.5528, -0.7384, -1.3319, -0.6972,\n",
            "          0.2233,  0.2098,  0.0145, -0.2349,  0.4461,  0.3460, -0.9351, -0.1647,\n",
            "         -0.2192, -1.0097,  0.1381, -1.6677,  0.2420,  0.3112,  0.3525,  0.1132,\n",
            "          0.6348, -0.0745, -0.3950,  0.2542, -0.6237, -0.9030,  0.2188, -0.3927,\n",
            "         -0.0602,  0.2973, -0.7136, -0.4418, -0.2223, -0.0863,  0.0473,  1.4105,\n",
            "          0.1801,  0.5048, -0.8178, -0.9113, -0.3816, -0.4450,  0.5459,  0.0581,\n",
            "          0.2204, -0.6118,  0.1545,  1.0790,  1.3028, -1.0005, -1.0092,  0.4577,\n",
            "         -1.0886, -1.0698,  0.2109,  0.0058, -0.9410,  0.5526,  0.1925,  0.1399,\n",
            "          0.4932, -0.1627, -1.8627, -0.2582, -0.0637,  0.1770, -1.2293,  0.1931,\n",
            "          0.2724, -0.6452,  0.9268,  0.7701,  0.1967, -0.0654, -0.1485, -0.3133,\n",
            "         -0.9160,  0.1696,  0.2367,  0.1670,  0.3414,  0.1829,  0.1953,  0.3365]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[ 8.7918e-01, -2.1798e-01,  3.9060e-01,  1.3970e+00,  5.2656e-02,\n",
            "          7.5172e-01,  3.1980e-01, -1.2091e+00,  1.3798e-01, -4.4989e-01,\n",
            "          4.7438e-02,  3.1873e-01, -2.5690e-01,  6.5471e-01,  4.6472e-01,\n",
            "          1.0750e+00, -6.1406e-02, -2.2300e-01,  3.2743e-02,  2.1830e-01,\n",
            "         -3.5748e-01, -7.2424e-01,  8.8031e-01,  5.6834e-02,  2.3774e-01,\n",
            "          1.0535e+00,  2.2357e-01,  6.8049e-01,  4.6093e-01, -8.8543e-02,\n",
            "         -1.1335e+00,  2.9198e-01,  2.6004e-01, -5.7509e-01, -1.1342e+00,\n",
            "          8.9109e-01,  3.3906e-01, -1.9857e-01,  3.1349e-01,  5.9698e-01,\n",
            "          4.2982e-01, -1.7819e-01,  3.2510e-01,  2.7358e-01,  2.4192e-01,\n",
            "          2.1291e-02, -1.1565e+00,  8.8696e-02, -3.0151e-01,  5.5703e-01,\n",
            "          7.2636e-01,  4.2797e-01,  8.3944e-02, -1.6043e-01, -7.1167e-01,\n",
            "         -1.0307e+00, -4.6385e-01,  3.2201e-02, -4.0855e-01,  3.8400e-01,\n",
            "         -5.2063e-01,  5.4970e-01, -3.3717e-01, -5.2133e-01, -3.9302e-01,\n",
            "         -7.3950e-01, -8.8270e-01,  4.4343e-01, -2.9889e-01, -3.5499e-01,\n",
            "         -1.3978e+00,  4.6061e-01,  2.1293e-01, -7.3840e-01,  5.5551e-02,\n",
            "          1.1719e-01, -4.8700e-01, -2.8919e-01,  3.5163e-02,  3.4832e-01,\n",
            "         -7.1012e-02, -1.1731e-01,  5.1801e-01,  5.5272e-02,  5.3167e-01,\n",
            "         -2.8744e-01, -1.8914e-01, -4.3950e-01,  3.4482e-03,  1.0697e-01,\n",
            "          4.9801e-01, -4.8404e-01,  1.4885e-01, -2.8562e-01, -6.9350e-01,\n",
            "          8.3406e-01, -1.1171e-01, -1.9613e-01, -1.2247e-01, -4.4850e-01,\n",
            "         -8.2686e-02,  4.6728e-01, -9.9082e-01,  5.6671e-01, -1.8699e-01,\n",
            "          1.1929e-01,  8.0022e-01,  2.7109e-01, -2.0760e-01,  2.8599e-01,\n",
            "         -8.1108e-02, -1.8822e-01,  2.9755e-01,  1.1031e+00, -1.4762e-02,\n",
            "          1.1412e-01,  6.6095e-01, -6.8059e-02, -1.0140e-02,  7.1893e-01,\n",
            "          5.8693e-01, -2.0866e-01,  5.0728e-01,  6.5715e-01, -9.5963e-01,\n",
            "         -4.1449e-01, -3.7340e-01, -4.2163e-01,  1.1949e-01,  3.2198e-01,\n",
            "         -1.6466e-01, -9.5382e-03, -5.7166e-01,  2.2105e-01,  9.4450e-01,\n",
            "         -3.7078e-01,  3.9942e-01, -5.6751e-01, -3.5035e-01,  3.9845e-02,\n",
            "         -2.1127e-01,  2.1976e-03, -1.0189e+00, -1.0077e-01,  9.8861e-02,\n",
            "          1.4673e-01, -8.8964e-02,  2.7242e-01, -1.1761e-01,  3.9232e-04,\n",
            "         -4.0943e-01,  3.6375e-01,  6.6064e-02, -1.4257e+00,  5.5705e-02,\n",
            "          2.1935e-01, -4.1798e-01,  3.7356e-01, -3.6664e-01, -3.7282e-01,\n",
            "          3.7570e-01,  3.1366e-01, -7.1574e-01, -1.6812e-01,  7.9660e-01,\n",
            "         -3.9959e-01,  4.1993e-01,  4.7373e-01, -3.3577e-01,  3.1873e-01]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[ 1.5139, -0.9669,  0.3302,  1.1171,  0.2189, -0.1146,  0.2051, -0.8720,\n",
            "          0.2396,  1.0835,  0.3173,  0.2953, -0.2099, -0.0703,  0.2736,  0.4933,\n",
            "          0.3167, -1.3174, -0.2042,  0.0831, -1.1383, -0.2414,  0.2432,  0.7004,\n",
            "         -0.2374,  0.3723,  0.7887,  0.0160,  0.2168,  0.2490,  0.3225,  0.7561,\n",
            "          0.2861,  1.0578, -0.8373,  0.0518,  0.2904, -0.7177,  0.0267,  0.0408,\n",
            "         -0.3856, -0.0708, -0.0198, -0.2672, -0.0277,  0.9297,  0.7686,  0.0148,\n",
            "         -0.0605, -0.2173, -0.8573,  0.3107,  0.1246, -0.0506, -0.3590, -0.3608,\n",
            "         -0.3009, -0.7501, -0.9414, -0.0024, -0.5022,  0.4471, -0.2577, -0.5978,\n",
            "         -0.6421,  0.1437, -1.1356,  0.8178,  0.1468,  0.5203, -0.3310, -0.4165,\n",
            "          0.3189, -0.3210, -0.4502, -0.2862, -1.1670, -0.7320, -0.9534,  0.3331,\n",
            "          0.1320, -0.4730, -0.2830, -0.9469,  0.6423,  0.1559,  0.0493,  0.2563,\n",
            "          0.2899, -0.1832,  1.0747,  0.6172, -0.2046,  0.4000,  0.4312, -0.3045,\n",
            "         -0.2066,  0.2176,  0.3852,  0.8090,  0.9550, -0.5986,  0.1337,  0.3525,\n",
            "          0.1568,  0.5500,  0.7679,  0.1958, -0.2365,  0.6670,  0.9138, -0.1337,\n",
            "         -0.0066, -0.4762, -0.2731,  0.5938,  0.2326, -0.4006, -0.2178,  0.0397,\n",
            "         -0.6858,  0.7403, -0.2572,  0.2394,  0.5767,  0.6174, -0.4284,  0.1267,\n",
            "          0.7003, -0.0549, -0.1531,  0.6742,  0.1462,  0.1690,  0.4727, -0.2525,\n",
            "         -0.4902, -0.3284,  0.1586,  0.6346,  0.2877, -0.1400,  0.7905,  0.1335,\n",
            "         -1.0925,  0.6410,  0.0075,  0.2531, -0.2566,  0.3016, -1.3206, -1.2865,\n",
            "         -0.0133,  0.1843,  0.6975,  0.8873, -0.0155, -0.3946,  0.1080, -0.1217,\n",
            "          0.2836,  0.1724, -0.6681, -0.6166,  0.1752,  0.2818,  0.2213, -0.1123,\n",
            "         -0.0950, -1.0333, -0.0898,  0.2084,  0.2717, -0.1457, -0.3259,  0.2310,\n",
            "          0.4814,  0.4892,  0.0391,  0.2389,  0.4031,  1.0222,  0.6503, -0.3248,\n",
            "          0.3167]], grad_fn=<SqueezeBackward1>), tensor([[ 1.3580e+00,  5.5255e-01,  2.8041e-01, -2.9729e-01, -1.9006e-02,\n",
            "          9.7237e-01,  3.9251e-02,  3.5960e-01, -6.0347e-01, -6.2001e-01,\n",
            "         -6.3917e-01,  6.8175e-01,  5.2560e-01, -5.0674e-01,  8.6860e-01,\n",
            "         -4.1170e-01,  1.4005e-01, -1.2758e-01,  3.1541e-01,  3.0559e-01,\n",
            "          2.3845e-01, -9.0480e-01, -4.2178e-01,  6.0059e-01,  5.5509e-01,\n",
            "          4.8463e-01,  7.4179e-02,  7.1463e-01,  1.2081e+00,  1.1513e+00,\n",
            "          6.1297e-01,  6.7536e-01,  3.6718e-01,  4.3579e-01,  7.0754e-01,\n",
            "          1.4246e+00,  8.1515e-02,  1.8961e-01,  4.5107e-01,  9.1331e-02,\n",
            "         -2.7395e-01,  2.8077e-01,  7.0103e-01, -2.9970e-01,  4.7359e-01,\n",
            "          1.2775e+00,  1.7501e-01,  1.9561e-02,  2.7772e-01,  6.3328e-01,\n",
            "          6.1558e-01,  8.2015e-01,  1.5303e-01,  5.2419e-01, -5.4062e-01,\n",
            "         -9.8884e-01, -1.3354e+00,  3.2754e-01,  1.0031e-01,  3.6145e-01,\n",
            "         -2.0615e-01,  3.1891e-01,  2.0700e-01, -1.7484e-01, -1.5761e-01,\n",
            "         -5.6297e-01,  7.2703e-01,  4.9416e-01,  1.5366e-01,  5.1630e-01,\n",
            "         -2.5990e-01,  7.4528e-02,  1.1784e+00, -4.5895e-01, -1.1951e-02,\n",
            "          2.9278e-02,  6.4653e-02, -6.1184e-03,  2.8645e-02,  3.5227e-01,\n",
            "          5.9871e-01, -6.0843e-01, -3.2110e-01,  2.3648e-01,  9.0389e-02,\n",
            "          1.0183e+00, -2.3949e-01,  2.3903e-01,  4.4438e-01,  5.8795e-01,\n",
            "          8.8555e-01,  5.9233e-01, -4.0605e-01, -8.9147e-01, -1.1336e+00,\n",
            "          5.0940e-01, -2.0852e-01,  1.9926e-01, -3.2318e-01,  3.8289e-02,\n",
            "         -8.9955e-02, -1.3423e-02, -8.9710e-01,  1.0486e+00,  4.9160e-01,\n",
            "          3.7335e-01,  1.4833e-01,  1.6177e-01,  5.5064e-01, -1.9128e-01,\n",
            "          1.0551e+00,  4.9994e-01,  5.7079e-01,  7.6960e-01,  5.3278e-01,\n",
            "          9.6851e-01, -5.7119e-01, -1.1030e+00, -1.3411e+00, -2.5211e-01,\n",
            "         -4.8788e-01,  5.0655e-01, -7.2237e-01, -1.1647e-01,  5.8278e-01,\n",
            "          5.6790e-02,  2.1718e-01,  7.2089e-02, -3.6333e-04,  6.9144e-01,\n",
            "          5.4270e-01,  9.4534e-01,  6.8780e-01,  4.9235e-01, -1.3538e-02,\n",
            "          1.2138e+00,  1.4776e-01, -1.5832e-01,  8.8618e-03,  3.3844e-01,\n",
            "         -4.4661e-01,  4.9185e-01, -3.6080e-01,  3.4718e-01,  1.1269e+00,\n",
            "          1.1916e-01,  3.1591e-01,  6.6124e-01, -5.7241e-01,  4.6495e-01,\n",
            "          1.0797e+00, -2.5987e-01, -3.2187e-01, -3.3026e-02,  3.8579e-01,\n",
            "          9.1024e-01,  1.0627e+00,  1.0568e+00,  8.7542e-01, -1.5959e-02,\n",
            "          3.1014e-01,  4.2879e-01, -3.3950e-01,  7.4705e-01, -3.3857e-01,\n",
            "          2.3802e-01,  3.2037e-01, -5.0380e-01,  3.7108e-01,  8.6548e-01,\n",
            "         -5.6111e-01,  3.1542e-01]], grad_fn=<SqueezeBackward1>), tensor([[ 1.3414, -0.1737, -0.6095, -0.2133,  0.2544,  1.1024, -0.6964, -0.3425,\n",
            "          0.2352,  0.7095, -0.2004,  0.1553,  0.4906,  0.3185,  0.2554, -0.0326,\n",
            "         -0.8470,  0.6463,  0.2956,  0.8583,  1.0421,  0.6842, -0.8356, -0.2516,\n",
            "         -0.2583, -0.2976,  0.1124, -0.1891,  0.1353,  0.0233,  0.5446,  0.4425,\n",
            "         -0.5835, -0.2238, -0.1290,  0.1789,  0.4901,  0.1141, -0.0497, -0.3403,\n",
            "          0.0053, -0.1297,  0.0620,  0.8889, -0.1886, -0.6410,  0.0189,  0.0238,\n",
            "          0.5314,  0.3148,  0.3514, -0.1930,  0.0376,  0.0539, -0.1332, -0.7171,\n",
            "          0.3432,  0.1031,  0.3625,  0.3390, -0.2637,  0.6208,  0.0723,  0.6748,\n",
            "          0.2818, -0.1850, -0.7067,  0.1841,  1.3770,  0.3867, -0.5387,  0.0336,\n",
            "         -0.3275,  0.6970,  0.4398,  0.2826,  1.2225,  0.4416,  0.0832, -0.0827,\n",
            "          0.5944,  1.0753,  0.3823,  0.4821,  0.6062,  0.5961,  0.6543,  0.6256,\n",
            "          0.1611,  0.1902, -0.7281, -0.1607,  0.0075,  0.0640, -0.3974, -0.5488,\n",
            "          0.0025, -0.1397, -0.7702, -0.3233, -0.2950,  0.0179,  0.3828, -0.4490,\n",
            "          0.2558, -0.1887,  0.1054,  0.4080, -0.1230,  0.3894, -0.2269, -0.1092,\n",
            "         -0.3379,  0.0652,  0.2224,  0.7409,  0.2696, -1.0499,  1.2567,  0.5511,\n",
            "         -0.2336,  0.0984, -0.0303, -0.1339,  0.3681, -0.7473, -0.4808,  0.5013,\n",
            "         -0.7530,  0.4516, -0.2513,  0.0698, -0.0044,  0.3185]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[ 1.6201e+00,  5.5634e-01,  1.1363e-01,  9.3639e-02, -6.0251e-01,\n",
            "         -7.9903e-01, -5.8514e-01,  3.6634e-01,  2.0101e-01,  3.4212e-01,\n",
            "         -1.0857e-03,  5.8914e-01,  3.1836e-01, -4.3981e-01,  4.3019e-01,\n",
            "          2.2298e-01,  8.4148e-01,  3.9075e-01,  5.4205e-02, -9.6725e-01,\n",
            "         -1.9714e-01, -5.6997e-01, -3.2722e-01,  7.3757e-01, -1.4262e+00,\n",
            "         -1.3306e+00,  8.5172e-02,  1.9039e-02,  6.9018e-01,  3.0820e-01,\n",
            "          8.3033e-02, -8.9282e-01, -3.8175e-02, -7.5869e-01,  4.1015e-01,\n",
            "          8.0949e-02,  2.5522e-01,  2.5984e-01,  2.7433e-02,  6.3443e-01,\n",
            "          3.4507e-02, -2.5399e-01, -8.5657e-01, -1.0304e-01,  9.5797e-04,\n",
            "          2.9822e-01, -2.9422e-01,  1.5313e-01,  3.9229e-01, -7.1963e-02,\n",
            "         -3.4114e-01, -2.0389e-01,  4.1568e-01, -5.1237e-01,  2.9278e-02,\n",
            "         -4.2570e-02,  2.6553e-01, -1.1108e+00,  1.7575e-01,  9.0687e-02,\n",
            "         -1.7920e+00, -1.5358e-01, -3.0866e-01, -6.4495e-01,  2.5396e-01,\n",
            "          1.7815e-01,  8.7424e-01, -2.7897e-01,  3.1905e-01, -8.3926e-02,\n",
            "          6.3161e-01,  1.8368e-01, -1.1074e+00, -6.5046e-01,  4.8638e-01,\n",
            "         -8.7040e-02, -1.4471e+00, -2.7048e-01, -4.2125e-01, -8.7714e-02,\n",
            "          1.0717e+00,  5.1591e-01,  2.0897e-01, -3.4510e-01,  2.6696e-01,\n",
            "          2.7177e-02,  7.7925e-01, -1.3859e-01, -2.2117e-01, -6.5482e-01,\n",
            "          3.5915e-01, -7.2729e-02, -5.7792e-02,  4.2919e-01, -3.5580e-01,\n",
            "         -1.4665e-01, -7.2910e-01, -1.7380e-01, -3.5819e-01, -5.9522e-01,\n",
            "         -3.8852e-01, -7.0899e-01,  5.9203e-01,  5.5967e-01, -1.1659e+00,\n",
            "         -4.2298e-01, -1.4501e+00, -2.4292e-01,  5.6571e-01,  5.7223e-01,\n",
            "          5.3741e-01,  6.2849e-01, -5.0560e-01, -1.5423e-01,  7.6740e-01,\n",
            "          1.0013e-01,  3.5270e-01,  2.3331e-01,  4.7878e-01,  4.2353e-02,\n",
            "         -9.5290e-02, -2.2600e-01,  8.6678e-01,  6.7277e-01,  1.0265e-01,\n",
            "          7.7293e-01, -4.4075e-01,  3.1836e-01]], grad_fn=<SqueezeBackward1>), tensor([[ 0.6577,  0.0769, -2.0698,  0.6302, -0.7739,  0.1003, -1.4609, -0.0563,\n",
            "          1.3152,  0.3620, -0.4041, -1.4760,  0.4945,  0.2631,  0.3044,  0.0741,\n",
            "         -1.1735, -0.2652, -0.5839,  0.0547, -0.4165, -0.6662,  0.5847,  0.0981,\n",
            "          0.7082,  0.0330,  0.1676,  0.2158, -0.2009, -0.1686,  0.4477, -0.0256,\n",
            "          0.1462,  0.1996, -0.4194,  0.1308,  0.6602, -0.7041, -0.1135,  0.1741,\n",
            "          0.3569,  0.1176, -0.0531, -0.1568, -0.0082,  0.3706, -0.0500,  0.1271,\n",
            "          0.1206,  0.6329,  0.2602,  0.8434,  0.9876,  0.7499,  0.9439,  0.1944,\n",
            "          1.2812,  0.3367,  0.1883, -0.3648, -0.1037,  0.5366,  0.3090,  0.9656,\n",
            "          0.2050,  0.1436,  0.4183, -1.4871, -0.8615, -0.1333,  0.4493, -0.1339,\n",
            "         -1.4715, -0.5103, -0.0352, -0.9334,  0.1436, -0.3359, -1.1670, -0.0314,\n",
            "         -0.4166, -0.8292, -0.4538, -0.3862, -0.9754, -1.6904,  0.3108, -0.2876,\n",
            "         -0.1431,  0.1334,  0.3299, -0.7597, -0.1918,  0.0187,  0.5554,  0.5456,\n",
            "          0.1306, -0.0766, -0.0791, -1.1028, -0.5350, -0.1549,  0.2586,  0.0439,\n",
            "          0.4167,  0.4296, -0.9880, -0.4621,  0.6400, -0.3462,  0.1035,  0.7041,\n",
            "          0.0400, -0.0820,  0.6803,  0.1692,  0.3725,  0.3347,  0.4463,  0.6377,\n",
            "         -0.0906,  0.5895,  0.4060, -0.2226, -0.5076,  0.3329, -0.2549,  0.9640,\n",
            "          0.6186,  0.2321, -0.2234, -0.2648, -0.4817, -0.4563, -0.2777,  0.4752,\n",
            "          0.0280, -0.8035,  0.2156, -0.6439,  0.8396, -0.6559, -0.5136,  0.4818,\n",
            "          0.6909,  0.1316, -0.0447, -0.2620,  0.5465,  0.1639,  0.4958,  0.4698,\n",
            "          0.1249,  0.2375,  0.8520,  0.4139,  0.3968, -0.5187, -1.4576,  0.6072,\n",
            "         -0.1181,  0.1269, -0.6048,  0.2870, -0.3551, -0.5953, -0.2969,  0.3472,\n",
            "         -0.2576, -0.9812, -0.0981, -0.4892, -0.9325, -1.0909, -0.4315,  0.9219,\n",
            "          0.9077,  0.6268, -0.7519,  0.5599, -0.0922,  0.2192, -0.0353,  0.5391,\n",
            "          0.0584,  0.1813,  0.6306, -0.0689, -1.1707, -0.4685,  0.4220, -0.3769,\n",
            "          0.4877, -0.1421,  0.6923, -0.4672,  0.6746, -0.3346,  0.8809,  0.8601,\n",
            "         -0.4747,  0.3992,  0.1695,  0.3809,  0.6904, -0.3789,  0.9063,  0.6430,\n",
            "          0.6629, -0.0609,  1.2050, -0.3645,  0.9162, -0.4683,  0.8550,  0.2604,\n",
            "          0.4558,  0.5313,  1.1208,  1.1295,  0.7420,  0.1774,  0.2315, -0.4257,\n",
            "          1.1750,  0.8388,  1.1261,  0.9232, -0.1963,  0.3044]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[ 0.8295,  0.2451, -0.7964, -0.0792, -0.4716, -0.0800,  0.2698,  0.0058,\n",
            "          0.1236,  0.3357,  0.3049, -0.2584, -0.6176, -1.1653, -0.7850, -0.6619,\n",
            "         -0.5687, -1.0754, -0.1064, -0.2091,  0.1660, -0.3582, -0.4231, -0.3638,\n",
            "         -0.0410, -0.9043,  0.0661, -0.1907, -0.2777, -0.3032,  0.2381, -0.1873,\n",
            "          0.1018, -1.5347, -0.9163, -0.7108, -0.8902, -1.0047, -0.2798, -1.0464,\n",
            "         -0.0386, -0.8732, -0.0679, -0.2285, -0.2164, -0.5818, -1.1238, -0.1634,\n",
            "          0.6745, -0.7150, -0.4141,  0.2046,  0.1972,  0.6231, -0.6305, -1.2569,\n",
            "         -0.2879,  0.5078, -0.7504,  0.7448, -0.1379, -0.5507, -0.5292,  0.2198,\n",
            "         -0.2350, -0.2591, -1.8225, -0.2150, -0.7303, -0.5362,  0.1392, -0.1851,\n",
            "          0.2527,  0.1520,  0.0175,  0.9974,  0.3198,  0.6703, -0.8432, -0.5138,\n",
            "         -0.2414, -0.1159, -0.0709, -0.9267,  0.1897, -0.5690,  0.0873,  0.0273,\n",
            "          0.0138, -0.4666, -0.1342, -0.4594, -0.4513,  0.4391,  0.2690, -0.0267,\n",
            "         -0.2395,  0.2380,  0.0675, -0.6933,  0.4068, -0.2962, -0.7048, -0.7896,\n",
            "         -0.4652, -0.1358, -0.6032,  0.0560,  0.2947,  0.0379,  0.1942, -0.7678,\n",
            "         -0.3397,  0.2418,  0.6542,  0.2178,  0.3518, -0.7780, -0.4240, -0.8079,\n",
            "          0.0757, -0.7782, -0.6889, -0.0020, -0.3960, -0.5695,  0.3049]],\n",
            "       grad_fn=<SqueezeBackward1>), tensor([[ 1.2553,  0.9693,  0.3886,  1.2756, -0.6646,  0.1642, -0.5705,  0.3038,\n",
            "          0.9081, -0.5637, -0.2938,  0.4317,  0.2857,  0.3099,  0.9044,  0.0611,\n",
            "          0.5614,  0.9390,  0.8195, -0.4734, -0.1322, -1.1014, -0.7723, -0.6528,\n",
            "          0.0621, -1.1741, -0.3496,  0.3795,  0.1212,  0.4495, -0.5846, -0.4567,\n",
            "          0.5055,  0.5869,  0.3438,  0.1179, -0.6867, -0.1867,  0.0587,  0.6829,\n",
            "         -0.0313, -0.7967,  0.5943,  0.3354,  0.4728, -0.3779, -0.1350,  0.3109,\n",
            "          0.6710,  1.0709, -0.6697,  0.5189,  0.1577,  0.4564,  0.3748,  1.0268,\n",
            "          1.4965,  0.7261, -0.4535,  0.1636,  1.1935, -0.5043, -0.1206,  0.4525,\n",
            "         -0.0602,  0.4932,  0.3541, -0.0433, -1.0264,  0.4225,  0.8114, -0.7926,\n",
            "         -0.2672,  0.7356, -0.0419, -0.0823, -1.7052,  0.0716,  0.2853, -0.6347,\n",
            "          0.0993, -0.2281,  0.4485, -1.3824,  0.3017,  0.1097,  0.4168,  0.1306,\n",
            "         -0.0733, -2.5977,  0.1053,  0.1111,  0.7975, -0.0626,  1.1498,  0.6298,\n",
            "          0.4356,  0.4572, -0.2907,  0.7203,  0.5235,  0.7193, -0.6489,  1.5148,\n",
            "          0.3108,  0.7731,  1.0593,  0.3714,  0.5217, -0.4068,  0.6780, -0.3423,\n",
            "          0.2464,  0.7599, -0.0518, -0.5786,  0.6383, -0.0323,  0.4542,  0.6103,\n",
            "          0.5959,  0.5224,  0.9849, -1.5451,  0.4834,  1.0375,  0.2361,  0.3977,\n",
            "          0.9672,  0.3216,  0.3105,  0.2764,  0.1246,  0.7892,  0.2793, -0.6972,\n",
            "         -0.9246, -1.0463, -0.7680, -0.4660,  0.1494, -1.6904, -1.3370,  0.5622,\n",
            "          0.8783,  0.3549, -0.5077,  0.0679, -0.5865,  0.3099]],\n",
            "       grad_fn=<SqueezeBackward1>)]\n",
            "tensor([202,  16, 114,  36,  28,  87,  71,  54,  11,   6], device='cuda:0')\n",
            "Finished after 0.2912752111752828 minutes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XigG3a6SkisN"
      },
      "source": [
        "Revalidate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JaSxK0ycAme"
      },
      "source": [
        "baseline_config = {\"modelname\": \"baseline\",\r\n",
        "                   \"train_batch_size\": 40,\r\n",
        "                   \"validation_batch_size\": 128,\r\n",
        "                   \"embedding_size\": 100,\r\n",
        "                   \"optimize_embeddings\": False,\r\n",
        "                   \"scale_emb_grad_by_freq\": False,\r\n",
        "                   \"RNN_input_dim\": 100,\r\n",
        "                   \"dropout_rate\": 0.1,\r\n",
        "                   \"RNN_nhidden\": 200,\r\n",
        "                   \"learning_rate\": 5e-3,\r\n",
        "                   \"RNN_layers\": 4,\r\n",
        "                   \"max_iterations\": 100,\r\n",
        "                   \"optimizer\": \"adam\",\r\n",
        "                   \"patience\":2,\r\n",
        "                   \"char_embeddings\": False}\r\n",
        "                   \r\n",
        "def revalidate(config):\r\n",
        "  # validate model\r\n",
        "  # print configuration\r\n",
        "  # print(json.dumps(config, indent=4, sort_keys=True))\r\n",
        "\r\n",
        "  # prepare torchtext fields (different in case of character embeddings)\r\n",
        "  if config[\"char_embeddings\"]:\r\n",
        "      fields = SquadDataset.prepare_fields_char()\r\n",
        "  else:\r\n",
        "      fields = SquadDataset.prepare_fields()\r\n",
        "  \r\n",
        "  # create train/validation datasets\r\n",
        "  train, val = SquadDataset.splits(fields)\r\n",
        "  fields = dict(fields)\r\n",
        "  \r\n",
        "  # we use the same field for question and document\r\n",
        "  # we can build vocabulary of words it represents by calling build_vocab [this takes a while]\r\n",
        "  # for each used word, we can pick the glove embedding and create an embedding matrix with index to embedding mapping\r\n",
        "  fields[\"question\"].build_vocab(train, val, vectors=GloVe(name='6B', dim=config[\"embedding_size\"]))\r\n",
        "  \r\n",
        "  # sort validation examples for faster validation\r\n",
        "  val_iter = BucketIterator(val, sort_key=lambda x: -(len(x.question) + len(x.document)), sort=True,\r\n",
        "                            batch_size=config[\"validation_batch_size\"],\r\n",
        "                            repeat=False,\r\n",
        "                            device=device)\r\n",
        "  validation_loss, em, f1 = validate(model, CrossEntropyLoss(reduction='none'), val_iter)     \r\n",
        "  print(f\"Validation loss/F1/EM: {validation_loss:.2f}, {f1:.2f}, {em:.2f}\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht3o4RnCe_5K"
      },
      "source": [
        "revalidate(baseline_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31IV1omUfXNP"
      },
      "source": [
        "# loading model\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvRWyxkFdKPX"
      },
      "source": [
        "if not os.path.isdir(\"saved\"):\n",
        "  os.mkdir(\"saved\")\n",
        "torch.save(model, \"./drive/MyDrive/data/saved/model_saved_4layers_256hidden_40tB_100embb.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A1KQGXAkbdZ"
      },
      "source": [
        "model = torch.load(\"./drive/MyDrive/data/saved/model_saved_4layers_256hidden_40tB_100embb.pt\")\n",
        "model = model.to(torch.device(\"cuda\"))\n",
        "model = model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv7PYyFzhLWS"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  \n",
        "print(f\"Models has {count_parameters(model)} parameters\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72C2xPKUMa06"
      },
      "source": [
        "def write_results(results, probs, val_file=\"./drive/MyDrive/data/squad/dev-v1.1.json\"):\n",
        "  if not os.path.isdir(\"results\"):\n",
        "    os.mkdir(\"results\")\n",
        "  f = open(f\"results/result_{get_timestamp()}_{socket.gethostname()}.csv\", mode=\"w\")\n",
        "  csvw = csv.writer(f, delimiter=',')\n",
        "  HEADER = [\"Correct\", \"Ground Truth(s)\", \"Prediction\", \"Confidence\", \"Question\", \"Context\", \"Topic\", \"ID\"]\n",
        "  csvw.writerow(HEADER)\n",
        "  with open(val_file) as fd:\n",
        "      data_json = json.load(fd)\n",
        "      for data_topic in data_json[\"data\"]:\n",
        "          for paragraph in data_topic[\"paragraphs\"]:\n",
        "              for question_and_answers in paragraph['qas']:\n",
        "                  prediction = results[question_and_answers[\"id\"]]\n",
        "                  confidence = str(f\"{probs[question_and_answers['id']]:.2f}\")\n",
        "                  answers = \"|\".join(map(lambda x: x['text'], question_and_answers['answers']))\n",
        "                  correct = int(results[question_and_answers[\"id\"]].lower() in map(lambda x: x['text'].lower(),\n",
        "                                                                                   question_and_answers[\n",
        "                                                                                       'answers']))\n",
        "                  ex = [correct,\n",
        "                        answers,\n",
        "                        prediction,\n",
        "                        confidence,\n",
        "                        question_and_answers['question'],\n",
        "                        paragraph[\"context\"],\n",
        "                        data_topic[\"title\"],\n",
        "                        question_and_answers[\"id\"]]\n",
        "                  csvw.writerow(ex)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whlGzus9Z4Tx"
      },
      "source": [
        "# RETRIEVER\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrJsP4y_p7P-"
      },
      "source": [
        "def extract_que_ans(dirnum, filename, lemmatized=False):\r\n",
        "  # get question and answer from czech squad \r\n",
        "  f = open(f\"drive/MyDrive/data/cz_sqad/data/{dirnum}/{filename}\", \"r\")\r\n",
        "  q = f.read().split(\"\\n\")\r\n",
        "  question = \"\"\r\n",
        "\r\n",
        "  #lemmatized parsed\r\n",
        "  if lemmatized:\r\n",
        "    answer = \"\"\r\n",
        "    for line in q:\r\n",
        "      if len(line.split()) < 2:\r\n",
        "        continue\r\n",
        "      line = line.split(\"\\t\")[1]\r\n",
        "      if line in {\"<s>\", \"<g/>\", \"</s>\"}:\r\n",
        "        answer = answer[:-1]\r\n",
        "        continue\r\n",
        "      if line != \"?\":\r\n",
        "        answer += line + \" \"\r\n",
        "      else:\r\n",
        "        answer += \" ? \"\r\n",
        "\r\n",
        "    f.close()\r\n",
        "    return answer\r\n",
        "\r\n",
        "  # normal parse\r\n",
        "  for line in q:\r\n",
        "    line = line.split(\"\\t\")[0]\r\n",
        "    if line in {\"<s>\", \"<g/>\", \"</s>\"}:\r\n",
        "      question = question[:-1]\r\n",
        "      continue\r\n",
        "    \r\n",
        "    if line != \"?\":\r\n",
        "      question += line + \" \"\r\n",
        "    else:\r\n",
        "      question += \" ? \"\r\n",
        "\r\n",
        "  f.close()\r\n",
        "  return question"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPfhmPY-xO85"
      },
      "source": [
        "# save the most common czech words\r\n",
        "common = \"být a se v na ten on že s z který mít do já o k i jeho ale svůj jako za moci rok pro tak po tento co když všechen už jak aby od nebo říci jeden jen můj jenž člověk ty stát u muset velký chtít také až než ještě při jít pak před dva však ani vědět nový hodně podle další celý jiný mezi dát tady den tam kde doba každý místo dobrý takový strana protože nic začít něco vidět říkat ne sám bez či dostat nějaký proto\"\r\n",
        "common = common.split()\r\n",
        "punctuation = \". , ? ! ... \\\" ( ) ; - /\"\r\n",
        "punctuation = punctuation.split()\r\n",
        "\r\n",
        "# decides if query token is common\r\n",
        "def iscommon(x):\r\n",
        "  if x in common or x in punctuation:\r\n",
        "    return True\r\n",
        "  else:\r\n",
        "    return False\r\n",
        "\r\n",
        "# remove the most common czech words from the query tokens (low information value)\r\n",
        "def delete_common(tokens):\r\n",
        "  tokens = [x for x in tokens if not iscommon(x)]\r\n",
        "      \r\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oNdBO3PCOjI"
      },
      "source": [
        "def search_again(tokens):\r\n",
        "\r\n",
        "  searched_term = (' ').join(tokens)\r\n",
        "  print(searched_term)\r\n",
        "  doc_list = wikipedia.search(searched_term)\r\n",
        "\r\n",
        "  if len(tokens) == 0:\r\n",
        "    return []\r\n",
        "\r\n",
        "  if len(doc_list) == 0:\r\n",
        "    del tokens[0]\r\n",
        "    return search_again(tokens)\r\n",
        "\r\n",
        "  return doc_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXpTErpEr1hN"
      },
      "source": [
        "morph = majka.Majka('drive/MyDrive/data/majka.w-lt')\r\n",
        "morph.flags |= majka.ADD_DIACRITICS  # find word forms with diacritics\r\n",
        "morph.flags |= majka.DISALLOW_LOWERCASE  # do not enable to find lowercase variants\r\n",
        "morph.flags |= majka.IGNORE_CASE  # ignore the word case whatsoever\r\n",
        "morph.flags = 0  # unset all flags\r\n",
        "\r\n",
        "morph.tags = False  # return just the lemma, do not process the tags\r\n",
        "morph.first_only = True  # return only the first entry\r\n",
        "morph.negative = \"ne\"\r\n",
        "\r\n",
        "# returns lemma of each token in a list of lemmatized tokens\r\n",
        "def lemmatize(text):\r\n",
        "\r\n",
        "  tok_text = text.lower()\r\n",
        "  tok_text = re.split(\"\\W\", text)\r\n",
        "\r\n",
        "  # lemmatize each token\r\n",
        "  lemmatized_tokens = []\r\n",
        "  for token in tok_text:\r\n",
        "    if token == '':\r\n",
        "      continue\r\n",
        "    lemma = morph.find(token)\r\n",
        "    if len(lemma) == 0:\r\n",
        "      lemmatized_tokens.append(token)\r\n",
        "    else:\r\n",
        "      lemmatized_tokens.append(lemma[0]['lemma'])\r\n",
        "\r\n",
        "  return lemmatized_tokens\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjYaStGxV7tw"
      },
      "source": [
        "def retrieve(question):  \r\n",
        "\r\n",
        "  #search for documents\r\n",
        "  doc_list = wikipedia.search(question)\r\n",
        "\r\n",
        "  # simplify the search if its too bad\r\n",
        "  if len(doc_list) == 0:\r\n",
        "    # extract important for wiki\r\n",
        "    tokens = lemmatize(question)\r\n",
        "    tokens = delete_common(tokens)\r\n",
        "    doc_list = search_again(tokens)\r\n",
        "    \r\n",
        "  if len(doc_list) == 0:\r\n",
        "      return \"\"\r\n",
        "\r\n",
        "  # split docs into paragraphs\r\n",
        "  pars = []\r\n",
        "  max_docs = 3\r\n",
        "  num_docs = 0\r\n",
        "\r\n",
        "  for doc in doc_list:\r\n",
        "    # get whole page content\r\n",
        "    try:\r\n",
        "      doc = wikipedia.page(doc)\r\n",
        "    except wikipedia.DisambiguationError as e:\r\n",
        "      s = e.options[0]\r\n",
        "      doc = wikipedia.page(s)\r\n",
        "    result = re.split('== .*. ==|\\n\\n', doc.content)\r\n",
        "\r\n",
        "    # save stripped paragraphs\r\n",
        "    for par in result:\r\n",
        "      par = par.strip()\r\n",
        "      par = par.strip('=')\r\n",
        "      par = par.strip('\\n')\r\n",
        "      par = par.strip('\\r\\n')\r\n",
        "\r\n",
        "      if par == '' or par == '\\n':\r\n",
        "        continue\r\n",
        "      pars.append(par)\r\n",
        "\r\n",
        "    num_docs += 1\r\n",
        "    if num_docs >= max_docs:\r\n",
        "      break\r\n",
        "\r\n",
        "  # tokenize for bm25\r\n",
        "  tok_text = []\r\n",
        "  for par in pars:\r\n",
        "    tok_par = par.lower()\r\n",
        "    tok_par = re.split(\"\\W\", tok_par)\r\n",
        "    for tok in tok_par:\r\n",
        "      if tok == \"\":\r\n",
        "        tok_par.remove(\"\")\r\n",
        "    tok_text.append(tok_par)\r\n",
        "\r\n",
        "  # build index\r\n",
        "  bm25 = BM25Okapi(tok_text)\r\n",
        "\r\n",
        "  # tokenize the query\r\n",
        "  tokenized_query = question.lower()\r\n",
        "  tokenized_query = re.split(\"\\W\", tokenized_query)\r\n",
        "\r\n",
        "  # get results\r\n",
        "  results = bm25.get_top_n(tokenized_query, pars, n=3)\r\n",
        "\r\n",
        "  return results\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4saxh9bfoqYe"
      },
      "source": [
        "# create very simple batch object\r\n",
        "class simple_batch():\r\n",
        "  def __init__(self,q,d,raw_d,d_pos):\r\n",
        "    map_to_gpu_tensor = lambda x: torch.Tensor(x).long().to(torch.device(\"cuda\")).unsqueeze(0)\r\n",
        "    self.question=map_to_gpu_tensor(q)\r\n",
        "    self.document=map_to_gpu_tensor(d)\r\n",
        "    self.raw_document_context = [raw_d]\r\n",
        "    self.document_token_positions = [d_pos]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vx4Wp01k0Gl"
      },
      "source": [
        "# Validace na sqad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn5_5_YKeC0w"
      },
      "source": [
        "# write results to\n",
        "f = open(\"drive/MyDrive/data/saved_answers/test.txt\", \"w\")\n",
        "\n",
        "# write first question-answer pairs in sqad\n",
        "from_q = 1\n",
        "to_q = 1\n",
        "for i in range(from_q, to_q+1):\n",
        "\n",
        "  # get question number\n",
        "  name = \"\"\n",
        "  for _ in range(len(str(i)), 6):\n",
        "    name += \"0\"\n",
        "  name += str(i)\n",
        "\n",
        "  # extract from dataset\n",
        "  question = extract_que_ans(name, \"01question.vert\")\n",
        "  correct_answer = extract_que_ans(name, \"09answer_extraction.vert\")\n",
        "  lemmatized_answer = extract_que_ans(name, \"09answer_extraction.vert\", lemmatized=True)\n",
        "\n",
        "  # wiki search\n",
        "  documents = retrieve(question)\n",
        "  # for saving the best results\n",
        "  bestAnswers = []\n",
        "  bestProbs = []\n",
        "  bestDocs = []\n",
        "  bestLogProbs = []\n",
        "\n",
        "  question_cs = question # save czech question\n",
        "\n",
        "  # iterate over retrieved paragraphs\n",
        "  for document in documents:\n",
        "\n",
        "    # strip whitespaces\n",
        "    document = document.strip()\n",
        "\n",
        "    # chceck if any document has been found for the question\n",
        "    if document == \"\":\n",
        "      f.write(\"question: \" + question + \"\\n\" +\n",
        "              \"answer: odpověď nenalezena\" + \"\\n\" + \n",
        "              \"correct answer: \" + correct_answer + \"\\n\\n\")\n",
        "      continue;\n",
        "    try:\n",
        "      document_cs = document\n",
        "      document = translator.translate(document, dest='en').text\n",
        "    except TypeError:\n",
        "      continue\n",
        "\n",
        "    # remove some trash\n",
        "    # TODO this not cool (in retriever)\n",
        "    if (document_cs.strip().startswith(\"Obrázky, zvuky či videa k tématu\")):\n",
        "      continue\n",
        "\n",
        "\n",
        "    bestDocs.append(document_cs)\n",
        "    # translate\n",
        "    question = translator.translate(question, dest='en').text\n",
        "\n",
        "    # make sure the current vocab is model's vocab\n",
        "    vocab = model.embedder.vocab\n",
        "\n",
        "    # tokenization\n",
        "    document_tokens, tokenized_document_list = tokenize(document)\n",
        "    tokenized_question_list = tokenize(question)[1]\n",
        "\n",
        "    # keep positions of each token in document, we will need this later, when decoding model outputs\n",
        "    document_token_positions = [[token.idx, token.idx + len(token.text)] for token in document_tokens]\n",
        "\n",
        "    # lowercasing and numericalization\n",
        "    numericalized_document = [vocab.stoi[s.lower()] for s in tokenized_document_list]\n",
        "    numericalized_question = [vocab.stoi[s.lower()] for s in tokenized_question_list]\n",
        "\n",
        "    batch = simple_batch(numericalized_question,numericalized_document,document,document_token_positions)\n",
        "\n",
        "    # get predictions with arg_maxes\n",
        "    logprobs_S, logprobs_E, argmax_Q = model.forward(batch, return_max=True)\n",
        "\n",
        "    # decode from log probabilities to predictions\n",
        "    best_span_prob, candidate, logprobs = decode(logprobs_S, logprobs_E)\n",
        "    confidence =  best_span_prob.item()\n",
        "    log_conf = logprobs.item()\n",
        "\n",
        "    #get answer\n",
        "    answers = get_spans(batch, candidate)\n",
        "    answer = answers[0]\n",
        "\n",
        "    # save probs and answer\n",
        "    bestAnswers.append(answer)\n",
        "    bestProbs.append(confidence)\n",
        "    bestLogProbs.append(log_conf)\n",
        "\n",
        "  # check if any answer was found\n",
        "  if len(bestProbs) == 0:\n",
        "    f.write(\"question: \" + question_cs + \"\\n\" +\n",
        "              \"answer: odpověď nenalezena\" + \"\\n\" + \n",
        "              \"correct answer: \" + correct_answer + \"\\n\\n\")\n",
        "    continue\n",
        "\n",
        "  print(bestAnswers)\n",
        "  print(bestProbs)\n",
        "  print(bestLogProbs)\n",
        "  # get the best doc\n",
        "  # get best answer from retriever according to reader\n",
        "  # get the best confidence\n",
        "  document = bestDocs[np.argmax(bestLogProbs, axis=0)]\n",
        "  answer = bestAnswers[np.argmax(bestLogProbs, axis=0)]\n",
        "  confidence = bestProbs[np.argmax(bestLogProbs, axis=0)]\n",
        "\n",
        "  # translate the final answer\n",
        "  answer =  translator.translate(answer, dest='cs').text\n",
        "\n",
        "  # write the result to file\n",
        "  f.write(\"otázka č.\" + name + \": \" + question_cs + \"\\n\" +\n",
        "          \"::odpověď: \" + answer + \"\\n\" + \n",
        "          \"::správná odpověď podle sqad : \" + correct_answer + \"\\n\" +\n",
        "          # \"::sqad lemmatized : \" + lemmatized_answer + \"\\n\" +\n",
        "          \"----------------------------------------------------------------\\n\"+\n",
        "          \"získaný dokument: \" + document + \n",
        "          \"\\n----------------------------------------------------------------\\n\"+\n",
        "          \"----------------------------------------------------------------\"+\n",
        "          \"\\n\\n\")\n",
        "  \n",
        "  # controlprint\n",
        "  print(\"wrote: \" + name)\n",
        "\n",
        "  # print(f\"The answer is: \\\"{answer}\\\".\")\n",
        "  # print(f\"The model is confident with {confidence:.2f} probability.\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}